[0m19:29:17.285329 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000215FF36FE90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000215FF62CF90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000215FF36F8D0>]}


============================== 19:29:17.289327 | d1bb6b3b-30e5-45a6-97b7-1cd657012689 ==============================
[0m19:29:17.289327 [info ] [MainThread]: Running with dbt=1.7.7
[0m19:29:17.290083 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt debug', 'send_anonymous_usage_stats': 'True'}
[0m19:29:17.291118 [info ] [MainThread]: dbt version: 1.7.7
[0m19:29:17.292097 [info ] [MainThread]: python version: 3.11.2
[0m19:29:17.293096 [info ] [MainThread]: python path: C:\Users\hanhn\AppData\Local\Programs\Python\Python311\python.exe
[0m19:29:17.293096 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m19:29:18.487264 [info ] [MainThread]: Using profiles dir at C:\Users\hanhn\.dbt
[0m19:29:18.488251 [info ] [MainThread]: Using profiles.yml file at C:\Users\hanhn\.dbt\profiles.yml
[0m19:29:18.489258 [info ] [MainThread]: Using dbt_project.yml file at D:\Data Engineering\Medallion-Spark-Azure-DBT\medallion_dbt_spark\dbt_project.yml
[0m19:29:18.490260 [info ] [MainThread]: adapter type: databricks
[0m19:29:18.491257 [info ] [MainThread]: adapter version: 1.7.7
[0m19:29:18.554184 [info ] [MainThread]: Configuration:
[0m19:29:18.554738 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m19:29:18.555758 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m19:29:18.556791 [info ] [MainThread]: Required dependencies:
[0m19:29:18.557996 [debug] [MainThread]: Executing "git --help"
[0m19:29:18.594160 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m19:29:18.594160 [debug] [MainThread]: STDERR: "b''"
[0m19:29:18.595172 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m19:29:18.596169 [info ] [MainThread]: Connection:
[0m19:29:18.597166 [info ] [MainThread]:   host: https://adb-4151724469439226.6.azuredatabricks.net
[0m19:29:18.598168 [info ] [MainThread]:   http_path: sql/protocolv1/o/4151724469439226/0213-101109-dhbx09c2
[0m19:29:18.598168 [info ] [MainThread]:   catalog: hive_metastore
[0m19:29:18.599313 [info ] [MainThread]:   schema: saleslt
[0m19:29:18.600316 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m19:29:18.600316 [debug] [MainThread]: Databricks adapter: conn: 2291661667344: Creating DatabricksDBTConnection sess: None, name: debug, idle: 0s, acqrelcnt: 0, lang: None, thrd: (14944, 13032), cmpt: ``, lut: None
[0m19:29:18.601339 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m19:29:18.601339 [debug] [MainThread]: Databricks adapter: Thread (14944, 13032) using default compute resource.
[0m19:29:18.602315 [debug] [MainThread]: Databricks adapter: conn: 2291661667344: _acquire sess: None, name: debug, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (14944, 13032), cmpt: ``, lut: 1707827358.602315
[0m19:29:18.602315 [debug] [MainThread]: Databricks adapter: conn: 2291661667344: get_thread_connection: sess: None, name: debug, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (14944, 13032), cmpt: ``, lut: 1707827358.602315
[0m19:29:18.603313 [debug] [MainThread]: Databricks adapter: conn: 2291661667344: idle check connection: sess: None, name: debug, idle: 0.0009987354278564453s, acqrelcnt: 1, lang: None, thrd: (14944, 13032), cmpt: ``, lut: 1707827358.602315
[0m19:29:18.603313 [debug] [MainThread]: Using databricks connection "debug"
[0m19:29:18.603313 [debug] [MainThread]: On debug: select 1 as id
[0m19:29:18.604314 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:29:21.314339 [debug] [MainThread]: Databricks adapter: Error while running:
select 1 as id
[0m19:29:21.315258 [debug] [MainThread]: Databricks adapter: Database Error
  HTTPSConnectionPool(host='https', port=None): Max retries exceeded with url: //adb-4151724469439226.6.azuredatabricks.net:443/sql/protocolv1/o/4151724469439226/0213-101109-dhbx09c2 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000215919F5C10>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))
[0m19:29:21.317258 [debug] [MainThread]: Databricks adapter: conn: 2291661667344: _release sess: None, name: debug, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (14944, 13032), cmpt: ``, lut: 1707827361.317258
[0m19:29:21.319339 [info ] [MainThread]:   Connection test: [[31mERROR[0m]

[0m19:29:21.320275 [info ] [MainThread]: [31m1 check failed:[0m
[0m19:29:21.321265 [info ] [MainThread]: dbt was unable to connect to the specified database.
The database returned the following error:

  >Runtime Error
  Database Error
    HTTPSConnectionPool(host='https', port=None): Max retries exceeded with url: //adb-4151724469439226.6.azuredatabricks.net:443/sql/protocolv1/o/4151724469439226/0213-101109-dhbx09c2 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000215919F5C10>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))

Check your database credentials and try again. For more information, visit:
https://docs.getdbt.com/docs/configure-your-profile


[0m19:29:21.326494 [debug] [MainThread]: Command `dbt debug` failed at 19:29:21.326494 after 4.10 seconds
[0m19:29:21.327497 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m19:29:21.327497 [debug] [MainThread]: On debug: No close available on handle
[0m19:29:21.328497 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000215FF36DA10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000215FF417150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000215FF660D10>]}
[0m19:29:21.329509 [debug] [MainThread]: Flushing usage events
[0m19:33:38.074099 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000256CBCD4D10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000256CB6F5B50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000256CB685610>]}


============================== 19:33:38.081271 | 9c00a1b1-4358-44f8-8fd3-a6035939bc67 ==============================
[0m19:33:38.081271 [info ] [MainThread]: Running with dbt=1.7.7
[0m19:33:38.082447 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt debug', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m19:33:38.083447 [info ] [MainThread]: dbt version: 1.7.7
[0m19:33:38.084450 [info ] [MainThread]: python version: 3.11.2
[0m19:33:38.085452 [info ] [MainThread]: python path: C:\Users\hanhn\AppData\Local\Programs\Python\Python311\python.exe
[0m19:33:38.086451 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m19:33:39.576637 [info ] [MainThread]: Using profiles dir at C:\Users\hanhn\.dbt
[0m19:33:39.577575 [info ] [MainThread]: Using profiles.yml file at C:\Users\hanhn\.dbt\profiles.yml
[0m19:33:39.578084 [info ] [MainThread]: Using dbt_project.yml file at D:\Data Engineering\Medallion-Spark-Azure-DBT\medallion_dbt_spark\dbt_project.yml
[0m19:33:39.579138 [info ] [MainThread]: adapter type: databricks
[0m19:33:39.580092 [info ] [MainThread]: adapter version: 1.7.7
[0m19:33:39.643095 [info ] [MainThread]: Configuration:
[0m19:33:39.644093 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m19:33:39.645016 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m19:33:39.646039 [info ] [MainThread]: Required dependencies:
[0m19:33:39.646039 [debug] [MainThread]: Executing "git --help"
[0m19:33:39.679384 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m19:33:39.680296 [debug] [MainThread]: STDERR: "b''"
[0m19:33:39.681491 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m19:33:39.682499 [info ] [MainThread]: Connection:
[0m19:33:39.684495 [info ] [MainThread]:   host: adb-4151724469439226.6.azuredatabricks.net
[0m19:33:39.685495 [info ] [MainThread]:   http_path: sql/protocolv1/o/4151724469439226/0213-101109-dhbx09c2
[0m19:33:39.686494 [info ] [MainThread]:   catalog: hive_metastore
[0m19:33:39.687563 [info ] [MainThread]:   schema: saleslt
[0m19:33:39.688572 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m19:33:39.689573 [debug] [MainThread]: Databricks adapter: conn: 2572113682704: Creating DatabricksDBTConnection sess: None, name: debug, idle: 0s, acqrelcnt: 0, lang: None, thrd: (6764, 6964), cmpt: ``, lut: None
[0m19:33:39.689573 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m19:33:39.690574 [debug] [MainThread]: Databricks adapter: Thread (6764, 6964) using default compute resource.
[0m19:33:39.690574 [debug] [MainThread]: Databricks adapter: conn: 2572113682704: _acquire sess: None, name: debug, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (6764, 6964), cmpt: ``, lut: 1707827619.6905744
[0m19:33:39.691575 [debug] [MainThread]: Databricks adapter: conn: 2572113682704: get_thread_connection: sess: None, name: debug, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (6764, 6964), cmpt: ``, lut: 1707827619.6905744
[0m19:33:39.691575 [debug] [MainThread]: Databricks adapter: conn: 2572113682704: idle check connection: sess: None, name: debug, idle: 0.0010013580322265625s, acqrelcnt: 1, lang: None, thrd: (6764, 6964), cmpt: ``, lut: 1707827619.6905744
[0m19:33:39.691575 [debug] [MainThread]: Using databricks connection "debug"
[0m19:33:39.692594 [debug] [MainThread]: On debug: select 1 as id
[0m19:33:39.692594 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:33:40.801274 [debug] [MainThread]: Databricks adapter: conn: 2572113682704: session opened sess: df5d4167-a14b-407f-a71d-5b437f013f0d, name: debug, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (6764, 6964), cmpt: ``, lut: 1707827620.8002253
[0m19:33:43.063547 [debug] [MainThread]: SQL status: OK in 3.369999885559082 seconds
[0m19:33:43.064560 [debug] [MainThread]: Databricks adapter: conn: 2572113682704: _release sess: df5d4167-a14b-407f-a71d-5b437f013f0d, name: debug, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (6764, 6964), cmpt: ``, lut: 1707827623.0645602
[0m19:33:43.065586 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m19:33:43.066555 [info ] [MainThread]: [32mAll checks passed![0m
[0m19:33:43.068214 [debug] [MainThread]: Command `dbt debug` succeeded at 19:33:43.068214 after 5.05 seconds
[0m19:33:43.068214 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m19:33:43.068214 [debug] [MainThread]: On debug: Close
[0m19:33:43.487136 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000256CBA36B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000256CB657910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000256CB6571D0>]}
[0m19:33:43.489134 [debug] [MainThread]: Flushing usage events
[0m20:47:23.069072 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013D4C309B10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013D4C3093D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013D4C342A90>]}


============================== 20:47:23.074593 | 40586b6f-136d-41a7-b7ce-5362f116a129 ==============================
[0m20:47:23.074593 [info ] [MainThread]: Running with dbt=1.7.7
[0m20:47:23.075591 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt snapshot', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m20:47:24.962826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '40586b6f-136d-41a7-b7ce-5362f116a129', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013D5E743190>]}
[0m20:47:25.033813 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '40586b6f-136d-41a7-b7ce-5362f116a129', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013D4C6F5990>]}
[0m20:47:25.034839 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m20:47:25.059440 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m20:47:25.061953 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m20:47:25.062953 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '40586b6f-136d-41a7-b7ce-5362f116a129', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013D5E81D990>]}
[0m20:47:27.977278 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '40586b6f-136d-41a7-b7ce-5362f116a129', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013D5E8C6CD0>]}
[0m20:47:27.997580 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '40586b6f-136d-41a7-b7ce-5362f116a129', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013D5EAF4C50>]}
[0m20:47:27.998677 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 535 macros, 0 groups, 0 semantic models
[0m20:47:27.999906 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '40586b6f-136d-41a7-b7ce-5362f116a129', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013D5EB72A10>]}
[0m20:47:28.000913 [info ] [MainThread]: 
[0m20:47:28.001921 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m20:47:28.003923 [debug] [MainThread]: Command end result
[0m20:47:28.020288 [debug] [MainThread]: Command `dbt snapshot` succeeded at 20:47:28.020288 after 5.01 seconds
[0m20:47:28.021288 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013D44F88310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013D4B910E50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013D4C4267D0>]}
[0m20:47:28.021796 [debug] [MainThread]: Flushing usage events
[0m20:54:31.118310 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000213AB11F210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000213AB124050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000213AB11F950>]}


============================== 20:54:31.123624 | b9da586e-f109-44f4-80c5-7e73ae52c838 ==============================
[0m20:54:31.123624 [info ] [MainThread]: Running with dbt=1.7.7
[0m20:54:31.125793 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt debug', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:54:31.125793 [info ] [MainThread]: dbt version: 1.7.7
[0m20:54:31.126793 [info ] [MainThread]: python version: 3.11.2
[0m20:54:31.127793 [info ] [MainThread]: python path: C:\Users\hanhn\AppData\Local\Programs\Python\Python311\python.exe
[0m20:54:31.127793 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m20:54:32.784842 [info ] [MainThread]: Using profiles dir at C:\Users\hanhn\.dbt
[0m20:54:32.785570 [info ] [MainThread]: Using profiles.yml file at C:\Users\hanhn\.dbt\profiles.yml
[0m20:54:32.786632 [info ] [MainThread]: Using dbt_project.yml file at D:\Data Engineering\Medallion-Spark-Azure-DBT\medallion_dbt_spark\dbt_project.yml
[0m20:54:32.787642 [info ] [MainThread]: adapter type: databricks
[0m20:54:32.789152 [info ] [MainThread]: adapter version: 1.7.7
[0m20:54:32.867965 [info ] [MainThread]: Configuration:
[0m20:54:32.870485 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m20:54:32.871489 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m20:54:32.872842 [info ] [MainThread]: Required dependencies:
[0m20:54:32.873906 [debug] [MainThread]: Executing "git --help"
[0m20:54:32.913730 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m20:54:32.914730 [debug] [MainThread]: STDERR: "b''"
[0m20:54:32.915727 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m20:54:32.917730 [info ] [MainThread]: Connection:
[0m20:54:32.917730 [info ] [MainThread]:   host: adb-4151724469439226.6.azuredatabricks.net
[0m20:54:32.919237 [info ] [MainThread]:   http_path: sql/protocolv1/o/4151724469439226/0213-101109-dhbx09c2
[0m20:54:32.921248 [info ] [MainThread]:   catalog: hive_metastore
[0m20:54:32.921248 [info ] [MainThread]:   schema: saleslt
[0m20:54:32.923712 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m20:54:32.925223 [debug] [MainThread]: Databricks adapter: conn: 2283802012112: Creating DatabricksDBTConnection sess: None, name: debug, idle: 0s, acqrelcnt: 0, lang: None, thrd: (6320, 23264), cmpt: ``, lut: None
[0m20:54:32.926268 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m20:54:32.926268 [debug] [MainThread]: Databricks adapter: Thread (6320, 23264) using default compute resource.
[0m20:54:32.926268 [debug] [MainThread]: Databricks adapter: conn: 2283802012112: _acquire sess: None, name: debug, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (6320, 23264), cmpt: ``, lut: 1707832472.9262683
[0m20:54:32.927231 [debug] [MainThread]: Databricks adapter: conn: 2283802012112: get_thread_connection: sess: None, name: debug, idle: 0.0009629726409912109s, acqrelcnt: 1, lang: None, thrd: (6320, 23264), cmpt: ``, lut: 1707832472.9262683
[0m20:54:32.927231 [debug] [MainThread]: Databricks adapter: conn: 2283802012112: idle check connection: sess: None, name: debug, idle: 0.0009629726409912109s, acqrelcnt: 1, lang: None, thrd: (6320, 23264), cmpt: ``, lut: 1707832472.9262683
[0m20:54:32.928237 [debug] [MainThread]: Using databricks connection "debug"
[0m20:54:32.928237 [debug] [MainThread]: On debug: select 1 as id
[0m20:54:32.928237 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:54:33.672701 [debug] [MainThread]: Databricks adapter: conn: 2283802012112: session opened sess: ec77f38e-bd86-42e4-8350-13c35cc88222, name: debug, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (6320, 23264), cmpt: ``, lut: 1707832473.6727011
[0m20:54:34.197483 [debug] [MainThread]: SQL status: OK in 1.2699999809265137 seconds
[0m20:54:34.199666 [debug] [MainThread]: Databricks adapter: conn: 2283802012112: _release sess: ec77f38e-bd86-42e4-8350-13c35cc88222, name: debug, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (6320, 23264), cmpt: ``, lut: 1707832474.199667
[0m20:54:34.200670 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m20:54:34.201669 [info ] [MainThread]: [32mAll checks passed![0m
[0m20:54:34.203222 [debug] [MainThread]: Command `dbt debug` succeeded at 20:54:34.202126 after 3.15 seconds
[0m20:54:34.203222 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m20:54:34.204135 [debug] [MainThread]: On debug: Close
[0m20:54:34.297565 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000213AADAC450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000213BD28D1D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000213AB14AC90>]}
[0m20:54:34.298517 [debug] [MainThread]: Flushing usage events
[0m20:55:55.923651 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018E440123D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018E4436FCD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018E44091810>]}


============================== 20:55:55.927963 | 8e006903-fbee-495e-8595-4095e199fe65 ==============================
[0m20:55:55.927963 [info ] [MainThread]: Running with dbt=1.7.7
[0m20:55:55.928962 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt snapshot', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m20:55:57.191106 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8e006903-fbee-495e-8595-4095e199fe65', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018E567752D0>]}
[0m20:55:57.257281 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8e006903-fbee-495e-8595-4095e199fe65', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018E56721B90>]}
[0m20:55:57.257281 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m20:55:57.267600 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m20:55:57.451422 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m20:55:57.452453 [debug] [MainThread]: Partial parsing: added file: medallion_dbt_spark://models\example\staging\bronze.yml
[0m20:55:57.512626 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two sources with the name "saleslt_product".
  
  Since these resources have the same name, dbt will be unable to find the correct resource
  when looking for source("saleslt", "product").
  
  To fix this, change the name of one of these resources:
  - source.medallion_dbt_spark.saleslt.product (models\example\staging\bronze.yml)
  - source.medallion_dbt_spark.saleslt.product (models\example\staging\bronze.yml)
[0m20:55:57.516312 [debug] [MainThread]: Command `dbt snapshot` failed at 20:55:57.516312 after 1.63 seconds
[0m20:55:57.516312 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018E442F2290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018E4465CC10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018E44685690>]}
[0m20:55:57.517313 [debug] [MainThread]: Flushing usage events
[0m20:58:00.114746 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F5A4297950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F5A38BD5D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F5A28B1F50>]}


============================== 20:58:00.118751 | 029010bf-5dac-43ec-bae0-977def7b138a ==============================
[0m20:58:00.118751 [info ] [MainThread]: Running with dbt=1.7.7
[0m20:58:00.120747 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt snapshot', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:58:01.382199 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '029010bf-5dac-43ec-bae0-977def7b138a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F5B63D92D0>]}
[0m20:58:01.447159 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '029010bf-5dac-43ec-bae0-977def7b138a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F5A4297950>]}
[0m20:58:01.448157 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m20:58:01.457781 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m20:58:01.538364 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m20:58:01.539386 [debug] [MainThread]: Partial parsing: added file: medallion_dbt_spark://models\example\staging\bronze.yml
[0m20:58:01.628651 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '029010bf-5dac-43ec-bae0-977def7b138a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F5B63D9950>]}
[0m20:58:01.642336 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '029010bf-5dac-43ec-bae0-977def7b138a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F5B660B290>]}
[0m20:58:01.643337 [info ] [MainThread]: Found 2 models, 4 tests, 9 sources, 0 exposures, 0 metrics, 535 macros, 0 groups, 0 semantic models
[0m20:58:01.644338 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '029010bf-5dac-43ec-bae0-977def7b138a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F5B63D9150>]}
[0m20:58:01.646410 [info ] [MainThread]: 
[0m20:58:01.647335 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m20:58:01.648345 [debug] [MainThread]: Command end result
[0m20:58:01.656668 [debug] [MainThread]: Command `dbt snapshot` succeeded at 20:58:01.656668 after 1.58 seconds
[0m20:58:01.657665 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F5A3CD26D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F5A3C22F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F5A3C20410>]}
[0m20:58:01.657665 [debug] [MainThread]: Flushing usage events
[0m21:05:43.298339 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001348B0A2B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001348AEFF050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001348AB07CD0>]}


============================== 21:05:43.304427 | 7e137e83-2082-4196-9839-c2281087556b ==============================
[0m21:05:43.304427 [info ] [MainThread]: Running with dbt=1.7.7
[0m21:05:43.305337 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt snapshot', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m21:05:45.023789 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7e137e83-2082-4196-9839-c2281087556b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001349D52F610>]}
[0m21:05:45.089065 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7e137e83-2082-4196-9839-c2281087556b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001348B475D90>]}
[0m21:05:45.090095 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m21:05:45.100078 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m21:05:45.241778 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:05:45.242777 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://snapshots\address.sql
[0m21:05:45.257097 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7e137e83-2082-4196-9839-c2281087556b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001349D536B90>]}
[0m21:05:45.270353 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7e137e83-2082-4196-9839-c2281087556b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001349D6EFD10>]}
[0m21:05:45.271349 [info ] [MainThread]: Found 2 models, 4 tests, 9 sources, 0 exposures, 0 metrics, 535 macros, 0 groups, 0 semantic models
[0m21:05:45.272350 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7e137e83-2082-4196-9839-c2281087556b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001349D842310>]}
[0m21:05:45.273349 [info ] [MainThread]: 
[0m21:05:45.274689 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m21:05:45.275690 [debug] [MainThread]: Command end result
[0m21:05:45.283946 [debug] [MainThread]: Command `dbt snapshot` succeeded at 21:05:45.282857 after 2.03 seconds
[0m21:05:45.283946 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001348B0B5950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001348A6907D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001348B08AD10>]}
[0m21:05:45.284888 [debug] [MainThread]: Flushing usage events
[0m21:32:46.328378 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E57A3A8290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E501812F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E501825290>]}


============================== 21:32:46.335656 | e2456685-0d49-4e6d-ba98-9ea07b7c3ba8 ==============================
[0m21:32:46.335656 [info ] [MainThread]: Running with dbt=1.7.7
[0m21:32:46.337653 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt debug', 'send_anonymous_usage_stats': 'True'}
[0m21:32:46.337653 [info ] [MainThread]: dbt version: 1.7.7
[0m21:32:46.338648 [info ] [MainThread]: python version: 3.11.2
[0m21:32:46.339643 [info ] [MainThread]: python path: C:\Users\hanhn\AppData\Local\Programs\Python\Python311\python.exe
[0m21:32:46.340644 [info ] [MainThread]: os info: Windows-10-10.0.22621-SP0
[0m21:32:48.063155 [info ] [MainThread]: Using profiles dir at C:\Users\hanhn\.dbt
[0m21:32:48.064533 [info ] [MainThread]: Using profiles.yml file at C:\Users\hanhn\.dbt\profiles.yml
[0m21:32:48.065539 [info ] [MainThread]: Using dbt_project.yml file at D:\Data Engineering\Medallion-Spark-Azure-DBT\medallion_dbt_spark\dbt_project.yml
[0m21:32:48.066542 [info ] [MainThread]: adapter type: databricks
[0m21:32:48.067598 [info ] [MainThread]: adapter version: 1.7.7
[0m21:32:48.132459 [info ] [MainThread]: Configuration:
[0m21:32:48.134465 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m21:32:48.135463 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m21:32:48.136124 [info ] [MainThread]: Required dependencies:
[0m21:32:48.136867 [debug] [MainThread]: Executing "git --help"
[0m21:32:48.170854 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--config-env=<name>=<envvar>] <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m21:32:48.171854 [debug] [MainThread]: STDERR: "b''"
[0m21:32:48.172857 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m21:32:48.173856 [info ] [MainThread]: Connection:
[0m21:32:48.174856 [info ] [MainThread]:   host: adb-4151724469439226.6.azuredatabricks.net
[0m21:32:48.175859 [info ] [MainThread]:   http_path: sql/protocolv1/o/4151724469439226/0213-101109-dhbx09c2
[0m21:32:48.176854 [info ] [MainThread]:   catalog: hive_metastore
[0m21:32:48.177854 [info ] [MainThread]:   schema: saleslt
[0m21:32:48.178855 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m21:32:48.178855 [debug] [MainThread]: Databricks adapter: conn: 2083390859664: Creating DatabricksDBTConnection sess: None, name: debug, idle: 0s, acqrelcnt: 0, lang: None, thrd: (704, 16712), cmpt: ``, lut: None
[0m21:32:48.180313 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m21:32:48.181319 [debug] [MainThread]: Databricks adapter: Thread (704, 16712) using default compute resource.
[0m21:32:48.181319 [debug] [MainThread]: Databricks adapter: conn: 2083390859664: _acquire sess: None, name: debug, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (704, 16712), cmpt: ``, lut: 1707834768.18132
[0m21:32:48.182538 [debug] [MainThread]: Databricks adapter: conn: 2083390859664: get_thread_connection: sess: None, name: debug, idle: 0.0012187957763671875s, acqrelcnt: 1, lang: None, thrd: (704, 16712), cmpt: ``, lut: 1707834768.18132
[0m21:32:48.183046 [debug] [MainThread]: Databricks adapter: conn: 2083390859664: idle check connection: sess: None, name: debug, idle: 0.0017268657684326172s, acqrelcnt: 1, lang: None, thrd: (704, 16712), cmpt: ``, lut: 1707834768.18132
[0m21:32:48.183046 [debug] [MainThread]: Using databricks connection "debug"
[0m21:32:48.184130 [debug] [MainThread]: On debug: select 1 as id
[0m21:32:48.184130 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:32:48.764776 [debug] [MainThread]: Databricks adapter: conn: 2083390859664: session opened sess: 277e1bce-3a77-4e71-b0c5-31103505d19a, name: debug, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (704, 16712), cmpt: ``, lut: 1707834768.7647762
[0m21:32:49.080993 [debug] [MainThread]: SQL status: OK in 0.8999999761581421 seconds
[0m21:32:49.082093 [debug] [MainThread]: Databricks adapter: conn: 2083390859664: _release sess: 277e1bce-3a77-4e71-b0c5-31103505d19a, name: debug, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (704, 16712), cmpt: ``, lut: 1707834769.0820932
[0m21:32:49.082093 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m21:32:49.083115 [info ] [MainThread]: [32mAll checks passed![0m
[0m21:32:49.084094 [debug] [MainThread]: Command `dbt debug` succeeded at 21:32:49.084094 after 2.80 seconds
[0m21:32:49.085112 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m21:32:49.085112 [debug] [MainThread]: On debug: Close
[0m21:32:49.273163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E501516C50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E5018F2CD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E513BCB290>]}
[0m21:32:49.274163 [debug] [MainThread]: Flushing usage events
[0m21:32:57.486176 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001952514E150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019524F9B150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019524DDCA50>]}


============================== 21:32:57.490176 | 84f8e42c-ea9d-4b00-8fe0-f07824e392f7 ==============================
[0m21:32:57.490176 [info ] [MainThread]: Running with dbt=1.7.7
[0m21:32:57.491201 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt snapshot', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m21:32:58.914493 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '84f8e42c-ea9d-4b00-8fe0-f07824e392f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019537352D90>]}
[0m21:32:58.983290 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '84f8e42c-ea9d-4b00-8fe0-f07824e392f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001953726E4D0>]}
[0m21:32:58.984297 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m21:32:58.995277 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m21:32:59.112488 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:32:59.113489 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:32:59.119487 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '84f8e42c-ea9d-4b00-8fe0-f07824e392f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195375738D0>]}
[0m21:32:59.133805 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '84f8e42c-ea9d-4b00-8fe0-f07824e392f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019537451C50>]}
[0m21:32:59.134806 [info ] [MainThread]: Found 2 models, 4 tests, 9 sources, 0 exposures, 0 metrics, 535 macros, 0 groups, 0 semantic models
[0m21:32:59.135502 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '84f8e42c-ea9d-4b00-8fe0-f07824e392f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195373C4390>]}
[0m21:32:59.137586 [info ] [MainThread]: 
[0m21:32:59.138509 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m21:32:59.139509 [debug] [MainThread]: Command end result
[0m21:32:59.147513 [debug] [MainThread]: Command `dbt snapshot` succeeded at 21:32:59.147513 after 1.70 seconds
[0m21:32:59.148592 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019525179F10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001952481A150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001951DA48DD0>]}
[0m21:32:59.148592 [debug] [MainThread]: Flushing usage events
[0m21:57:50.068908 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC01A93910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC01A74950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC0161D690>]}


============================== 21:57:50.074882 | 037efa23-f86e-4e0b-b81e-6d4befcdc1be ==============================
[0m21:57:50.074882 [info ] [MainThread]: Running with dbt=1.7.7
[0m21:57:50.075883 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt snapshot', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m21:57:51.732968 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '037efa23-f86e-4e0b-b81e-6d4befcdc1be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC13BD3FD0>]}
[0m21:57:51.801412 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '037efa23-f86e-4e0b-b81e-6d4befcdc1be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC13B11C10>]}
[0m21:57:51.801412 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m21:57:51.815305 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m21:57:51.960335 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 7 files changed.
[0m21:57:51.960335 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://snapshots\productmodel.sql
[0m21:57:51.961335 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://snapshots\address.sql
[0m21:57:51.961335 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://snapshots\customeraddress.sql
[0m21:57:51.962336 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://snapshots\salesorderheader.sql
[0m21:57:51.962336 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://snapshots\salesorderdetail.sql
[0m21:57:51.963334 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://snapshots\product.sql
[0m21:57:51.963334 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://snapshots\customer.sql
[0m21:57:52.139096 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '037efa23-f86e-4e0b-b81e-6d4befcdc1be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC13F82190>]}
[0m21:57:52.154234 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '037efa23-f86e-4e0b-b81e-6d4befcdc1be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC13FCEB10>]}
[0m21:57:52.155220 [info ] [MainThread]: Found 2 models, 4 tests, 7 snapshots, 9 sources, 0 exposures, 0 metrics, 535 macros, 0 groups, 0 semantic models
[0m21:57:52.156228 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '037efa23-f86e-4e0b-b81e-6d4befcdc1be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC13A3EB10>]}
[0m21:57:52.158497 [info ] [MainThread]: 
[0m21:57:52.160400 [debug] [MainThread]: Databricks adapter: conn: 2113456038160: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (25040, 11312), cmpt: ``, lut: None
[0m21:57:52.160400 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:57:52.160400 [debug] [MainThread]: Databricks adapter: Thread (25040, 11312) using default compute resource.
[0m21:57:52.161398 [debug] [MainThread]: Databricks adapter: conn: 2113456038160: _acquire sess: None, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (25040, 11312), cmpt: ``, lut: 1707836272.161399
[0m21:57:52.163494 [debug] [ThreadPool]: Databricks adapter: conn: 2113459250960: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (25040, 18520), cmpt: ``, lut: None
[0m21:57:52.163494 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m21:57:52.164508 [debug] [ThreadPool]: Databricks adapter: Thread (25040, 18520) using default compute resource.
[0m21:57:52.164508 [debug] [ThreadPool]: Databricks adapter: conn: 2113459250960: _acquire sess: None, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (25040, 18520), cmpt: ``, lut: 1707836272.164508
[0m21:57:52.165403 [debug] [ThreadPool]: Databricks adapter: conn: 2113459250960: get_thread_connection: sess: None, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (25040, 18520), cmpt: ``, lut: 1707836272.164508
[0m21:57:52.165403 [debug] [ThreadPool]: Databricks adapter: conn: 2113459250960: idle check connection: sess: None, name: list_hive_metastore, idle: 0.0008950233459472656s, acqrelcnt: 1, lang: None, thrd: (25040, 18520), cmpt: ``, lut: 1707836272.164508
[0m21:57:52.165403 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m21:57:52.166399 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m21:57:52.166399 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:57:52.729557 [debug] [ThreadPool]: Databricks adapter: conn: 2113459250960: session opened sess: 11e03981-b60d-4fc6-812a-1640af3267bc, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (25040, 18520), cmpt: ``, lut: 1707836272.729557
[0m21:57:53.108060 [debug] [ThreadPool]: SQL status: OK in 0.9399999976158142 seconds
[0m21:57:53.126875 [debug] [ThreadPool]: Databricks adapter: conn: 2113459250960: _release sess: 11e03981-b60d-4fc6-812a-1640af3267bc, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (25040, 18520), cmpt: ``, lut: 1707836273.126875
[0m21:57:53.128900 [debug] [ThreadPool]: Databricks adapter: conn: 2113459250960: idle check connection: sess: 11e03981-b60d-4fc6-812a-1640af3267bc, name: list_hive_metastore, idle: 0.0010190010070800781s, acqrelcnt: 0, lang: None, thrd: (25040, 18520), cmpt: ``, lut: 1707836273.126875
[0m21:57:53.128900 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_snapshots)
[0m21:57:53.129876 [debug] [ThreadPool]: Databricks adapter: conn: 2113459250960: reusing connection list_hive_metastore sess: 11e03981-b60d-4fc6-812a-1640af3267bc, name: create_hive_metastore_snapshots, idle: 0.002025604248046875s, acqrelcnt: 0, lang: None, thrd: (25040, 18520), cmpt: ``, lut: 1707836273.126875
[0m21:57:53.129876 [debug] [ThreadPool]: Databricks adapter: Thread (25040, 18520) using default compute resource.
[0m21:57:53.129876 [debug] [ThreadPool]: Databricks adapter: conn: 2113459250960: _acquire sess: 11e03981-b60d-4fc6-812a-1640af3267bc, name: create_hive_metastore_snapshots, idle: 0.0030019283294677734s, acqrelcnt: 1, lang: None, thrd: (25040, 18520), cmpt: ``, lut: 1707836273.126875
[0m21:57:53.130874 [debug] [ThreadPool]: Databricks adapter: conn: 2113459250960: idle check connection: sess: 11e03981-b60d-4fc6-812a-1640af3267bc, name: create_hive_metastore_snapshots, idle: 0.003999948501586914s, acqrelcnt: 1, lang: None, thrd: (25040, 18520), cmpt: ``, lut: 1707836273.126875
[0m21:57:53.130874 [debug] [ThreadPool]: Databricks adapter: Thread (25040, 18520) using default compute resource.
[0m21:57:53.131995 [debug] [ThreadPool]: Databricks adapter: conn: 2113459250960: _acquire sess: 11e03981-b60d-4fc6-812a-1640af3267bc, name: create_hive_metastore_snapshots, idle: 0.005120038986206055s, acqrelcnt: 2, lang: None, thrd: (25040, 18520), cmpt: ``, lut: 1707836273.126875
[0m21:57:53.131995 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "snapshots"
"
[0m21:57:53.140548 [debug] [ThreadPool]: Databricks adapter: conn: 2113459250960: get_thread_connection: sess: 11e03981-b60d-4fc6-812a-1640af3267bc, name: create_hive_metastore_snapshots, idle: 0.013673067092895508s, acqrelcnt: 2, lang: None, thrd: (25040, 18520), cmpt: ``, lut: 1707836273.126875
[0m21:57:53.140548 [debug] [ThreadPool]: Databricks adapter: conn: 2113459250960: idle check connection: sess: 11e03981-b60d-4fc6-812a-1640af3267bc, name: create_hive_metastore_snapshots, idle: 0.013673067092895508s, acqrelcnt: 2, lang: None, thrd: (25040, 18520), cmpt: ``, lut: 1707836273.126875
[0m21:57:53.141547 [debug] [ThreadPool]: Databricks adapter: conn: 2113459250960: get_thread_connection: sess: 11e03981-b60d-4fc6-812a-1640af3267bc, name: create_hive_metastore_snapshots, idle: 0.013673067092895508s, acqrelcnt: 2, lang: None, thrd: (25040, 18520), cmpt: ``, lut: 1707836273.126875
[0m21:57:53.141547 [debug] [ThreadPool]: Databricks adapter: conn: 2113459250960: idle check connection: sess: 11e03981-b60d-4fc6-812a-1640af3267bc, name: create_hive_metastore_snapshots, idle: 0.014672040939331055s, acqrelcnt: 2, lang: None, thrd: (25040, 18520), cmpt: ``, lut: 1707836273.126875
[0m21:57:53.141547 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m21:57:53.142546 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_snapshots"
[0m21:57:53.142546 [debug] [ThreadPool]: On create_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "create_hive_metastore_snapshots"} */
create schema if not exists `hive_metastore`.`snapshots`
  
[0m21:57:53.500663 [debug] [ThreadPool]: SQL status: OK in 0.36000001430511475 seconds
[0m21:57:53.502664 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m21:57:53.503665 [debug] [ThreadPool]: Databricks adapter: conn: 2113459250960: _release sess: 11e03981-b60d-4fc6-812a-1640af3267bc, name: create_hive_metastore_snapshots, idle: 0.37679100036621094s, acqrelcnt: 1, lang: None, thrd: (25040, 18520), cmpt: ``, lut: 1707836273.126875
[0m21:57:53.504628 [debug] [ThreadPool]: Databricks adapter: conn: 2113459250960: _release sess: 11e03981-b60d-4fc6-812a-1640af3267bc, name: create_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (25040, 18520), cmpt: ``, lut: 1707836273.504629
[0m21:57:53.507688 [debug] [ThreadPool]: Databricks adapter: conn: 2113459248784: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_saleslt, idle: 0s, acqrelcnt: 0, lang: None, thrd: (25040, 10632), cmpt: ``, lut: None
[0m21:57:53.507688 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m21:57:53.508691 [debug] [ThreadPool]: Databricks adapter: Thread (25040, 10632) using default compute resource.
[0m21:57:53.508691 [debug] [ThreadPool]: Databricks adapter: conn: 2113459248784: _acquire sess: None, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (25040, 10632), cmpt: ``, lut: 1707836273.5086908
[0m21:57:53.512764 [debug] [ThreadPool]: Databricks adapter: conn: 2113459248784: get_thread_connection: sess: None, name: list_hive_metastore_saleslt, idle: 0.0040738582611083984s, acqrelcnt: 1, lang: None, thrd: (25040, 10632), cmpt: ``, lut: 1707836273.5086908
[0m21:57:53.514315 [debug] [ThreadPool]: Databricks adapter: conn: 2113459248784: idle check connection: sess: None, name: list_hive_metastore_saleslt, idle: 0.005624532699584961s, acqrelcnt: 1, lang: None, thrd: (25040, 10632), cmpt: ``, lut: 1707836273.5086908
[0m21:57:53.514315 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m21:57:53.515322 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m21:57:53.515322 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:57:53.811614 [debug] [ThreadPool]: Databricks adapter: conn: 2113459248784: session opened sess: ef4ee823-f146-4c9e-a71c-9d87d7ab5492, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (25040, 10632), cmpt: ``, lut: 1707836273.8106098
[0m21:57:54.202272 [debug] [ThreadPool]: SQL status: OK in 0.6899999976158142 seconds
[0m21:57:54.213982 [debug] [ThreadPool]: Databricks adapter: conn: 2113459248784: get_thread_connection: sess: ef4ee823-f146-4c9e-a71c-9d87d7ab5492, name: list_hive_metastore_saleslt, idle: 0.40337300300598145s, acqrelcnt: 1, lang: None, thrd: (25040, 10632), cmpt: ``, lut: 1707836273.8106098
[0m21:57:54.214950 [debug] [ThreadPool]: Databricks adapter: conn: 2113459248784: idle check connection: sess: ef4ee823-f146-4c9e-a71c-9d87d7ab5492, name: list_hive_metastore_saleslt, idle: 0.40337300300598145s, acqrelcnt: 1, lang: None, thrd: (25040, 10632), cmpt: ``, lut: 1707836273.8106098
[0m21:57:54.214950 [debug] [ThreadPool]: Databricks adapter: conn: 2113459248784: get_thread_connection: sess: ef4ee823-f146-4c9e-a71c-9d87d7ab5492, name: list_hive_metastore_saleslt, idle: 0.4043405055999756s, acqrelcnt: 1, lang: None, thrd: (25040, 10632), cmpt: ``, lut: 1707836273.8106098
[0m21:57:54.215901 [debug] [ThreadPool]: Databricks adapter: conn: 2113459248784: idle check connection: sess: ef4ee823-f146-4c9e-a71c-9d87d7ab5492, name: list_hive_metastore_saleslt, idle: 0.4043405055999756s, acqrelcnt: 1, lang: None, thrd: (25040, 10632), cmpt: ``, lut: 1707836273.8106098
[0m21:57:54.215901 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m21:57:54.215901 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m21:57:54.216947 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m21:57:54.535168 [debug] [ThreadPool]: SQL status: OK in 0.3199999928474426 seconds
[0m21:57:54.542363 [debug] [ThreadPool]: Databricks adapter: conn: 2113459248784: get_thread_connection: sess: ef4ee823-f146-4c9e-a71c-9d87d7ab5492, name: list_hive_metastore_saleslt, idle: 0.7317535877227783s, acqrelcnt: 1, lang: None, thrd: (25040, 10632), cmpt: ``, lut: 1707836273.8106098
[0m21:57:54.543362 [debug] [ThreadPool]: Databricks adapter: conn: 2113459248784: idle check connection: sess: ef4ee823-f146-4c9e-a71c-9d87d7ab5492, name: list_hive_metastore_saleslt, idle: 0.7327523231506348s, acqrelcnt: 1, lang: None, thrd: (25040, 10632), cmpt: ``, lut: 1707836273.8106098
[0m21:57:54.543362 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m21:57:54.544298 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m21:57:54.772626 [debug] [ThreadPool]: SQL status: OK in 0.23000000417232513 seconds
[0m21:57:54.775779 [debug] [ThreadPool]: Databricks adapter: conn: 2113459248784: _release sess: ef4ee823-f146-4c9e-a71c-9d87d7ab5492, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (25040, 10632), cmpt: ``, lut: 1707836274.7757792
[0m21:57:54.776836 [debug] [ThreadPool]: Databricks adapter: conn: 2113459248784: idle check connection: sess: ef4ee823-f146-4c9e-a71c-9d87d7ab5492, name: list_hive_metastore_saleslt, idle: 0.0010569095611572266s, acqrelcnt: 0, lang: None, thrd: (25040, 10632), cmpt: ``, lut: 1707836274.7757792
[0m21:57:54.779796 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m21:57:54.779796 [debug] [ThreadPool]: Databricks adapter: conn: 2113459248784: reusing connection list_hive_metastore_saleslt sess: ef4ee823-f146-4c9e-a71c-9d87d7ab5492, name: list_hive_metastore_snapshots, idle: 0.0040171146392822266s, acqrelcnt: 0, lang: None, thrd: (25040, 10632), cmpt: ``, lut: 1707836274.7757792
[0m21:57:54.780812 [debug] [ThreadPool]: Databricks adapter: Thread (25040, 10632) using default compute resource.
[0m21:57:54.780812 [debug] [ThreadPool]: Databricks adapter: conn: 2113459248784: _acquire sess: ef4ee823-f146-4c9e-a71c-9d87d7ab5492, name: list_hive_metastore_snapshots, idle: 0.005033016204833984s, acqrelcnt: 1, lang: None, thrd: (25040, 10632), cmpt: ``, lut: 1707836274.7757792
[0m21:57:54.783098 [debug] [ThreadPool]: Databricks adapter: conn: 2113459248784: get_thread_connection: sess: ef4ee823-f146-4c9e-a71c-9d87d7ab5492, name: list_hive_metastore_snapshots, idle: 0.00731968879699707s, acqrelcnt: 1, lang: None, thrd: (25040, 10632), cmpt: ``, lut: 1707836274.7757792
[0m21:57:54.783098 [debug] [ThreadPool]: Databricks adapter: conn: 2113459248784: idle check connection: sess: ef4ee823-f146-4c9e-a71c-9d87d7ab5492, name: list_hive_metastore_snapshots, idle: 0.00731968879699707s, acqrelcnt: 1, lang: None, thrd: (25040, 10632), cmpt: ``, lut: 1707836274.7757792
[0m21:57:54.784059 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m21:57:54.784059 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m21:57:54.921960 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m21:57:54.926344 [debug] [ThreadPool]: Databricks adapter: conn: 2113459248784: _release sess: ef4ee823-f146-4c9e-a71c-9d87d7ab5492, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (25040, 10632), cmpt: ``, lut: 1707836274.9263446
[0m21:57:54.927344 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '037efa23-f86e-4e0b-b81e-6d4befcdc1be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC13E82310>]}
[0m21:57:54.928343 [debug] [MainThread]: Databricks adapter: conn: 2113456038160: get_thread_connection: sess: None, name: master, idle: 2.766944646835327s, acqrelcnt: 1, lang: None, thrd: (25040, 11312), cmpt: ``, lut: 1707836272.161399
[0m21:57:54.929344 [debug] [MainThread]: Databricks adapter: conn: 2113456038160: idle check connection: sess: None, name: master, idle: 2.766944646835327s, acqrelcnt: 1, lang: None, thrd: (25040, 11312), cmpt: ``, lut: 1707836272.161399
[0m21:57:54.929344 [debug] [MainThread]: Databricks adapter: conn: 2113456038160: get_thread_connection: sess: None, name: master, idle: 2.7679455280303955s, acqrelcnt: 1, lang: None, thrd: (25040, 11312), cmpt: ``, lut: 1707836272.161399
[0m21:57:54.929344 [debug] [MainThread]: Databricks adapter: conn: 2113456038160: idle check connection: sess: None, name: master, idle: 2.7679455280303955s, acqrelcnt: 1, lang: None, thrd: (25040, 11312), cmpt: ``, lut: 1707836272.161399
[0m21:57:54.930343 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:57:54.930343 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:57:54.931343 [debug] [MainThread]: Databricks adapter: conn: 2113456038160: _release sess: None, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (25040, 11312), cmpt: ``, lut: 1707836274.9303434
[0m21:57:54.931549 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:57:54.932554 [info ] [MainThread]: 
[0m21:57:55.002988 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.address_snapshot
[0m21:57:55.004989 [info ] [Thread-1 (]: 1 of 7 START snapshot snapshots.address_snapshot ............................... [RUN]
[0m21:57:55.006988 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: Creating DatabricksDBTConnection sess: None, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 0s, acqrelcnt: 0, lang: None, thrd: (25040, 6908), cmpt: ``, lut: None
[0m21:57:55.007990 [debug] [Thread-1 (]: Acquiring new databricks connection 'snapshot.medallion_dbt_spark.address_snapshot'
[0m21:57:55.008989 [debug] [Thread-1 (]: Databricks adapter: On thread (25040, 6908): `hive_metastore`.`snapshots`.`address_snapshot` using default compute resource.
[0m21:57:55.008989 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: _acquire sess: None, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836275.008989
[0m21:57:55.009989 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.address_snapshot
[0m21:57:55.019987 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.address_snapshot (compile): 21:57:55.009989 => 21:57:55.019987
[0m21:57:55.020989 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.address_snapshot
[0m21:57:55.127553 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.medallion_dbt_spark.address_snapshot"
[0m21:57:55.131622 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: get_thread_connection: sess: None, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 0.12167882919311523s, acqrelcnt: 1, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836275.008989
[0m21:57:55.131622 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: idle check connection: sess: None, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 0.12263298034667969s, acqrelcnt: 1, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836275.008989
[0m21:57:55.132731 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: get_thread_connection: sess: None, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 0.12374281883239746s, acqrelcnt: 1, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836275.008989
[0m21:57:55.132731 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: idle check connection: sess: None, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 0.12374281883239746s, acqrelcnt: 1, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836275.008989
[0m21:57:55.133682 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:57:55.133682 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.address_snapshot"
[0m21:57:55.134620 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.address_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`address_snapshot`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/address/address_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(AddressID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with source_data as (
    select 
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data
    ) sbq



  
      
[0m21:57:55.134620 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m21:57:55.419187 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: session opened sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836275.418185
[0m21:58:09.400697 [debug] [Thread-1 (]: SQL status: OK in 14.260000228881836 seconds
[0m21:58:09.591616 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m21:58:09.592544 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.address_snapshot (execute): 21:57:55.020989 => 21:58:09.592544
[0m21:58:09.593539 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: _release sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836289.592545
[0m21:58:09.593539 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: _release sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836289.593539
[0m21:58:09.594634 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '037efa23-f86e-4e0b-b81e-6d4befcdc1be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC13FD4210>]}
[0m21:58:09.594634 [info ] [Thread-1 (]: 1 of 7 OK snapshotted snapshots.address_snapshot ............................... [[32mOK[0m in 14.59s]
[0m21:58:09.595628 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.address_snapshot
[0m21:58:09.596568 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.customer_snapshot
[0m21:58:09.596568 [info ] [Thread-1 (]: 2 of 7 START snapshot snapshots.customer_snapshot .............................. [RUN]
[0m21:58:09.598540 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: idle check connection: sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 0.00500178337097168s, acqrelcnt: 0, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836289.593539
[0m21:58:09.598540 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.address_snapshot, now snapshot.medallion_dbt_spark.customer_snapshot)
[0m21:58:09.599750 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: reusing connection snapshot.medallion_dbt_spark.address_snapshot sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 0.0060040950775146484s, acqrelcnt: 0, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836289.593539
[0m21:58:09.599750 [debug] [Thread-1 (]: Databricks adapter: On thread (25040, 6908): `hive_metastore`.`snapshots`.`customer_snapshot` using default compute resource.
[0m21:58:09.599750 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: _acquire sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 0.006211519241333008s, acqrelcnt: 1, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836289.593539
[0m21:58:09.600756 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.customer_snapshot
[0m21:58:09.603841 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.customer_snapshot (compile): 21:58:09.600756 => 21:58:09.603841
[0m21:58:09.604861 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.customer_snapshot
[0m21:58:09.610107 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.medallion_dbt_spark.customer_snapshot"
[0m21:58:09.614482 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: get_thread_connection: sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 0.0199429988861084s, acqrelcnt: 1, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836289.593539
[0m21:58:09.615486 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: idle check connection: sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 0.020943164825439453s, acqrelcnt: 1, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836289.593539
[0m21:58:09.615486 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customer_snapshot"
[0m21:58:09.616493 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customer_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`customer_snapshot`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/customer_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(CustomerId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with source_data as (
    select 
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data
    ) sbq



  
      
[0m21:58:14.308825 [debug] [Thread-1 (]: SQL status: OK in 4.690000057220459 seconds
[0m21:58:14.310822 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m21:58:14.312741 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.customer_snapshot (execute): 21:58:09.604861 => 21:58:14.311820
[0m21:58:14.312741 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: _release sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836294.312741
[0m21:58:14.313738 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: _release sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836294.3137386
[0m21:58:14.313738 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '037efa23-f86e-4e0b-b81e-6d4befcdc1be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC141C9C50>]}
[0m21:58:14.314738 [info ] [Thread-1 (]: 2 of 7 OK snapshotted snapshots.customer_snapshot .............................. [[32mOK[0m in 4.72s]
[0m21:58:14.315740 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.customer_snapshot
[0m21:58:14.316749 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m21:58:14.316749 [info ] [Thread-1 (]: 3 of 7 START snapshot snapshots.customeraddress_snapshot ....................... [RUN]
[0m21:58:14.319192 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: idle check connection: sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 0.005454301834106445s, acqrelcnt: 0, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836294.3137386
[0m21:58:14.319192 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.customer_snapshot, now snapshot.medallion_dbt_spark.customeraddress_snapshot)
[0m21:58:14.320184 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: reusing connection snapshot.medallion_dbt_spark.customer_snapshot sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 0.006445407867431641s, acqrelcnt: 0, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836294.3137386
[0m21:58:14.320184 [debug] [Thread-1 (]: Databricks adapter: On thread (25040, 6908): `hive_metastore`.`snapshots`.`customeraddress_snapshot` using default compute resource.
[0m21:58:14.321219 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: _acquire sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 0.007481098175048828s, acqrelcnt: 1, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836294.3137386
[0m21:58:14.321219 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m21:58:14.325289 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.customeraddress_snapshot (compile): 21:58:14.321219 => 21:58:14.325289
[0m21:58:14.326189 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m21:58:14.332650 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.medallion_dbt_spark.customeraddress_snapshot"
[0m21:58:14.336285 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: get_thread_connection: sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 0.02254652976989746s, acqrelcnt: 1, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836294.3137386
[0m21:58:14.337292 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: idle check connection: sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 0.02355360984802246s, acqrelcnt: 1, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836294.3137386
[0m21:58:14.337292 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customeraddress_snapshot"
[0m21:58:14.338292 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customeraddress_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customeraddress/customeraddress_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(CustomerId||'-'||AddressID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with source_data as (
    select 
        CustomerID,
        AddressID,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data
    ) sbq



  
      
[0m21:58:18.070009 [debug] [Thread-1 (]: SQL status: OK in 3.7300000190734863 seconds
[0m21:58:18.074926 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m21:58:18.077036 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.customeraddress_snapshot (execute): 21:58:14.326189 => 21:58:18.077036
[0m21:58:18.078041 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: _release sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836298.0770369
[0m21:58:18.078041 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: _release sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836298.0780416
[0m21:58:18.079016 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '037efa23-f86e-4e0b-b81e-6d4befcdc1be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC1402C310>]}
[0m21:58:18.079973 [info ] [Thread-1 (]: 3 of 7 OK snapshotted snapshots.customeraddress_snapshot ....................... [[32mOK[0m in 3.76s]
[0m21:58:18.081023 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m21:58:18.081023 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.product_snapshot
[0m21:58:18.082274 [info ] [Thread-1 (]: 4 of 7 START snapshot snapshots.product_snapshot ............................... [RUN]
[0m21:58:18.082997 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: idle check connection: sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 0.00495600700378418s, acqrelcnt: 0, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836298.0780416
[0m21:58:18.084078 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.customeraddress_snapshot, now snapshot.medallion_dbt_spark.product_snapshot)
[0m21:58:18.084078 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: reusing connection snapshot.medallion_dbt_spark.customeraddress_snapshot sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 0.006036520004272461s, acqrelcnt: 0, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836298.0780416
[0m21:58:18.085037 [debug] [Thread-1 (]: Databricks adapter: On thread (25040, 6908): `hive_metastore`.`snapshots`.`product_snapshot` using default compute resource.
[0m21:58:18.085037 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: _acquire sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 0.00699615478515625s, acqrelcnt: 1, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836298.0780416
[0m21:58:18.086007 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.product_snapshot
[0m21:58:18.089021 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.product_snapshot (compile): 21:58:18.086007 => 21:58:18.089021
[0m21:58:18.090092 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.product_snapshot
[0m21:58:18.098011 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.medallion_dbt_spark.product_snapshot"
[0m21:58:18.100005 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: get_thread_connection: sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 0.021964311599731445s, acqrelcnt: 1, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836298.0780416
[0m21:58:18.101005 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: idle check connection: sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 0.0229642391204834s, acqrelcnt: 1, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836298.0780416
[0m21:58:18.102012 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.product_snapshot"
[0m21:58:18.102012 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.product_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`product_snapshot`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/product/product_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(ProductId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with source_data as (
    select 
        ProductId,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    from `hive_metastore`.`saleslt`.`product`
)
select *
from source_data
    ) sbq



  
      
[0m21:58:22.263195 [debug] [Thread-1 (]: SQL status: OK in 4.159999847412109 seconds
[0m21:58:22.268192 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m21:58:22.269114 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.product_snapshot (execute): 21:58:18.090092 => 21:58:22.269114
[0m21:58:22.270115 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: _release sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836302.2701159
[0m21:58:22.271132 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: _release sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836302.2711322
[0m21:58:22.272194 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '037efa23-f86e-4e0b-b81e-6d4befcdc1be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC1421EC50>]}
[0m21:58:22.273144 [info ] [Thread-1 (]: 4 of 7 OK snapshotted snapshots.product_snapshot ............................... [[32mOK[0m in 4.19s]
[0m21:58:22.274118 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.product_snapshot
[0m21:58:22.274118 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.productmodel_snapshot
[0m21:58:22.275237 [info ] [Thread-1 (]: 5 of 7 START snapshot snapshots.productmodel_snapshot .......................... [RUN]
[0m21:58:22.276241 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: idle check connection: sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 0.005109548568725586s, acqrelcnt: 0, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836302.2711322
[0m21:58:22.277316 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.product_snapshot, now snapshot.medallion_dbt_spark.productmodel_snapshot)
[0m21:58:22.277316 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: reusing connection snapshot.medallion_dbt_spark.product_snapshot sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 0.00618433952331543s, acqrelcnt: 0, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836302.2711322
[0m21:58:22.278245 [debug] [Thread-1 (]: Databricks adapter: On thread (25040, 6908): `hive_metastore`.`snapshots`.`productmodel_snapshot` using default compute resource.
[0m21:58:22.278245 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: _acquire sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 0.007112979888916016s, acqrelcnt: 1, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836302.2711322
[0m21:58:22.278245 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.productmodel_snapshot
[0m21:58:22.361855 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.productmodel_snapshot (compile): 21:58:22.279248 => 21:58:22.360865
[0m21:58:22.361855 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.productmodel_snapshot
[0m21:58:22.368770 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.medallion_dbt_spark.productmodel_snapshot"
[0m21:58:22.369777 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: get_thread_connection: sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 0.09864521026611328s, acqrelcnt: 1, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836302.2711322
[0m21:58:22.370776 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: idle check connection: sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 0.09964418411254883s, acqrelcnt: 1, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836302.2711322
[0m21:58:22.370776 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.productmodel_snapshot"
[0m21:58:22.371780 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.productmodel_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`productmodel_snapshot`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/productmodel/productmodel_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(ProductModelID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with source_data as (
    select 
        ProductModelID,
        Name,
        CatalogDescription
    from `hive_metastore`.`saleslt`.`productmodel`
)
select *
from source_data
    ) sbq



  
      
[0m21:58:26.118991 [debug] [Thread-1 (]: SQL status: OK in 3.75 seconds
[0m21:58:26.122991 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m21:58:26.124245 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.productmodel_snapshot (execute): 21:58:22.362809 => 21:58:26.124245
[0m21:58:26.124245 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: _release sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836306.1242454
[0m21:58:26.125247 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: _release sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836306.1252472
[0m21:58:26.126247 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '037efa23-f86e-4e0b-b81e-6d4befcdc1be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC13B64310>]}
[0m21:58:26.127246 [info ] [Thread-1 (]: 5 of 7 OK snapshotted snapshots.productmodel_snapshot .......................... [[32mOK[0m in 3.85s]
[0m21:58:26.128247 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.productmodel_snapshot
[0m21:58:26.129248 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m21:58:26.129248 [info ] [Thread-1 (]: 6 of 7 START snapshot snapshots.salesorderdetail_snapshot ...................... [RUN]
[0m21:58:26.131247 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: idle check connection: sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 0.006000041961669922s, acqrelcnt: 0, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836306.1252472
[0m21:58:26.132249 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.productmodel_snapshot, now snapshot.medallion_dbt_spark.salesorderdetail_snapshot)
[0m21:58:26.132413 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: reusing connection snapshot.medallion_dbt_spark.productmodel_snapshot sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 0.0071659088134765625s, acqrelcnt: 0, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836306.1252472
[0m21:58:26.133417 [debug] [Thread-1 (]: Databricks adapter: On thread (25040, 6908): `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` using default compute resource.
[0m21:58:26.133417 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: _acquire sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 0.008169889450073242s, acqrelcnt: 1, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836306.1252472
[0m21:58:26.134415 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m21:58:26.139539 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.salesorderdetail_snapshot (compile): 21:58:26.134415 => 21:58:26.139539
[0m21:58:26.140539 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m21:58:26.150002 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"
[0m21:58:26.152006 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: get_thread_connection: sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 0.02575993537902832s, acqrelcnt: 1, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836306.1252472
[0m21:58:26.152006 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: idle check connection: sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 0.026758909225463867s, acqrelcnt: 1, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836306.1252472
[0m21:58:26.153445 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"
[0m21:58:26.153445 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/salesorderdetail/salesorderdetail_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(SalesOrderDetailID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with source_data as (
    select 
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    from `hive_metastore`.`saleslt`.`salesorderdetail`
)
select *
from source_data
    ) sbq



  
      
[0m21:58:29.668630 [debug] [Thread-1 (]: SQL status: OK in 3.509999990463257 seconds
[0m21:58:29.670668 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m21:58:29.671659 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.salesorderdetail_snapshot (execute): 21:58:26.140539 => 21:58:29.671659
[0m21:58:29.672592 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: _release sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836309.6725924
[0m21:58:29.673606 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: _release sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836309.6736066
[0m21:58:29.673606 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '037efa23-f86e-4e0b-b81e-6d4befcdc1be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC14213FD0>]}
[0m21:58:29.674836 [info ] [Thread-1 (]: 6 of 7 OK snapshotted snapshots.salesorderdetail_snapshot ...................... [[32mOK[0m in 3.54s]
[0m21:58:29.676839 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m21:58:29.676839 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.salesorderheader_snapshot
[0m21:58:29.677833 [info ] [Thread-1 (]: 7 of 7 START snapshot snapshots.salesorderheader_snapshot ...................... [RUN]
[0m21:58:29.679711 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: idle check connection: sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 0.005098104476928711s, acqrelcnt: 0, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836309.6736066
[0m21:58:29.679711 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.salesorderdetail_snapshot, now snapshot.medallion_dbt_spark.salesorderheader_snapshot)
[0m21:58:29.680711 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: reusing connection snapshot.medallion_dbt_spark.salesorderdetail_snapshot sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle: 0.006104946136474609s, acqrelcnt: 0, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836309.6736066
[0m21:58:29.682786 [debug] [Thread-1 (]: Databricks adapter: On thread (25040, 6908): `hive_metastore`.`snapshots`.`salesorderheader_snapshot` using default compute resource.
[0m21:58:29.683797 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: _acquire sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle: 0.010190725326538086s, acqrelcnt: 1, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836309.6736066
[0m21:58:29.683797 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.salesorderheader_snapshot
[0m21:58:29.688785 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.salesorderheader_snapshot (compile): 21:58:29.684716 => 21:58:29.687711
[0m21:58:29.688785 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.salesorderheader_snapshot
[0m21:58:29.695711 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.medallion_dbt_spark.salesorderheader_snapshot"
[0m21:58:29.697467 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: get_thread_connection: sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle: 0.023860692977905273s, acqrelcnt: 1, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836309.6736066
[0m21:58:29.698480 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: idle check connection: sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle: 0.02487349510192871s, acqrelcnt: 1, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836309.6736066
[0m21:58:29.698480 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.salesorderheader_snapshot"
[0m21:58:29.699672 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.salesorderheader_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/salesorderheader/salesorderheader_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(SalesOrderId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with source_data as (
    select 
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        SalesPersonID,
        TerritoryID,
        BillToAddressID,
        ShipToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    from `hive_metastore`.`saleslt`.`salesorderheader`
)
select *
from source_data
    ) sbq



  
      
[0m21:58:30.065553 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.salesorderheader_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/salesorderheader/salesorderheader_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(SalesOrderId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with source_data as (
    select 
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        SalesPersonID,
        TerritoryID,
        BillToAddressID,
        ShipToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    from `hive_metastore`.`saleslt`.`salesorderheader`
)
select *
from source_data
    ) sbq



  
      
[0m21:58:30.067555 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `SalesPersonID` cannot be resolved. Did you mean one of the following? [`SalesOrderID`, `Comment`, `CustomerID`, `OrderDate`, `SalesOrderNumber`].; line 56 pos 8
[0m21:58:30.068555 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `SalesPersonID` cannot be resolved. Did you mean one of the following? [`SalesOrderID`, `Comment`, `CustomerID`, `OrderDate`, `SalesOrderNumber`].; line 56 pos 8
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `SalesPersonID` cannot be resolved. Did you mean one of the following? [`SalesOrderID`, `Comment`, `CustomerID`, `OrderDate`, `SalesOrderNumber`].; line 56 pos 8
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:671)
	... 35 more

[0m21:58:30.069547 [debug] [Thread-1 (]: Databricks adapter: operation-id: 076f1314-6229-408e-9d38-3ec34d6063cd
[0m21:58:30.069547 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.salesorderheader_snapshot (execute): 21:58:29.689759 => 21:58:30.069547
[0m21:58:30.070546 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: _release sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836310.0705466
[0m21:58:30.141326 [debug] [Thread-1 (]: Runtime Error in snapshot salesorderheader_snapshot (snapshots\salesorderheader.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `SalesPersonID` cannot be resolved. Did you mean one of the following? [`SalesOrderID`, `Comment`, `CustomerID`, `OrderDate`, `SalesOrderNumber`].; line 56 pos 8
[0m21:58:30.142327 [debug] [Thread-1 (]: Databricks adapter: conn: 2113459028176: _release sess: aa949276-e70c-4ddd-b444-9e7261251b6d, name: snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (25040, 6908), cmpt: ``, lut: 1707836310.1413264
[0m21:58:30.142327 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '037efa23-f86e-4e0b-b81e-6d4befcdc1be', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC14178350>]}
[0m21:58:30.143256 [error] [Thread-1 (]: 7 of 7 ERROR snapshotting snapshots.salesorderheader_snapshot .................. [[31mERROR[0m in 0.46s]
[0m21:58:30.144261 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.salesorderheader_snapshot
[0m21:58:30.146264 [debug] [MainThread]: Databricks adapter: conn: 2113456038160: idle check connection: sess: None, name: master, idle: 35.21592092514038s, acqrelcnt: 0, lang: None, thrd: (25040, 11312), cmpt: ``, lut: 1707836274.9303434
[0m21:58:30.147274 [debug] [MainThread]: Databricks adapter: conn: 2113456038160: reusing connection master sess: None, name: master, idle: 35.216931104660034s, acqrelcnt: 0, lang: None, thrd: (25040, 11312), cmpt: ``, lut: 1707836274.9303434
[0m21:58:30.148256 [debug] [MainThread]: Databricks adapter: Thread (25040, 11312) using default compute resource.
[0m21:58:30.148256 [debug] [MainThread]: Databricks adapter: conn: 2113456038160: _acquire sess: None, name: master, idle: 35.21791315078735s, acqrelcnt: 1, lang: None, thrd: (25040, 11312), cmpt: ``, lut: 1707836274.9303434
[0m21:58:30.148256 [debug] [MainThread]: Databricks adapter: conn: 2113456038160: get_thread_connection: sess: None, name: master, idle: 35.21791315078735s, acqrelcnt: 1, lang: None, thrd: (25040, 11312), cmpt: ``, lut: 1707836274.9303434
[0m21:58:30.149480 [debug] [MainThread]: Databricks adapter: conn: 2113456038160: idle check connection: sess: None, name: master, idle: 35.2191367149353s, acqrelcnt: 1, lang: None, thrd: (25040, 11312), cmpt: ``, lut: 1707836274.9303434
[0m21:58:30.149480 [debug] [MainThread]: On master: ROLLBACK
[0m21:58:30.150480 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:58:30.391419 [debug] [MainThread]: Databricks adapter: conn: 2113456038160: session opened sess: 85311422-c7f1-4db8-b843-752bd1090b65, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (25040, 11312), cmpt: ``, lut: 1707836310.39142
[0m21:58:30.392417 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:58:30.393418 [debug] [MainThread]: Databricks adapter: conn: 2113456038160: get_thread_connection: sess: 85311422-c7f1-4db8-b843-752bd1090b65, name: master, idle: 0.001998424530029297s, acqrelcnt: 1, lang: None, thrd: (25040, 11312), cmpt: ``, lut: 1707836310.39142
[0m21:58:30.393418 [debug] [MainThread]: Databricks adapter: conn: 2113456038160: idle check connection: sess: 85311422-c7f1-4db8-b843-752bd1090b65, name: master, idle: 0.001998424530029297s, acqrelcnt: 1, lang: None, thrd: (25040, 11312), cmpt: ``, lut: 1707836310.39142
[0m21:58:30.394415 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:58:30.394415 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:58:30.395414 [debug] [MainThread]: Databricks adapter: conn: 2113456038160: _release sess: 85311422-c7f1-4db8-b843-752bd1090b65, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (25040, 11312), cmpt: ``, lut: 1707836310.3954148
[0m21:58:30.396419 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:58:30.397414 [debug] [MainThread]: On master: ROLLBACK
[0m21:58:30.397414 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:58:30.398414 [debug] [MainThread]: On master: Close
[0m21:58:30.521503 [debug] [MainThread]: Connection 'create_hive_metastore_snapshots' was properly closed.
[0m21:58:30.522507 [debug] [MainThread]: On create_hive_metastore_snapshots: ROLLBACK
[0m21:58:30.523508 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:58:30.524485 [debug] [MainThread]: On create_hive_metastore_snapshots: Close
[0m21:58:30.615748 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m21:58:30.617743 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m21:58:30.618741 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:58:30.619740 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m21:58:30.719905 [debug] [MainThread]: Connection 'snapshot.medallion_dbt_spark.salesorderheader_snapshot' was properly closed.
[0m21:58:30.719905 [debug] [MainThread]: On snapshot.medallion_dbt_spark.salesorderheader_snapshot: ROLLBACK
[0m21:58:30.720904 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:58:30.720904 [debug] [MainThread]: On snapshot.medallion_dbt_spark.salesorderheader_snapshot: Close
[0m21:58:30.809680 [info ] [MainThread]: 
[0m21:58:30.811607 [info ] [MainThread]: Finished running 7 snapshots in 0 hours 0 minutes and 38.65 seconds (38.65s).
[0m21:58:30.816679 [debug] [MainThread]: Command end result
[0m21:58:30.830997 [info ] [MainThread]: 
[0m21:58:30.832369 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:58:30.833371 [info ] [MainThread]: 
[0m21:58:30.834528 [error] [MainThread]:   Runtime Error in snapshot salesorderheader_snapshot (snapshots\salesorderheader.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `SalesPersonID` cannot be resolved. Did you mean one of the following? [`SalesOrderID`, `Comment`, `CustomerID`, `OrderDate`, `SalesOrderNumber`].; line 56 pos 8
[0m21:58:30.836613 [info ] [MainThread]: 
[0m21:58:30.837581 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=1 SKIP=0 TOTAL=7
[0m21:58:30.838585 [debug] [MainThread]: Command `dbt snapshot` failed at 21:58:30.838585 after 40.83 seconds
[0m21:58:30.839667 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC01A95510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC01756E50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC13A3EB10>]}
[0m21:58:30.839667 [debug] [MainThread]: Flushing usage events
[0m21:59:47.662603 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AA96141350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AA95AE6BD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AA961748D0>]}


============================== 21:59:47.679468 | b4c3fcbb-4f23-4946-8c48-087595f6626d ==============================
[0m21:59:47.679468 [info ] [MainThread]: Running with dbt=1.7.7
[0m21:59:47.679468 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt snapshot', 'send_anonymous_usage_stats': 'True'}
[0m21:59:48.946325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b4c3fcbb-4f23-4946-8c48-087595f6626d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAA8279610>]}
[0m21:59:49.013072 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b4c3fcbb-4f23-4946-8c48-087595f6626d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAA81D7710>]}
[0m21:59:49.013945 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m21:59:49.013945 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m21:59:49.099449 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:59:49.099449 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://snapshots\salesorderheader.sql
[0m21:59:49.270598 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b4c3fcbb-4f23-4946-8c48-087595f6626d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAA8610A90>]}
[0m21:59:49.286314 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b4c3fcbb-4f23-4946-8c48-087595f6626d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAA85FFE50>]}
[0m21:59:49.287364 [info ] [MainThread]: Found 2 models, 4 tests, 7 snapshots, 9 sources, 0 exposures, 0 metrics, 535 macros, 0 groups, 0 semantic models
[0m21:59:49.288764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b4c3fcbb-4f23-4946-8c48-087595f6626d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAA8257F10>]}
[0m21:59:49.290845 [info ] [MainThread]: 
[0m21:59:49.291775 [debug] [MainThread]: Databricks adapter: conn: 1832481329488: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (13924, 15656), cmpt: ``, lut: None
[0m21:59:49.292774 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:59:49.292774 [debug] [MainThread]: Databricks adapter: Thread (13924, 15656) using default compute resource.
[0m21:59:49.293721 [debug] [MainThread]: Databricks adapter: conn: 1832481329488: _acquire sess: None, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (13924, 15656), cmpt: ``, lut: 1707836389.2937212
[0m21:59:49.295747 [debug] [ThreadPool]: Databricks adapter: conn: 1832480929872: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (13924, 16740), cmpt: ``, lut: None
[0m21:59:49.295747 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m21:59:49.296739 [debug] [ThreadPool]: Databricks adapter: Thread (13924, 16740) using default compute resource.
[0m21:59:49.297301 [debug] [ThreadPool]: Databricks adapter: conn: 1832480929872: _acquire sess: None, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (13924, 16740), cmpt: ``, lut: 1707836389.2967396
[0m21:59:49.297819 [debug] [ThreadPool]: Databricks adapter: conn: 1832480929872: get_thread_connection: sess: None, name: list_hive_metastore, idle: 0.0010797977447509766s, acqrelcnt: 1, lang: None, thrd: (13924, 16740), cmpt: ``, lut: 1707836389.2967396
[0m21:59:49.298680 [debug] [ThreadPool]: Databricks adapter: conn: 1832480929872: idle check connection: sess: None, name: list_hive_metastore, idle: 0.0016224384307861328s, acqrelcnt: 1, lang: None, thrd: (13924, 16740), cmpt: ``, lut: 1707836389.2967396
[0m21:59:49.298680 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m21:59:49.298680 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m21:59:49.299875 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:59:49.648231 [debug] [ThreadPool]: Databricks adapter: conn: 1832480929872: session opened sess: fcf0bf82-fb37-40b4-947c-6df3a0785e6e, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (13924, 16740), cmpt: ``, lut: 1707836389.6479099
[0m21:59:49.749041 [debug] [ThreadPool]: SQL status: OK in 0.44999998807907104 seconds
[0m21:59:49.749041 [debug] [ThreadPool]: Databricks adapter: conn: 1832480929872: _release sess: fcf0bf82-fb37-40b4-947c-6df3a0785e6e, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (13924, 16740), cmpt: ``, lut: 1707836389.749041
[0m21:59:49.764053 [debug] [ThreadPool]: Databricks adapter: conn: 1832157072144: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_saleslt, idle: 0s, acqrelcnt: 0, lang: None, thrd: (13924, 17104), cmpt: ``, lut: None
[0m21:59:49.765049 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m21:59:49.765688 [debug] [ThreadPool]: Databricks adapter: Thread (13924, 17104) using default compute resource.
[0m21:59:49.765688 [debug] [ThreadPool]: Databricks adapter: conn: 1832157072144: _acquire sess: None, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (13924, 17104), cmpt: ``, lut: 1707836389.7656882
[0m21:59:49.765688 [debug] [ThreadPool]: Databricks adapter: conn: 1832157072144: get_thread_connection: sess: None, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (13924, 17104), cmpt: ``, lut: 1707836389.7656882
[0m21:59:49.765688 [debug] [ThreadPool]: Databricks adapter: conn: 1832157072144: idle check connection: sess: None, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (13924, 17104), cmpt: ``, lut: 1707836389.7656882
[0m21:59:49.765688 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m21:59:49.765688 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m21:59:49.765688 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:59:49.998582 [debug] [ThreadPool]: Databricks adapter: conn: 1832157072144: session opened sess: 7934e5e8-94d8-41e6-8740-a469ebd2d107, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (13924, 17104), cmpt: ``, lut: 1707836389.9985826
[0m21:59:50.149014 [debug] [ThreadPool]: SQL status: OK in 0.3799999952316284 seconds
[0m21:59:50.165443 [debug] [ThreadPool]: Databricks adapter: conn: 1832157072144: get_thread_connection: sess: 7934e5e8-94d8-41e6-8740-a469ebd2d107, name: list_hive_metastore_saleslt, idle: 0.16686129570007324s, acqrelcnt: 1, lang: None, thrd: (13924, 17104), cmpt: ``, lut: 1707836389.9985826
[0m21:59:50.165443 [debug] [ThreadPool]: Databricks adapter: conn: 1832157072144: idle check connection: sess: 7934e5e8-94d8-41e6-8740-a469ebd2d107, name: list_hive_metastore_saleslt, idle: 0.16686129570007324s, acqrelcnt: 1, lang: None, thrd: (13924, 17104), cmpt: ``, lut: 1707836389.9985826
[0m21:59:50.165443 [debug] [ThreadPool]: Databricks adapter: conn: 1832157072144: get_thread_connection: sess: 7934e5e8-94d8-41e6-8740-a469ebd2d107, name: list_hive_metastore_saleslt, idle: 0.16686129570007324s, acqrelcnt: 1, lang: None, thrd: (13924, 17104), cmpt: ``, lut: 1707836389.9985826
[0m21:59:50.165443 [debug] [ThreadPool]: Databricks adapter: conn: 1832157072144: idle check connection: sess: 7934e5e8-94d8-41e6-8740-a469ebd2d107, name: list_hive_metastore_saleslt, idle: 0.16686129570007324s, acqrelcnt: 1, lang: None, thrd: (13924, 17104), cmpt: ``, lut: 1707836389.9985826
[0m21:59:50.165443 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m21:59:50.165443 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m21:59:50.165443 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m21:59:50.415616 [debug] [ThreadPool]: SQL status: OK in 0.25 seconds
[0m21:59:50.415616 [debug] [ThreadPool]: Databricks adapter: conn: 1832157072144: get_thread_connection: sess: 7934e5e8-94d8-41e6-8740-a469ebd2d107, name: list_hive_metastore_saleslt, idle: 0.4170341491699219s, acqrelcnt: 1, lang: None, thrd: (13924, 17104), cmpt: ``, lut: 1707836389.9985826
[0m21:59:50.415616 [debug] [ThreadPool]: Databricks adapter: conn: 1832157072144: idle check connection: sess: 7934e5e8-94d8-41e6-8740-a469ebd2d107, name: list_hive_metastore_saleslt, idle: 0.4170341491699219s, acqrelcnt: 1, lang: None, thrd: (13924, 17104), cmpt: ``, lut: 1707836389.9985826
[0m21:59:50.415616 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m21:59:50.415616 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m21:59:50.599431 [debug] [ThreadPool]: SQL status: OK in 0.18000000715255737 seconds
[0m21:59:50.599431 [debug] [ThreadPool]: Databricks adapter: conn: 1832157072144: _release sess: 7934e5e8-94d8-41e6-8740-a469ebd2d107, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (13924, 17104), cmpt: ``, lut: 1707836390.599432
[0m21:59:50.599431 [debug] [ThreadPool]: Databricks adapter: conn: 1832157072144: idle check connection: sess: 7934e5e8-94d8-41e6-8740-a469ebd2d107, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (13924, 17104), cmpt: ``, lut: 1707836390.599432
[0m21:59:50.615671 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m21:59:50.616657 [debug] [ThreadPool]: Databricks adapter: conn: 1832157072144: reusing connection list_hive_metastore_saleslt sess: 7934e5e8-94d8-41e6-8740-a469ebd2d107, name: list_hive_metastore_snapshots, idle: 0.016239643096923828s, acqrelcnt: 0, lang: None, thrd: (13924, 17104), cmpt: ``, lut: 1707836390.599432
[0m21:59:50.616657 [debug] [ThreadPool]: Databricks adapter: Thread (13924, 17104) using default compute resource.
[0m21:59:50.617900 [debug] [ThreadPool]: Databricks adapter: conn: 1832157072144: _acquire sess: 7934e5e8-94d8-41e6-8740-a469ebd2d107, name: list_hive_metastore_snapshots, idle: 0.018468856811523438s, acqrelcnt: 1, lang: None, thrd: (13924, 17104), cmpt: ``, lut: 1707836390.599432
[0m21:59:50.620289 [debug] [ThreadPool]: Databricks adapter: conn: 1832157072144: get_thread_connection: sess: 7934e5e8-94d8-41e6-8740-a469ebd2d107, name: list_hive_metastore_snapshots, idle: 0.020857810974121094s, acqrelcnt: 1, lang: None, thrd: (13924, 17104), cmpt: ``, lut: 1707836390.599432
[0m21:59:50.620289 [debug] [ThreadPool]: Databricks adapter: conn: 1832157072144: idle check connection: sess: 7934e5e8-94d8-41e6-8740-a469ebd2d107, name: list_hive_metastore_snapshots, idle: 0.020857810974121094s, acqrelcnt: 1, lang: None, thrd: (13924, 17104), cmpt: ``, lut: 1707836390.599432
[0m21:59:50.620289 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m21:59:50.620289 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m21:59:50.749766 [debug] [ThreadPool]: SQL status: OK in 0.12999999523162842 seconds
[0m21:59:50.749766 [debug] [ThreadPool]: Databricks adapter: conn: 1832157072144: get_thread_connection: sess: 7934e5e8-94d8-41e6-8740-a469ebd2d107, name: list_hive_metastore_snapshots, idle: 0.15033459663391113s, acqrelcnt: 1, lang: None, thrd: (13924, 17104), cmpt: ``, lut: 1707836390.599432
[0m21:59:50.749766 [debug] [ThreadPool]: Databricks adapter: conn: 1832157072144: idle check connection: sess: 7934e5e8-94d8-41e6-8740-a469ebd2d107, name: list_hive_metastore_snapshots, idle: 0.15033459663391113s, acqrelcnt: 1, lang: None, thrd: (13924, 17104), cmpt: ``, lut: 1707836390.599432
[0m21:59:50.749766 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m21:59:50.749766 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m21:59:50.920910 [debug] [ThreadPool]: SQL status: OK in 0.17000000178813934 seconds
[0m21:59:50.923616 [debug] [ThreadPool]: Databricks adapter: conn: 1832157072144: get_thread_connection: sess: 7934e5e8-94d8-41e6-8740-a469ebd2d107, name: list_hive_metastore_snapshots, idle: 0.3241848945617676s, acqrelcnt: 1, lang: None, thrd: (13924, 17104), cmpt: ``, lut: 1707836390.599432
[0m21:59:50.923616 [debug] [ThreadPool]: Databricks adapter: conn: 1832157072144: idle check connection: sess: 7934e5e8-94d8-41e6-8740-a469ebd2d107, name: list_hive_metastore_snapshots, idle: 0.3241848945617676s, acqrelcnt: 1, lang: None, thrd: (13924, 17104), cmpt: ``, lut: 1707836390.599432
[0m21:59:50.923616 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m21:59:50.923616 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m21:59:51.166800 [debug] [ThreadPool]: SQL status: OK in 0.23999999463558197 seconds
[0m21:59:51.166800 [debug] [ThreadPool]: Databricks adapter: conn: 1832157072144: _release sess: 7934e5e8-94d8-41e6-8740-a469ebd2d107, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (13924, 17104), cmpt: ``, lut: 1707836391.1668007
[0m21:59:51.166800 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b4c3fcbb-4f23-4946-8c48-087595f6626d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAA85EF2D0>]}
[0m21:59:51.166800 [debug] [MainThread]: Databricks adapter: conn: 1832481329488: get_thread_connection: sess: None, name: master, idle: 1.873079538345337s, acqrelcnt: 1, lang: None, thrd: (13924, 15656), cmpt: ``, lut: 1707836389.2937212
[0m21:59:51.166800 [debug] [MainThread]: Databricks adapter: conn: 1832481329488: idle check connection: sess: None, name: master, idle: 1.873079538345337s, acqrelcnt: 1, lang: None, thrd: (13924, 15656), cmpt: ``, lut: 1707836389.2937212
[0m21:59:51.166800 [debug] [MainThread]: Databricks adapter: conn: 1832481329488: get_thread_connection: sess: None, name: master, idle: 1.873079538345337s, acqrelcnt: 1, lang: None, thrd: (13924, 15656), cmpt: ``, lut: 1707836389.2937212
[0m21:59:51.166800 [debug] [MainThread]: Databricks adapter: conn: 1832481329488: idle check connection: sess: None, name: master, idle: 1.873079538345337s, acqrelcnt: 1, lang: None, thrd: (13924, 15656), cmpt: ``, lut: 1707836389.2937212
[0m21:59:51.166800 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:59:51.166800 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:59:51.166800 [debug] [MainThread]: Databricks adapter: conn: 1832481329488: _release sess: None, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (13924, 15656), cmpt: ``, lut: 1707836391.1668007
[0m21:59:51.166800 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:59:51.166800 [info ] [MainThread]: 
[0m21:59:51.166800 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.address_snapshot
[0m21:59:51.184803 [info ] [Thread-1 (]: 1 of 7 START snapshot snapshots.address_snapshot ............................... [RUN]
[0m21:59:51.186838 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: Creating DatabricksDBTConnection sess: None, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 0s, acqrelcnt: 0, lang: None, thrd: (13924, 24776), cmpt: ``, lut: None
[0m21:59:51.187893 [debug] [Thread-1 (]: Acquiring new databricks connection 'snapshot.medallion_dbt_spark.address_snapshot'
[0m21:59:51.188286 [debug] [Thread-1 (]: Databricks adapter: On thread (13924, 24776): `hive_metastore`.`snapshots`.`address_snapshot` using default compute resource.
[0m21:59:51.188795 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: _acquire sess: None, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836391.188795
[0m21:59:51.188795 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.address_snapshot
[0m21:59:51.188795 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.address_snapshot (compile): 21:59:51.188795 => 21:59:51.188795
[0m21:59:51.188795 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.address_snapshot
[0m21:59:51.221215 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: None, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 0.03241991996765137s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836391.188795
[0m21:59:51.221215 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: None, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 0.03241991996765137s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836391.188795
[0m21:59:51.221215 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: None, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 0.03241991996765137s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836391.188795
[0m21:59:51.221215 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: None, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 0.03241991996765137s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836391.188795
[0m21:59:51.221215 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m21:59:51.221215 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.address_snapshot"
[0m21:59:51.232783 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m21:59:51.232783 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m21:59:51.617098 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: session opened sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836391.6170988
[0m21:59:52.369116 [debug] [Thread-1 (]: SQL status: OK in 1.1399999856948853 seconds
[0m21:59:52.399998 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 0.7828993797302246s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836391.6170988
[0m21:59:52.401093 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 0.783994197845459s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836391.6170988
[0m21:59:52.401093 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.address_snapshot"
[0m21:59:52.401093 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.address_snapshot"} */
select * from (
        



with source_data as (
    select 
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data
    ) as __dbt_sbq
    where false
    limit 0

    
[0m21:59:52.651429 [debug] [Thread-1 (]: SQL status: OK in 0.25 seconds
[0m21:59:52.651429 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 1.0343308448791504s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836391.6170988
[0m21:59:52.651429 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 1.0343308448791504s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836391.6170988
[0m21:59:52.651429 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.address_snapshot"
[0m21:59:52.651429 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m21:59:52.935831 [debug] [Thread-1 (]: SQL status: OK in 0.2800000011920929 seconds
[0m21:59:52.935831 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 1.318732500076294s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836391.6170988
[0m21:59:52.951691 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 1.3344089984893799s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836391.6170988
[0m21:59:52.951981 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.address_snapshot"
[0m21:59:52.951981 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m21:59:53.285731 [debug] [Thread-1 (]: SQL status: OK in 0.33000001311302185 seconds
[0m21:59:53.302134 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 1.685035228729248s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836391.6170988
[0m21:59:53.302134 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 1.685035228729248s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836391.6170988
[0m21:59:53.302134 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.address_snapshot"
[0m21:59:53.302134 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.address_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with source_data as (
    select 
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`saleslt`.`address`
)
select *
from source_data

    ),

    snapshotted_data as (

        select *,
            AddressID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`address_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            AddressID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(AddressID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            AddressID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            AddressID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`AddressID` != source_data.`AddressID`
        or
        (
            ((snapshotted_data.`AddressID` is null) and not (source_data.`AddressID` is null))
            or
            ((not snapshotted_data.`AddressID` is null) and (source_data.`AddressID` is null))
        ) or snapshotted_data.`AddressLine1` != source_data.`AddressLine1`
        or
        (
            ((snapshotted_data.`AddressLine1` is null) and not (source_data.`AddressLine1` is null))
            or
            ((not snapshotted_data.`AddressLine1` is null) and (source_data.`AddressLine1` is null))
        ) or snapshotted_data.`AddressLine2` != source_data.`AddressLine2`
        or
        (
            ((snapshotted_data.`AddressLine2` is null) and not (source_data.`AddressLine2` is null))
            or
            ((not snapshotted_data.`AddressLine2` is null) and (source_data.`AddressLine2` is null))
        ) or snapshotted_data.`City` != source_data.`City`
        or
        (
            ((snapshotted_data.`City` is null) and not (source_data.`City` is null))
            or
            ((not snapshotted_data.`City` is null) and (source_data.`City` is null))
        ) or snapshotted_data.`StateProvince` != source_data.`StateProvince`
        or
        (
            ((snapshotted_data.`StateProvince` is null) and not (source_data.`StateProvince` is null))
            or
            ((not snapshotted_data.`StateProvince` is null) and (source_data.`StateProvince` is null))
        ) or snapshotted_data.`CountryRegion` != source_data.`CountryRegion`
        or
        (
            ((snapshotted_data.`CountryRegion` is null) and not (source_data.`CountryRegion` is null))
            or
            ((not snapshotted_data.`CountryRegion` is null) and (source_data.`CountryRegion` is null))
        ) or snapshotted_data.`PostalCode` != source_data.`PostalCode`
        or
        (
            ((snapshotted_data.`PostalCode` is null) and not (source_data.`PostalCode` is null))
            or
            ((not snapshotted_data.`PostalCode` is null) and (source_data.`PostalCode` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`AddressID` != source_data.`AddressID`
        or
        (
            ((snapshotted_data.`AddressID` is null) and not (source_data.`AddressID` is null))
            or
            ((not snapshotted_data.`AddressID` is null) and (source_data.`AddressID` is null))
        ) or snapshotted_data.`AddressLine1` != source_data.`AddressLine1`
        or
        (
            ((snapshotted_data.`AddressLine1` is null) and not (source_data.`AddressLine1` is null))
            or
            ((not snapshotted_data.`AddressLine1` is null) and (source_data.`AddressLine1` is null))
        ) or snapshotted_data.`AddressLine2` != source_data.`AddressLine2`
        or
        (
            ((snapshotted_data.`AddressLine2` is null) and not (source_data.`AddressLine2` is null))
            or
            ((not snapshotted_data.`AddressLine2` is null) and (source_data.`AddressLine2` is null))
        ) or snapshotted_data.`City` != source_data.`City`
        or
        (
            ((snapshotted_data.`City` is null) and not (source_data.`City` is null))
            or
            ((not snapshotted_data.`City` is null) and (source_data.`City` is null))
        ) or snapshotted_data.`StateProvince` != source_data.`StateProvince`
        or
        (
            ((snapshotted_data.`StateProvince` is null) and not (source_data.`StateProvince` is null))
            or
            ((not snapshotted_data.`StateProvince` is null) and (source_data.`StateProvince` is null))
        ) or snapshotted_data.`CountryRegion` != source_data.`CountryRegion`
        or
        (
            ((snapshotted_data.`CountryRegion` is null) and not (source_data.`CountryRegion` is null))
            or
            ((not snapshotted_data.`CountryRegion` is null) and (source_data.`CountryRegion` is null))
        ) or snapshotted_data.`PostalCode` != source_data.`PostalCode`
        or
        (
            ((snapshotted_data.`PostalCode` is null) and not (source_data.`PostalCode` is null))
            or
            ((not snapshotted_data.`PostalCode` is null) and (source_data.`PostalCode` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m21:59:54.053518 [debug] [Thread-1 (]: SQL status: OK in 0.75 seconds
[0m21:59:54.136538 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 2.519439458847046s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836391.6170988
[0m21:59:54.136538 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 2.519439458847046s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836391.6170988
[0m21:59:54.136538 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.address_snapshot"
[0m21:59:54.136538 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
[0m21:59:54.386920 [debug] [Thread-1 (]: SQL status: OK in 0.25 seconds
[0m21:59:54.386920 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 2.769821882247925s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836391.6170988
[0m21:59:54.386920 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 2.769821882247925s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836391.6170988
[0m21:59:54.386920 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.address_snapshot"
[0m21:59:54.386920 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m21:59:54.670243 [debug] [Thread-1 (]: SQL status: OK in 0.2800000011920929 seconds
[0m21:59:54.670243 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 3.053144693374634s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836391.6170988
[0m21:59:54.670243 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 3.053144693374634s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836391.6170988
[0m21:59:54.670243 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.address_snapshot"
[0m21:59:54.670243 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
[0m21:59:54.870715 [debug] [Thread-1 (]: SQL status: OK in 0.20000000298023224 seconds
[0m21:59:54.870715 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 3.25361704826355s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836391.6170988
[0m21:59:54.870715 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 3.25361704826355s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836391.6170988
[0m21:59:54.870715 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.address_snapshot"
[0m21:59:54.870715 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m21:59:55.154318 [debug] [Thread-1 (]: SQL status: OK in 0.2800000011920929 seconds
[0m21:59:55.169839 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 3.5527405738830566s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836391.6170988
[0m21:59:55.170906 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 3.5538077354431152s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836391.6170988
[0m21:59:55.170906 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.address_snapshot"
[0m21:59:55.170906 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.address_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
  
[0m21:59:55.671441 [debug] [Thread-1 (]: SQL status: OK in 0.5 seconds
[0m21:59:55.671639 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_dbt_spark.address_snapshot"
[0m21:59:55.671639 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 4.054541110992432s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836391.6170988
[0m21:59:55.671639 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 4.054541110992432s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836391.6170988
[0m21:59:55.671639 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.address_snapshot"
[0m21:59:55.671639 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.address_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`address_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m22:00:05.549683 [debug] [Thread-1 (]: SQL status: OK in 9.880000114440918 seconds
[0m22:00:05.650162 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
[0m22:00:05.650162 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 14.033063888549805s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836391.6170988
[0m22:00:05.650162 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 14.033063888549805s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836391.6170988
[0m22:00:05.650162 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.address_snapshot"
[0m22:00:05.650162 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.address_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.address_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`address_snapshot__dbt_tmp`
[0m22:00:06.500444 [debug] [Thread-1 (]: SQL status: OK in 0.8500000238418579 seconds
[0m22:00:06.516920 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m22:00:06.533306 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.address_snapshot (execute): 21:59:51.188795 => 22:00:06.533306
[0m22:00:06.533306 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: _release sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836406.5333061
[0m22:00:06.533306 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: _release sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836406.5333061
[0m22:00:06.533306 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b4c3fcbb-4f23-4946-8c48-087595f6626d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAA84B6F10>]}
[0m22:00:06.533306 [info ] [Thread-1 (]: 1 of 7 OK snapshotted snapshots.address_snapshot ............................... [[32mOK[0m in 15.35s]
[0m22:00:06.533306 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.address_snapshot
[0m22:00:06.533306 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.customer_snapshot
[0m22:00:06.533306 [info ] [Thread-1 (]: 2 of 7 START snapshot snapshots.customer_snapshot .............................. [RUN]
[0m22:00:06.533306 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836406.5333061
[0m22:00:06.533306 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.address_snapshot, now snapshot.medallion_dbt_spark.customer_snapshot)
[0m22:00:06.533306 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: reusing connection snapshot.medallion_dbt_spark.address_snapshot sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836406.5333061
[0m22:00:06.533306 [debug] [Thread-1 (]: Databricks adapter: On thread (13924, 24776): `hive_metastore`.`snapshots`.`customer_snapshot` using default compute resource.
[0m22:00:06.533306 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: _acquire sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836406.5333061
[0m22:00:06.533306 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.customer_snapshot
[0m22:00:06.533306 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.customer_snapshot (compile): 22:00:06.533306 => 22:00:06.533306
[0m22:00:06.533306 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.customer_snapshot
[0m22:00:06.550016 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 0.016710758209228516s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836406.5333061
[0m22:00:06.551196 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 0.017889976501464844s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836406.5333061
[0m22:00:06.552249 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customer_snapshot"
[0m22:00:06.552249 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot`
  
[0m22:00:06.966386 [debug] [Thread-1 (]: SQL status: OK in 0.4000000059604645 seconds
[0m22:00:06.967269 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 0.4339637756347656s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836406.5333061
[0m22:00:06.967269 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 0.4339637756347656s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836406.5333061
[0m22:00:06.967269 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customer_snapshot"
[0m22:00:06.967269 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customer_snapshot"} */
select * from (
        



with source_data as (
    select 
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data
    ) as __dbt_sbq
    where false
    limit 0

    
[0m22:00:07.184116 [debug] [Thread-1 (]: SQL status: OK in 0.2199999988079071 seconds
[0m22:00:07.200857 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 0.6675517559051514s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836406.5333061
[0m22:00:07.200857 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 0.6675517559051514s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836406.5333061
[0m22:00:07.200857 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customer_snapshot"
[0m22:00:07.200857 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot`
  
[0m22:00:07.501239 [debug] [Thread-1 (]: SQL status: OK in 0.30000001192092896 seconds
[0m22:00:07.501239 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 0.9679334163665771s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836406.5333061
[0m22:00:07.501239 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 0.9679334163665771s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836406.5333061
[0m22:00:07.501239 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customer_snapshot"
[0m22:00:07.501239 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot`
  
[0m22:00:07.818267 [debug] [Thread-1 (]: SQL status: OK in 0.3199999928474426 seconds
[0m22:00:07.818267 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 1.284961223602295s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836406.5333061
[0m22:00:07.818267 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 1.284961223602295s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836406.5333061
[0m22:00:07.818267 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customer_snapshot"
[0m22:00:07.818267 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customer_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`customer_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with source_data as (
    select 
        CustomerId,
        NameStyle,
        Title,
        FirstName,
        MiddleName,
        LastName,
        Suffix,
        CompanyName,
        SalesPerson,
        EmailAddress,
        Phone,
        PasswordHash,
        PasswordSalt
    from `hive_metastore`.`saleslt`.`customer`
)
select *
from source_data

    ),

    snapshotted_data as (

        select *,
            CustomerId as dbt_unique_key

        from `hive_metastore`.`snapshots`.`customer_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            CustomerId as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(CustomerId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            CustomerId as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            CustomerId as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`CustomerId` != source_data.`CustomerId`
        or
        (
            ((snapshotted_data.`CustomerId` is null) and not (source_data.`CustomerId` is null))
            or
            ((not snapshotted_data.`CustomerId` is null) and (source_data.`CustomerId` is null))
        ) or snapshotted_data.`NameStyle` != source_data.`NameStyle`
        or
        (
            ((snapshotted_data.`NameStyle` is null) and not (source_data.`NameStyle` is null))
            or
            ((not snapshotted_data.`NameStyle` is null) and (source_data.`NameStyle` is null))
        ) or snapshotted_data.`Title` != source_data.`Title`
        or
        (
            ((snapshotted_data.`Title` is null) and not (source_data.`Title` is null))
            or
            ((not snapshotted_data.`Title` is null) and (source_data.`Title` is null))
        ) or snapshotted_data.`FirstName` != source_data.`FirstName`
        or
        (
            ((snapshotted_data.`FirstName` is null) and not (source_data.`FirstName` is null))
            or
            ((not snapshotted_data.`FirstName` is null) and (source_data.`FirstName` is null))
        ) or snapshotted_data.`MiddleName` != source_data.`MiddleName`
        or
        (
            ((snapshotted_data.`MiddleName` is null) and not (source_data.`MiddleName` is null))
            or
            ((not snapshotted_data.`MiddleName` is null) and (source_data.`MiddleName` is null))
        ) or snapshotted_data.`LastName` != source_data.`LastName`
        or
        (
            ((snapshotted_data.`LastName` is null) and not (source_data.`LastName` is null))
            or
            ((not snapshotted_data.`LastName` is null) and (source_data.`LastName` is null))
        ) or snapshotted_data.`Suffix` != source_data.`Suffix`
        or
        (
            ((snapshotted_data.`Suffix` is null) and not (source_data.`Suffix` is null))
            or
            ((not snapshotted_data.`Suffix` is null) and (source_data.`Suffix` is null))
        ) or snapshotted_data.`CompanyName` != source_data.`CompanyName`
        or
        (
            ((snapshotted_data.`CompanyName` is null) and not (source_data.`CompanyName` is null))
            or
            ((not snapshotted_data.`CompanyName` is null) and (source_data.`CompanyName` is null))
        ) or snapshotted_data.`SalesPerson` != source_data.`SalesPerson`
        or
        (
            ((snapshotted_data.`SalesPerson` is null) and not (source_data.`SalesPerson` is null))
            or
            ((not snapshotted_data.`SalesPerson` is null) and (source_data.`SalesPerson` is null))
        ) or snapshotted_data.`EmailAddress` != source_data.`EmailAddress`
        or
        (
            ((snapshotted_data.`EmailAddress` is null) and not (source_data.`EmailAddress` is null))
            or
            ((not snapshotted_data.`EmailAddress` is null) and (source_data.`EmailAddress` is null))
        ) or snapshotted_data.`Phone` != source_data.`Phone`
        or
        (
            ((snapshotted_data.`Phone` is null) and not (source_data.`Phone` is null))
            or
            ((not snapshotted_data.`Phone` is null) and (source_data.`Phone` is null))
        ) or snapshotted_data.`PasswordHash` != source_data.`PasswordHash`
        or
        (
            ((snapshotted_data.`PasswordHash` is null) and not (source_data.`PasswordHash` is null))
            or
            ((not snapshotted_data.`PasswordHash` is null) and (source_data.`PasswordHash` is null))
        ) or snapshotted_data.`PasswordSalt` != source_data.`PasswordSalt`
        or
        (
            ((snapshotted_data.`PasswordSalt` is null) and not (source_data.`PasswordSalt` is null))
            or
            ((not snapshotted_data.`PasswordSalt` is null) and (source_data.`PasswordSalt` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`CustomerId` != source_data.`CustomerId`
        or
        (
            ((snapshotted_data.`CustomerId` is null) and not (source_data.`CustomerId` is null))
            or
            ((not snapshotted_data.`CustomerId` is null) and (source_data.`CustomerId` is null))
        ) or snapshotted_data.`NameStyle` != source_data.`NameStyle`
        or
        (
            ((snapshotted_data.`NameStyle` is null) and not (source_data.`NameStyle` is null))
            or
            ((not snapshotted_data.`NameStyle` is null) and (source_data.`NameStyle` is null))
        ) or snapshotted_data.`Title` != source_data.`Title`
        or
        (
            ((snapshotted_data.`Title` is null) and not (source_data.`Title` is null))
            or
            ((not snapshotted_data.`Title` is null) and (source_data.`Title` is null))
        ) or snapshotted_data.`FirstName` != source_data.`FirstName`
        or
        (
            ((snapshotted_data.`FirstName` is null) and not (source_data.`FirstName` is null))
            or
            ((not snapshotted_data.`FirstName` is null) and (source_data.`FirstName` is null))
        ) or snapshotted_data.`MiddleName` != source_data.`MiddleName`
        or
        (
            ((snapshotted_data.`MiddleName` is null) and not (source_data.`MiddleName` is null))
            or
            ((not snapshotted_data.`MiddleName` is null) and (source_data.`MiddleName` is null))
        ) or snapshotted_data.`LastName` != source_data.`LastName`
        or
        (
            ((snapshotted_data.`LastName` is null) and not (source_data.`LastName` is null))
            or
            ((not snapshotted_data.`LastName` is null) and (source_data.`LastName` is null))
        ) or snapshotted_data.`Suffix` != source_data.`Suffix`
        or
        (
            ((snapshotted_data.`Suffix` is null) and not (source_data.`Suffix` is null))
            or
            ((not snapshotted_data.`Suffix` is null) and (source_data.`Suffix` is null))
        ) or snapshotted_data.`CompanyName` != source_data.`CompanyName`
        or
        (
            ((snapshotted_data.`CompanyName` is null) and not (source_data.`CompanyName` is null))
            or
            ((not snapshotted_data.`CompanyName` is null) and (source_data.`CompanyName` is null))
        ) or snapshotted_data.`SalesPerson` != source_data.`SalesPerson`
        or
        (
            ((snapshotted_data.`SalesPerson` is null) and not (source_data.`SalesPerson` is null))
            or
            ((not snapshotted_data.`SalesPerson` is null) and (source_data.`SalesPerson` is null))
        ) or snapshotted_data.`EmailAddress` != source_data.`EmailAddress`
        or
        (
            ((snapshotted_data.`EmailAddress` is null) and not (source_data.`EmailAddress` is null))
            or
            ((not snapshotted_data.`EmailAddress` is null) and (source_data.`EmailAddress` is null))
        ) or snapshotted_data.`Phone` != source_data.`Phone`
        or
        (
            ((snapshotted_data.`Phone` is null) and not (source_data.`Phone` is null))
            or
            ((not snapshotted_data.`Phone` is null) and (source_data.`Phone` is null))
        ) or snapshotted_data.`PasswordHash` != source_data.`PasswordHash`
        or
        (
            ((snapshotted_data.`PasswordHash` is null) and not (source_data.`PasswordHash` is null))
            or
            ((not snapshotted_data.`PasswordHash` is null) and (source_data.`PasswordHash` is null))
        ) or snapshotted_data.`PasswordSalt` != source_data.`PasswordSalt`
        or
        (
            ((snapshotted_data.`PasswordSalt` is null) and not (source_data.`PasswordSalt` is null))
            or
            ((not snapshotted_data.`PasswordSalt` is null) and (source_data.`PasswordSalt` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m22:00:08.318943 [debug] [Thread-1 (]: SQL status: OK in 0.5 seconds
[0m22:00:08.335513 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 1.80190110206604s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836406.5333061
[0m22:00:08.335627 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 1.802321434020996s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836406.5333061
[0m22:00:08.335627 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customer_snapshot"
[0m22:00:08.335627 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot__dbt_tmp`
  
[0m22:00:08.519003 [debug] [Thread-1 (]: SQL status: OK in 0.18000000715255737 seconds
[0m22:00:08.519399 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 1.9860928058624268s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836406.5333061
[0m22:00:08.519399 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 1.9860928058624268s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836406.5333061
[0m22:00:08.519399 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customer_snapshot"
[0m22:00:08.519399 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot`
  
[0m22:00:08.819523 [debug] [Thread-1 (]: SQL status: OK in 0.30000001192092896 seconds
[0m22:00:08.819523 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 2.28621768951416s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836406.5333061
[0m22:00:08.819523 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 2.28621768951416s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836406.5333061
[0m22:00:08.819523 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customer_snapshot"
[0m22:00:08.819523 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot__dbt_tmp`
  
[0m22:00:09.019950 [debug] [Thread-1 (]: SQL status: OK in 0.20000000298023224 seconds
[0m22:00:09.019950 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 2.4866440296173096s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836406.5333061
[0m22:00:09.019950 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 2.4866440296173096s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836406.5333061
[0m22:00:09.019950 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customer_snapshot"
[0m22:00:09.019950 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot`
  
[0m22:00:09.337066 [debug] [Thread-1 (]: SQL status: OK in 0.3199999928474426 seconds
[0m22:00:09.353327 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 2.820021390914917s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836406.5333061
[0m22:00:09.353327 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 2.820021390914917s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836406.5333061
[0m22:00:09.353327 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customer_snapshot"
[0m22:00:09.353327 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customer_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customer_snapshot__dbt_tmp`
  
[0m22:00:09.553279 [debug] [Thread-1 (]: SQL status: OK in 0.20000000298023224 seconds
[0m22:00:09.570597 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_dbt_spark.customer_snapshot"
[0m22:00:09.570597 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 3.0372910499572754s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836406.5333061
[0m22:00:09.570597 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 3.0372910499572754s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836406.5333061
[0m22:00:09.570597 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customer_snapshot"
[0m22:00:09.570597 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customer_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`customer_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`customer_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m22:00:14.125419 [debug] [Thread-1 (]: SQL status: OK in 4.550000190734863 seconds
[0m22:00:14.125419 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`customer_snapshot__dbt_tmp`
[0m22:00:14.125419 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 7.59211277961731s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836406.5333061
[0m22:00:14.125419 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 7.59211277961731s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836406.5333061
[0m22:00:14.125419 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customer_snapshot"
[0m22:00:14.125419 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customer_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customer_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`customer_snapshot__dbt_tmp`
[0m22:00:14.776238 [debug] [Thread-1 (]: SQL status: OK in 0.6499999761581421 seconds
[0m22:00:14.776238 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m22:00:14.776238 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.customer_snapshot (execute): 22:00:06.533306 => 22:00:14.776238
[0m22:00:14.776238 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: _release sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836414.776239
[0m22:00:14.776238 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: _release sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836414.776239
[0m22:00:14.776238 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b4c3fcbb-4f23-4946-8c48-087595f6626d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAA88A9F90>]}
[0m22:00:14.776238 [info ] [Thread-1 (]: 2 of 7 OK snapshotted snapshots.customer_snapshot .............................. [[32mOK[0m in 8.24s]
[0m22:00:14.776238 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.customer_snapshot
[0m22:00:14.776238 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m22:00:14.791873 [info ] [Thread-1 (]: 3 of 7 START snapshot snapshots.customeraddress_snapshot ....................... [RUN]
[0m22:00:14.792379 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 0.016140460968017578s, acqrelcnt: 0, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836414.776239
[0m22:00:14.792379 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.customer_snapshot, now snapshot.medallion_dbt_spark.customeraddress_snapshot)
[0m22:00:14.792379 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: reusing connection snapshot.medallion_dbt_spark.customer_snapshot sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 0.016140460968017578s, acqrelcnt: 0, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836414.776239
[0m22:00:14.792379 [debug] [Thread-1 (]: Databricks adapter: On thread (13924, 24776): `hive_metastore`.`snapshots`.`customeraddress_snapshot` using default compute resource.
[0m22:00:14.792379 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: _acquire sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 0.016140460968017578s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836414.776239
[0m22:00:14.792379 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m22:00:14.792379 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.customeraddress_snapshot (compile): 22:00:14.792379 => 22:00:14.792379
[0m22:00:14.792379 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m22:00:14.792379 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 0.016140460968017578s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836414.776239
[0m22:00:14.792379 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 0.016140460968017578s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836414.776239
[0m22:00:14.792379 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customeraddress_snapshot"
[0m22:00:14.792379 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m22:00:15.109599 [debug] [Thread-1 (]: SQL status: OK in 0.3199999928474426 seconds
[0m22:00:15.109599 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 0.3333601951599121s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836414.776239
[0m22:00:15.109599 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 0.3333601951599121s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836414.776239
[0m22:00:15.109599 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customeraddress_snapshot"
[0m22:00:15.109599 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customeraddress_snapshot"} */
select * from (
        



with source_data as (
    select 
        CustomerID,
        AddressID,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data
    ) as __dbt_sbq
    where false
    limit 0

    
[0m22:00:15.292898 [debug] [Thread-1 (]: SQL status: OK in 0.18000000715255737 seconds
[0m22:00:15.292898 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 0.5166590213775635s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836414.776239
[0m22:00:15.292898 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 0.5166590213775635s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836414.776239
[0m22:00:15.292898 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customeraddress_snapshot"
[0m22:00:15.292898 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m22:00:15.560201 [debug] [Thread-1 (]: SQL status: OK in 0.27000001072883606 seconds
[0m22:00:15.560201 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 0.7839622497558594s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836414.776239
[0m22:00:15.576379 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 0.7996871471405029s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836414.776239
[0m22:00:15.576563 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customeraddress_snapshot"
[0m22:00:15.576563 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m22:00:15.826755 [debug] [Thread-1 (]: SQL status: OK in 0.25 seconds
[0m22:00:15.826755 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 1.0505168437957764s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836414.776239
[0m22:00:15.826755 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 1.0505168437957764s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836414.776239
[0m22:00:15.826755 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customeraddress_snapshot"
[0m22:00:15.826755 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customeraddress_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with source_data as (
    select 
        CustomerID,
        AddressID,
        AddressType
    from `hive_metastore`.`saleslt`.`customeraddress`
)
select *
from source_data

    ),

    snapshotted_data as (

        select *,
            CustomerId||'-'||AddressID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`customeraddress_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            CustomerId||'-'||AddressID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(CustomerId||'-'||AddressID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            CustomerId||'-'||AddressID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            CustomerId||'-'||AddressID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`CustomerID` != source_data.`CustomerID`
        or
        (
            ((snapshotted_data.`CustomerID` is null) and not (source_data.`CustomerID` is null))
            or
            ((not snapshotted_data.`CustomerID` is null) and (source_data.`CustomerID` is null))
        ) or snapshotted_data.`AddressID` != source_data.`AddressID`
        or
        (
            ((snapshotted_data.`AddressID` is null) and not (source_data.`AddressID` is null))
            or
            ((not snapshotted_data.`AddressID` is null) and (source_data.`AddressID` is null))
        ) or snapshotted_data.`AddressType` != source_data.`AddressType`
        or
        (
            ((snapshotted_data.`AddressType` is null) and not (source_data.`AddressType` is null))
            or
            ((not snapshotted_data.`AddressType` is null) and (source_data.`AddressType` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`CustomerID` != source_data.`CustomerID`
        or
        (
            ((snapshotted_data.`CustomerID` is null) and not (source_data.`CustomerID` is null))
            or
            ((not snapshotted_data.`CustomerID` is null) and (source_data.`CustomerID` is null))
        ) or snapshotted_data.`AddressID` != source_data.`AddressID`
        or
        (
            ((snapshotted_data.`AddressID` is null) and not (source_data.`AddressID` is null))
            or
            ((not snapshotted_data.`AddressID` is null) and (source_data.`AddressID` is null))
        ) or snapshotted_data.`AddressType` != source_data.`AddressType`
        or
        (
            ((snapshotted_data.`AddressType` is null) and not (source_data.`AddressType` is null))
            or
            ((not snapshotted_data.`AddressType` is null) and (source_data.`AddressType` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m22:00:16.244487 [debug] [Thread-1 (]: SQL status: OK in 0.41999998688697815 seconds
[0m22:00:16.244487 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 1.4682483673095703s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836414.776239
[0m22:00:16.244487 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 1.4682483673095703s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836414.776239
[0m22:00:16.244487 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customeraddress_snapshot"
[0m22:00:16.244487 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
[0m22:00:16.439747 [debug] [Thread-1 (]: SQL status: OK in 0.1899999976158142 seconds
[0m22:00:16.444816 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 1.6685771942138672s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836414.776239
[0m22:00:16.444816 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 1.6685771942138672s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836414.776239
[0m22:00:16.444816 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customeraddress_snapshot"
[0m22:00:16.444816 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m22:00:16.727836 [debug] [Thread-1 (]: SQL status: OK in 0.2800000011920929 seconds
[0m22:00:16.727836 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 1.9515979290008545s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836414.776239
[0m22:00:16.727836 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 1.9515979290008545s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836414.776239
[0m22:00:16.727836 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customeraddress_snapshot"
[0m22:00:16.727836 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
[0m22:00:16.913977 [debug] [Thread-1 (]: SQL status: OK in 0.1899999976158142 seconds
[0m22:00:16.913977 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 2.1377384662628174s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836414.776239
[0m22:00:16.913977 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 2.1377384662628174s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836414.776239
[0m22:00:16.913977 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customeraddress_snapshot"
[0m22:00:16.913977 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m22:00:17.295809 [debug] [Thread-1 (]: SQL status: OK in 0.3799999952316284 seconds
[0m22:00:17.295809 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 2.519570827484131s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836414.776239
[0m22:00:17.295809 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 2.519570827484131s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836414.776239
[0m22:00:17.295809 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customeraddress_snapshot"
[0m22:00:17.295809 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customeraddress_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
  
[0m22:00:17.495374 [debug] [Thread-1 (]: SQL status: OK in 0.20000000298023224 seconds
[0m22:00:17.496186 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_dbt_spark.customeraddress_snapshot"
[0m22:00:17.496186 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 2.7199478149414062s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836414.776239
[0m22:00:17.496186 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 2.7199478149414062s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836414.776239
[0m22:00:17.496186 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customeraddress_snapshot"
[0m22:00:17.496186 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customeraddress_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`customeraddress_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m22:00:21.917406 [debug] [Thread-1 (]: SQL status: OK in 4.420000076293945 seconds
[0m22:00:21.933681 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
[0m22:00:21.933681 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 7.157442569732666s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836414.776239
[0m22:00:21.933681 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 7.157442569732666s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836414.776239
[0m22:00:21.933681 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.customeraddress_snapshot"
[0m22:00:21.933681 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.customeraddress_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.customeraddress_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`customeraddress_snapshot__dbt_tmp`
[0m22:00:22.434845 [debug] [Thread-1 (]: SQL status: OK in 0.5 seconds
[0m22:00:22.434845 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m22:00:22.434845 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.customeraddress_snapshot (execute): 22:00:14.792379 => 22:00:22.434845
[0m22:00:22.434845 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: _release sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836422.4348454
[0m22:00:22.434845 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: _release sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836422.4348454
[0m22:00:22.434845 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b4c3fcbb-4f23-4946-8c48-087595f6626d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AA961428D0>]}
[0m22:00:22.434845 [info ] [Thread-1 (]: 3 of 7 OK snapshotted snapshots.customeraddress_snapshot ....................... [[32mOK[0m in 7.64s]
[0m22:00:22.434845 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m22:00:22.434845 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.product_snapshot
[0m22:00:22.450844 [info ] [Thread-1 (]: 4 of 7 START snapshot snapshots.product_snapshot ............................... [RUN]
[0m22:00:22.450844 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 0.01599907875061035s, acqrelcnt: 0, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836422.4348454
[0m22:00:22.450844 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.customeraddress_snapshot, now snapshot.medallion_dbt_spark.product_snapshot)
[0m22:00:22.450844 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: reusing connection snapshot.medallion_dbt_spark.customeraddress_snapshot sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 0.01599907875061035s, acqrelcnt: 0, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836422.4348454
[0m22:00:22.450844 [debug] [Thread-1 (]: Databricks adapter: On thread (13924, 24776): `hive_metastore`.`snapshots`.`product_snapshot` using default compute resource.
[0m22:00:22.450844 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: _acquire sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 0.01599907875061035s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836422.4348454
[0m22:00:22.450844 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.product_snapshot
[0m22:00:22.450844 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.product_snapshot (compile): 22:00:22.450844 => 22:00:22.450844
[0m22:00:22.450844 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.product_snapshot
[0m22:00:22.450844 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 0.01599907875061035s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836422.4348454
[0m22:00:22.450844 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 0.01599907875061035s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836422.4348454
[0m22:00:22.450844 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.product_snapshot"
[0m22:00:22.450844 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot`
  
[0m22:00:22.818548 [debug] [Thread-1 (]: SQL status: OK in 0.3700000047683716 seconds
[0m22:00:22.818548 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 0.38370275497436523s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836422.4348454
[0m22:00:22.818548 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 0.38370275497436523s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836422.4348454
[0m22:00:22.818548 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.product_snapshot"
[0m22:00:22.818548 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.product_snapshot"} */
select * from (
        



with source_data as (
    select 
        ProductId,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    from `hive_metastore`.`saleslt`.`product`
)
select *
from source_data
    ) as __dbt_sbq
    where false
    limit 0

    
[0m22:00:23.185368 [debug] [Thread-1 (]: SQL status: OK in 0.3700000047683716 seconds
[0m22:00:23.185368 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 0.7505228519439697s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836422.4348454
[0m22:00:23.185368 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 0.7505228519439697s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836422.4348454
[0m22:00:23.185368 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.product_snapshot"
[0m22:00:23.185368 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot`
  
[0m22:00:23.452174 [debug] [Thread-1 (]: SQL status: OK in 0.27000001072883606 seconds
[0m22:00:23.452174 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 1.017329454421997s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836422.4348454
[0m22:00:23.452174 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 1.017329454421997s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836422.4348454
[0m22:00:23.467794 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.product_snapshot"
[0m22:00:23.467794 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot`
  
[0m22:00:23.753066 [debug] [Thread-1 (]: SQL status: OK in 0.2800000011920929 seconds
[0m22:00:23.753066 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 1.318221092224121s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836422.4348454
[0m22:00:23.753066 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 1.318221092224121s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836422.4348454
[0m22:00:23.753066 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.product_snapshot"
[0m22:00:23.768901 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.product_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`product_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with source_data as (
    select 
        ProductId,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    from `hive_metastore`.`saleslt`.`product`
)
select *
from source_data

    ),

    snapshotted_data as (

        select *,
            ProductId as dbt_unique_key

        from `hive_metastore`.`snapshots`.`product_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            ProductId as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(ProductId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            ProductId as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            ProductId as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`ProductId` != source_data.`ProductId`
        or
        (
            ((snapshotted_data.`ProductId` is null) and not (source_data.`ProductId` is null))
            or
            ((not snapshotted_data.`ProductId` is null) and (source_data.`ProductId` is null))
        ) or snapshotted_data.`Name` != source_data.`Name`
        or
        (
            ((snapshotted_data.`Name` is null) and not (source_data.`Name` is null))
            or
            ((not snapshotted_data.`Name` is null) and (source_data.`Name` is null))
        ) or snapshotted_data.`ProductNumber` != source_data.`ProductNumber`
        or
        (
            ((snapshotted_data.`ProductNumber` is null) and not (source_data.`ProductNumber` is null))
            or
            ((not snapshotted_data.`ProductNumber` is null) and (source_data.`ProductNumber` is null))
        ) or snapshotted_data.`Color` != source_data.`Color`
        or
        (
            ((snapshotted_data.`Color` is null) and not (source_data.`Color` is null))
            or
            ((not snapshotted_data.`Color` is null) and (source_data.`Color` is null))
        ) or snapshotted_data.`StandardCost` != source_data.`StandardCost`
        or
        (
            ((snapshotted_data.`StandardCost` is null) and not (source_data.`StandardCost` is null))
            or
            ((not snapshotted_data.`StandardCost` is null) and (source_data.`StandardCost` is null))
        ) or snapshotted_data.`ListPrice` != source_data.`ListPrice`
        or
        (
            ((snapshotted_data.`ListPrice` is null) and not (source_data.`ListPrice` is null))
            or
            ((not snapshotted_data.`ListPrice` is null) and (source_data.`ListPrice` is null))
        ) or snapshotted_data.`Size` != source_data.`Size`
        or
        (
            ((snapshotted_data.`Size` is null) and not (source_data.`Size` is null))
            or
            ((not snapshotted_data.`Size` is null) and (source_data.`Size` is null))
        ) or snapshotted_data.`Weight` != source_data.`Weight`
        or
        (
            ((snapshotted_data.`Weight` is null) and not (source_data.`Weight` is null))
            or
            ((not snapshotted_data.`Weight` is null) and (source_data.`Weight` is null))
        ) or snapshotted_data.`ProductCategoryID` != source_data.`ProductCategoryID`
        or
        (
            ((snapshotted_data.`ProductCategoryID` is null) and not (source_data.`ProductCategoryID` is null))
            or
            ((not snapshotted_data.`ProductCategoryID` is null) and (source_data.`ProductCategoryID` is null))
        ) or snapshotted_data.`ProductModelID` != source_data.`ProductModelID`
        or
        (
            ((snapshotted_data.`ProductModelID` is null) and not (source_data.`ProductModelID` is null))
            or
            ((not snapshotted_data.`ProductModelID` is null) and (source_data.`ProductModelID` is null))
        ) or snapshotted_data.`SellStartDate` != source_data.`SellStartDate`
        or
        (
            ((snapshotted_data.`SellStartDate` is null) and not (source_data.`SellStartDate` is null))
            or
            ((not snapshotted_data.`SellStartDate` is null) and (source_data.`SellStartDate` is null))
        ) or snapshotted_data.`SellEndDate` != source_data.`SellEndDate`
        or
        (
            ((snapshotted_data.`SellEndDate` is null) and not (source_data.`SellEndDate` is null))
            or
            ((not snapshotted_data.`SellEndDate` is null) and (source_data.`SellEndDate` is null))
        ) or snapshotted_data.`DiscontinuedDate` != source_data.`DiscontinuedDate`
        or
        (
            ((snapshotted_data.`DiscontinuedDate` is null) and not (source_data.`DiscontinuedDate` is null))
            or
            ((not snapshotted_data.`DiscontinuedDate` is null) and (source_data.`DiscontinuedDate` is null))
        ) or snapshotted_data.`ThumbNailPhoto` != source_data.`ThumbNailPhoto`
        or
        (
            ((snapshotted_data.`ThumbNailPhoto` is null) and not (source_data.`ThumbNailPhoto` is null))
            or
            ((not snapshotted_data.`ThumbNailPhoto` is null) and (source_data.`ThumbNailPhoto` is null))
        ) or snapshotted_data.`ThumbnailPhotoFileName` != source_data.`ThumbnailPhotoFileName`
        or
        (
            ((snapshotted_data.`ThumbnailPhotoFileName` is null) and not (source_data.`ThumbnailPhotoFileName` is null))
            or
            ((not snapshotted_data.`ThumbnailPhotoFileName` is null) and (source_data.`ThumbnailPhotoFileName` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`ProductId` != source_data.`ProductId`
        or
        (
            ((snapshotted_data.`ProductId` is null) and not (source_data.`ProductId` is null))
            or
            ((not snapshotted_data.`ProductId` is null) and (source_data.`ProductId` is null))
        ) or snapshotted_data.`Name` != source_data.`Name`
        or
        (
            ((snapshotted_data.`Name` is null) and not (source_data.`Name` is null))
            or
            ((not snapshotted_data.`Name` is null) and (source_data.`Name` is null))
        ) or snapshotted_data.`ProductNumber` != source_data.`ProductNumber`
        or
        (
            ((snapshotted_data.`ProductNumber` is null) and not (source_data.`ProductNumber` is null))
            or
            ((not snapshotted_data.`ProductNumber` is null) and (source_data.`ProductNumber` is null))
        ) or snapshotted_data.`Color` != source_data.`Color`
        or
        (
            ((snapshotted_data.`Color` is null) and not (source_data.`Color` is null))
            or
            ((not snapshotted_data.`Color` is null) and (source_data.`Color` is null))
        ) or snapshotted_data.`StandardCost` != source_data.`StandardCost`
        or
        (
            ((snapshotted_data.`StandardCost` is null) and not (source_data.`StandardCost` is null))
            or
            ((not snapshotted_data.`StandardCost` is null) and (source_data.`StandardCost` is null))
        ) or snapshotted_data.`ListPrice` != source_data.`ListPrice`
        or
        (
            ((snapshotted_data.`ListPrice` is null) and not (source_data.`ListPrice` is null))
            or
            ((not snapshotted_data.`ListPrice` is null) and (source_data.`ListPrice` is null))
        ) or snapshotted_data.`Size` != source_data.`Size`
        or
        (
            ((snapshotted_data.`Size` is null) and not (source_data.`Size` is null))
            or
            ((not snapshotted_data.`Size` is null) and (source_data.`Size` is null))
        ) or snapshotted_data.`Weight` != source_data.`Weight`
        or
        (
            ((snapshotted_data.`Weight` is null) and not (source_data.`Weight` is null))
            or
            ((not snapshotted_data.`Weight` is null) and (source_data.`Weight` is null))
        ) or snapshotted_data.`ProductCategoryID` != source_data.`ProductCategoryID`
        or
        (
            ((snapshotted_data.`ProductCategoryID` is null) and not (source_data.`ProductCategoryID` is null))
            or
            ((not snapshotted_data.`ProductCategoryID` is null) and (source_data.`ProductCategoryID` is null))
        ) or snapshotted_data.`ProductModelID` != source_data.`ProductModelID`
        or
        (
            ((snapshotted_data.`ProductModelID` is null) and not (source_data.`ProductModelID` is null))
            or
            ((not snapshotted_data.`ProductModelID` is null) and (source_data.`ProductModelID` is null))
        ) or snapshotted_data.`SellStartDate` != source_data.`SellStartDate`
        or
        (
            ((snapshotted_data.`SellStartDate` is null) and not (source_data.`SellStartDate` is null))
            or
            ((not snapshotted_data.`SellStartDate` is null) and (source_data.`SellStartDate` is null))
        ) or snapshotted_data.`SellEndDate` != source_data.`SellEndDate`
        or
        (
            ((snapshotted_data.`SellEndDate` is null) and not (source_data.`SellEndDate` is null))
            or
            ((not snapshotted_data.`SellEndDate` is null) and (source_data.`SellEndDate` is null))
        ) or snapshotted_data.`DiscontinuedDate` != source_data.`DiscontinuedDate`
        or
        (
            ((snapshotted_data.`DiscontinuedDate` is null) and not (source_data.`DiscontinuedDate` is null))
            or
            ((not snapshotted_data.`DiscontinuedDate` is null) and (source_data.`DiscontinuedDate` is null))
        ) or snapshotted_data.`ThumbNailPhoto` != source_data.`ThumbNailPhoto`
        or
        (
            ((snapshotted_data.`ThumbNailPhoto` is null) and not (source_data.`ThumbNailPhoto` is null))
            or
            ((not snapshotted_data.`ThumbNailPhoto` is null) and (source_data.`ThumbNailPhoto` is null))
        ) or snapshotted_data.`ThumbnailPhotoFileName` != source_data.`ThumbnailPhotoFileName`
        or
        (
            ((snapshotted_data.`ThumbnailPhotoFileName` is null) and not (source_data.`ThumbnailPhotoFileName` is null))
            or
            ((not snapshotted_data.`ThumbnailPhotoFileName` is null) and (source_data.`ThumbnailPhotoFileName` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m22:00:24.253312 [debug] [Thread-1 (]: SQL status: OK in 0.47999998927116394 seconds
[0m22:00:24.253312 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 1.818467617034912s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836422.4348454
[0m22:00:24.253312 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 1.818467617034912s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836422.4348454
[0m22:00:24.253312 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.product_snapshot"
[0m22:00:24.253312 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot__dbt_tmp`
  
[0m22:00:24.436952 [debug] [Thread-1 (]: SQL status: OK in 0.18000000715255737 seconds
[0m22:00:24.453415 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 2.0185704231262207s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836422.4348454
[0m22:00:24.453415 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 2.0185704231262207s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836422.4348454
[0m22:00:24.453415 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.product_snapshot"
[0m22:00:24.453415 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot`
  
[0m22:00:24.787038 [debug] [Thread-1 (]: SQL status: OK in 0.33000001311302185 seconds
[0m22:00:24.787038 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 2.3521928787231445s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836422.4348454
[0m22:00:24.787038 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 2.3521928787231445s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836422.4348454
[0m22:00:24.787038 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.product_snapshot"
[0m22:00:24.787038 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot__dbt_tmp`
  
[0m22:00:25.003637 [debug] [Thread-1 (]: SQL status: OK in 0.2199999988079071 seconds
[0m22:00:25.003637 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 2.5687921047210693s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836422.4348454
[0m22:00:25.003637 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 2.5687921047210693s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836422.4348454
[0m22:00:25.003637 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.product_snapshot"
[0m22:00:25.003637 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot`
  
[0m22:00:25.287459 [debug] [Thread-1 (]: SQL status: OK in 0.2800000011920929 seconds
[0m22:00:25.287459 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 2.852614402770996s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836422.4348454
[0m22:00:25.287459 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 2.852614402770996s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836422.4348454
[0m22:00:25.287459 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.product_snapshot"
[0m22:00:25.287459 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.product_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`product_snapshot__dbt_tmp`
  
[0m22:00:25.471464 [debug] [Thread-1 (]: SQL status: OK in 0.18000000715255737 seconds
[0m22:00:25.488020 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_dbt_spark.product_snapshot"
[0m22:00:25.488020 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 3.053175210952759s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836422.4348454
[0m22:00:25.488020 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 3.053175210952759s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836422.4348454
[0m22:00:25.488020 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.product_snapshot"
[0m22:00:25.488020 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.product_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`product_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`product_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m22:00:30.509820 [debug] [Thread-1 (]: SQL status: OK in 5.019999980926514 seconds
[0m22:00:30.526788 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`product_snapshot__dbt_tmp`
[0m22:00:30.526788 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 8.091942548751831s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836422.4348454
[0m22:00:30.526788 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 8.091942548751831s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836422.4348454
[0m22:00:30.526788 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.product_snapshot"
[0m22:00:30.526788 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.product_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.product_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`product_snapshot__dbt_tmp`
[0m22:00:31.010762 [debug] [Thread-1 (]: SQL status: OK in 0.47999998927116394 seconds
[0m22:00:31.026967 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m22:00:31.027263 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.product_snapshot (execute): 22:00:22.450844 => 22:00:31.027263
[0m22:00:31.027263 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: _release sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836431.0272632
[0m22:00:31.027263 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: _release sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836431.0272632
[0m22:00:31.027263 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b4c3fcbb-4f23-4946-8c48-087595f6626d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAA81F2D90>]}
[0m22:00:31.027263 [info ] [Thread-1 (]: 4 of 7 OK snapshotted snapshots.product_snapshot ............................... [[32mOK[0m in 8.58s]
[0m22:00:31.027263 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.product_snapshot
[0m22:00:31.027263 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.productmodel_snapshot
[0m22:00:31.027263 [info ] [Thread-1 (]: 5 of 7 START snapshot snapshots.productmodel_snapshot .......................... [RUN]
[0m22:00:31.027263 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836431.0272632
[0m22:00:31.027263 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.product_snapshot, now snapshot.medallion_dbt_spark.productmodel_snapshot)
[0m22:00:31.027263 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: reusing connection snapshot.medallion_dbt_spark.product_snapshot sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836431.0272632
[0m22:00:31.027263 [debug] [Thread-1 (]: Databricks adapter: On thread (13924, 24776): `hive_metastore`.`snapshots`.`productmodel_snapshot` using default compute resource.
[0m22:00:31.027263 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: _acquire sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836431.0272632
[0m22:00:31.027263 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.productmodel_snapshot
[0m22:00:31.027263 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.productmodel_snapshot (compile): 22:00:31.027263 => 22:00:31.027263
[0m22:00:31.027263 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.productmodel_snapshot
[0m22:00:31.043966 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 0.01670360565185547s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836431.0272632
[0m22:00:31.044918 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 0.017655611038208008s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836431.0272632
[0m22:00:31.044918 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.productmodel_snapshot"
[0m22:00:31.045965 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m22:00:31.410843 [debug] [Thread-1 (]: SQL status: OK in 0.36000001430511475 seconds
[0m22:00:31.410843 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 0.38357996940612793s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836431.0272632
[0m22:00:31.410843 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 0.38357996940612793s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836431.0272632
[0m22:00:31.410843 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.productmodel_snapshot"
[0m22:00:31.410843 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.productmodel_snapshot"} */
select * from (
        



with source_data as (
    select 
        ProductModelID,
        Name,
        CatalogDescription
    from `hive_metastore`.`saleslt`.`productmodel`
)
select *
from source_data
    ) as __dbt_sbq
    where false
    limit 0

    
[0m22:00:31.578022 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m22:00:31.594573 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 0.5673103332519531s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836431.0272632
[0m22:00:31.594573 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 0.5673103332519531s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836431.0272632
[0m22:00:31.594573 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.productmodel_snapshot"
[0m22:00:31.594573 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m22:00:32.045989 [debug] [Thread-1 (]: SQL status: OK in 0.44999998807907104 seconds
[0m22:00:32.045989 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 1.0187263488769531s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836431.0272632
[0m22:00:32.045989 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 1.0187263488769531s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836431.0272632
[0m22:00:32.045989 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.productmodel_snapshot"
[0m22:00:32.045989 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m22:00:32.313043 [debug] [Thread-1 (]: SQL status: OK in 0.27000001072883606 seconds
[0m22:00:32.313043 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 1.2857799530029297s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836431.0272632
[0m22:00:32.313043 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 1.2857799530029297s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836431.0272632
[0m22:00:32.313043 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.productmodel_snapshot"
[0m22:00:32.313043 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.productmodel_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with source_data as (
    select 
        ProductModelID,
        Name,
        CatalogDescription
    from `hive_metastore`.`saleslt`.`productmodel`
)
select *
from source_data

    ),

    snapshotted_data as (

        select *,
            ProductModelID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`productmodel_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            ProductModelID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(ProductModelID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            ProductModelID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            ProductModelID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`ProductModelID` != source_data.`ProductModelID`
        or
        (
            ((snapshotted_data.`ProductModelID` is null) and not (source_data.`ProductModelID` is null))
            or
            ((not snapshotted_data.`ProductModelID` is null) and (source_data.`ProductModelID` is null))
        ) or snapshotted_data.`Name` != source_data.`Name`
        or
        (
            ((snapshotted_data.`Name` is null) and not (source_data.`Name` is null))
            or
            ((not snapshotted_data.`Name` is null) and (source_data.`Name` is null))
        ) or snapshotted_data.`CatalogDescription` != source_data.`CatalogDescription`
        or
        (
            ((snapshotted_data.`CatalogDescription` is null) and not (source_data.`CatalogDescription` is null))
            or
            ((not snapshotted_data.`CatalogDescription` is null) and (source_data.`CatalogDescription` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`ProductModelID` != source_data.`ProductModelID`
        or
        (
            ((snapshotted_data.`ProductModelID` is null) and not (source_data.`ProductModelID` is null))
            or
            ((not snapshotted_data.`ProductModelID` is null) and (source_data.`ProductModelID` is null))
        ) or snapshotted_data.`Name` != source_data.`Name`
        or
        (
            ((snapshotted_data.`Name` is null) and not (source_data.`Name` is null))
            or
            ((not snapshotted_data.`Name` is null) and (source_data.`Name` is null))
        ) or snapshotted_data.`CatalogDescription` != source_data.`CatalogDescription`
        or
        (
            ((snapshotted_data.`CatalogDescription` is null) and not (source_data.`CatalogDescription` is null))
            or
            ((not snapshotted_data.`CatalogDescription` is null) and (source_data.`CatalogDescription` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m22:00:32.845053 [debug] [Thread-1 (]: SQL status: OK in 0.5199999809265137 seconds
[0m22:00:32.846894 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 1.819631576538086s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836431.0272632
[0m22:00:32.846894 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 1.819631576538086s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836431.0272632
[0m22:00:32.846894 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.productmodel_snapshot"
[0m22:00:32.846894 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
[0m22:00:32.996370 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m22:00:33.012576 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 1.9853131771087646s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836431.0272632
[0m22:00:33.012576 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 1.9853131771087646s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836431.0272632
[0m22:00:33.012576 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.productmodel_snapshot"
[0m22:00:33.012576 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m22:00:33.279438 [debug] [Thread-1 (]: SQL status: OK in 0.27000001072883606 seconds
[0m22:00:33.295070 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 2.267807722091675s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836431.0272632
[0m22:00:33.296226 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 2.268963098526001s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836431.0272632
[0m22:00:33.296226 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.productmodel_snapshot"
[0m22:00:33.296226 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
[0m22:00:33.980679 [debug] [Thread-1 (]: SQL status: OK in 0.6800000071525574 seconds
[0m22:00:33.997209 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 2.9699461460113525s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836431.0272632
[0m22:00:33.997209 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 2.9699461460113525s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836431.0272632
[0m22:00:33.997209 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.productmodel_snapshot"
[0m22:00:33.997209 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m22:00:34.229473 [debug] [Thread-1 (]: SQL status: OK in 0.2199999988079071 seconds
[0m22:00:34.230911 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 3.203648090362549s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836431.0272632
[0m22:00:34.230911 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 3.203648090362549s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836431.0272632
[0m22:00:34.230911 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.productmodel_snapshot"
[0m22:00:34.230911 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.productmodel_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
  
[0m22:00:34.397765 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m22:00:34.397765 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_dbt_spark.productmodel_snapshot"
[0m22:00:34.397765 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 3.37050199508667s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836431.0272632
[0m22:00:34.413396 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 3.37050199508667s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836431.0272632
[0m22:00:34.413867 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.productmodel_snapshot"
[0m22:00:34.414019 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.productmodel_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`productmodel_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m22:00:38.269605 [debug] [Thread-1 (]: SQL status: OK in 3.859999895095825 seconds
[0m22:00:38.269605 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
[0m22:00:38.269605 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 7.242342233657837s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836431.0272632
[0m22:00:38.269605 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 7.242342233657837s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836431.0272632
[0m22:00:38.269605 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.productmodel_snapshot"
[0m22:00:38.269605 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.productmodel_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.productmodel_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`productmodel_snapshot__dbt_tmp`
[0m22:00:38.686623 [debug] [Thread-1 (]: SQL status: OK in 0.41999998688697815 seconds
[0m22:00:38.705378 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m22:00:38.707375 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.productmodel_snapshot (execute): 22:00:31.027263 => 22:00:38.707375
[0m22:00:38.708376 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: _release sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836438.7073758
[0m22:00:38.709376 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: _release sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836438.7083764
[0m22:00:38.709376 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b4c3fcbb-4f23-4946-8c48-087595f6626d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAA8C990D0>]}
[0m22:00:38.710375 [info ] [Thread-1 (]: 5 of 7 OK snapshotted snapshots.productmodel_snapshot .......................... [[32mOK[0m in 7.68s]
[0m22:00:38.711148 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.productmodel_snapshot
[0m22:00:38.712156 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m22:00:38.713186 [info ] [Thread-1 (]: 6 of 7 START snapshot snapshots.salesorderdetail_snapshot ...................... [RUN]
[0m22:00:38.713865 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 0.005489349365234375s, acqrelcnt: 0, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836438.7083764
[0m22:00:38.714947 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.productmodel_snapshot, now snapshot.medallion_dbt_spark.salesorderdetail_snapshot)
[0m22:00:38.715873 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: reusing connection snapshot.medallion_dbt_spark.productmodel_snapshot sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 0.0065708160400390625s, acqrelcnt: 0, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836438.7083764
[0m22:00:38.716879 [debug] [Thread-1 (]: Databricks adapter: On thread (13924, 24776): `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` using default compute resource.
[0m22:00:38.716879 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: _acquire sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 0.008502960205078125s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836438.7083764
[0m22:00:38.718692 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m22:00:38.723252 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.salesorderdetail_snapshot (compile): 22:00:38.719242 => 22:00:38.723252
[0m22:00:38.723252 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m22:00:38.724322 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 0.015945911407470703s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836438.7083764
[0m22:00:38.724322 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 0.015945911407470703s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836438.7083764
[0m22:00:38.724322 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"
[0m22:00:38.724322 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m22:00:39.052843 [debug] [Thread-1 (]: SQL status: OK in 0.33000001311302185 seconds
[0m22:00:39.052843 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 0.3444674015045166s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836438.7083764
[0m22:00:39.052843 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 0.3444674015045166s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836438.7083764
[0m22:00:39.052843 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"
[0m22:00:39.052843 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"} */
select * from (
        



with source_data as (
    select 
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    from `hive_metastore`.`saleslt`.`salesorderdetail`
)
select *
from source_data
    ) as __dbt_sbq
    where false
    limit 0

    
[0m22:00:39.232308 [debug] [Thread-1 (]: SQL status: OK in 0.18000000715255737 seconds
[0m22:00:39.236510 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 0.5281341075897217s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836438.7083764
[0m22:00:39.236510 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 0.5281341075897217s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836438.7083764
[0m22:00:39.236510 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"
[0m22:00:39.236510 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m22:00:39.502285 [debug] [Thread-1 (]: SQL status: OK in 0.25 seconds
[0m22:00:39.503306 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 0.7949304580688477s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836438.7083764
[0m22:00:39.503306 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 0.7949304580688477s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836438.7083764
[0m22:00:39.503306 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"
[0m22:00:39.503306 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m22:00:39.746051 [debug] [Thread-1 (]: SQL status: OK in 0.23999999463558197 seconds
[0m22:00:39.752129 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 1.0427329540252686s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836438.7083764
[0m22:00:39.752129 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 1.0437533855438232s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836438.7083764
[0m22:00:39.753025 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"
[0m22:00:39.754139 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"} */

        create or replace view `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
  
  
  as
    with snapshot_query as (

        



with source_data as (
    select 
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    from `hive_metastore`.`saleslt`.`salesorderdetail`
)
select *
from source_data

    ),

    snapshotted_data as (

        select *,
            SalesOrderDetailID as dbt_unique_key

        from `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            SalesOrderDetailID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to,
            md5(coalesce(cast(SalesOrderDetailID as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            SalesOrderDetailID as dbt_unique_key,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_valid_to

        from snapshot_query
    ),

    deletes_source_data as (

        select
            *,
            SalesOrderDetailID as dbt_unique_key
        from snapshot_query
    ),
    

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`SalesOrderDetailID` != source_data.`SalesOrderDetailID`
        or
        (
            ((snapshotted_data.`SalesOrderDetailID` is null) and not (source_data.`SalesOrderDetailID` is null))
            or
            ((not snapshotted_data.`SalesOrderDetailID` is null) and (source_data.`SalesOrderDetailID` is null))
        ) or snapshotted_data.`OrderQty` != source_data.`OrderQty`
        or
        (
            ((snapshotted_data.`OrderQty` is null) and not (source_data.`OrderQty` is null))
            or
            ((not snapshotted_data.`OrderQty` is null) and (source_data.`OrderQty` is null))
        ) or snapshotted_data.`ProductID` != source_data.`ProductID`
        or
        (
            ((snapshotted_data.`ProductID` is null) and not (source_data.`ProductID` is null))
            or
            ((not snapshotted_data.`ProductID` is null) and (source_data.`ProductID` is null))
        ) or snapshotted_data.`UnitPrice` != source_data.`UnitPrice`
        or
        (
            ((snapshotted_data.`UnitPrice` is null) and not (source_data.`UnitPrice` is null))
            or
            ((not snapshotted_data.`UnitPrice` is null) and (source_data.`UnitPrice` is null))
        ) or snapshotted_data.`UnitPriceDiscount` != source_data.`UnitPriceDiscount`
        or
        (
            ((snapshotted_data.`UnitPriceDiscount` is null) and not (source_data.`UnitPriceDiscount` is null))
            or
            ((not snapshotted_data.`UnitPriceDiscount` is null) and (source_data.`UnitPriceDiscount` is null))
        ) or snapshotted_data.`LineTotal` != source_data.`LineTotal`
        or
        (
            ((snapshotted_data.`LineTotal` is null) and not (source_data.`LineTotal` is null))
            or
            ((not snapshotted_data.`LineTotal` is null) and (source_data.`LineTotal` is null))
        ))
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.`SalesOrderID` != source_data.`SalesOrderID`
        or
        (
            ((snapshotted_data.`SalesOrderID` is null) and not (source_data.`SalesOrderID` is null))
            or
            ((not snapshotted_data.`SalesOrderID` is null) and (source_data.`SalesOrderID` is null))
        ) or snapshotted_data.`SalesOrderDetailID` != source_data.`SalesOrderDetailID`
        or
        (
            ((snapshotted_data.`SalesOrderDetailID` is null) and not (source_data.`SalesOrderDetailID` is null))
            or
            ((not snapshotted_data.`SalesOrderDetailID` is null) and (source_data.`SalesOrderDetailID` is null))
        ) or snapshotted_data.`OrderQty` != source_data.`OrderQty`
        or
        (
            ((snapshotted_data.`OrderQty` is null) and not (source_data.`OrderQty` is null))
            or
            ((not snapshotted_data.`OrderQty` is null) and (source_data.`OrderQty` is null))
        ) or snapshotted_data.`ProductID` != source_data.`ProductID`
        or
        (
            ((snapshotted_data.`ProductID` is null) and not (source_data.`ProductID` is null))
            or
            ((not snapshotted_data.`ProductID` is null) and (source_data.`ProductID` is null))
        ) or snapshotted_data.`UnitPrice` != source_data.`UnitPrice`
        or
        (
            ((snapshotted_data.`UnitPrice` is null) and not (source_data.`UnitPrice` is null))
            or
            ((not snapshotted_data.`UnitPrice` is null) and (source_data.`UnitPrice` is null))
        ) or snapshotted_data.`UnitPriceDiscount` != source_data.`UnitPriceDiscount`
        or
        (
            ((snapshotted_data.`UnitPriceDiscount` is null) and not (source_data.`UnitPriceDiscount` is null))
            or
            ((not snapshotted_data.`UnitPriceDiscount` is null) and (source_data.`UnitPriceDiscount` is null))
        ) or snapshotted_data.`LineTotal` != source_data.`LineTotal`
        or
        (
            ((snapshotted_data.`LineTotal` is null) and not (source_data.`LineTotal` is null))
            or
            ((not snapshotted_data.`LineTotal` is null) and (source_data.`LineTotal` is null))
        ))
        )
    ),

    deletes as (

        select
            'delete' as dbt_change_type,
            source_data.*,
            
    current_timestamp()
 as dbt_valid_from,
            
    current_timestamp()
 as dbt_updated_at,
            
    current_timestamp()
 as dbt_valid_to,
            snapshotted_data.dbt_scd_id

        from snapshotted_data
        left join deletes_source_data as source_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where source_data.dbt_unique_key is null
    )

    select * from insertions
    union all
    select * from updates
    union all
    select * from deletes


    
[0m22:00:40.137503 [debug] [Thread-1 (]: SQL status: OK in 0.3799999952316284 seconds
[0m22:00:40.155769 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 1.4473927021026611s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836438.7083764
[0m22:00:40.155769 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 1.4473927021026611s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836438.7083764
[0m22:00:40.155769 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"
[0m22:00:40.155769 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
[0m22:00:40.304525 [debug] [Thread-1 (]: SQL status: OK in 0.15000000596046448 seconds
[0m22:00:40.320581 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 1.6122055053710938s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836438.7083764
[0m22:00:40.320581 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 1.6122055053710938s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836438.7083764
[0m22:00:40.320581 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"
[0m22:00:40.320581 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m22:00:40.554538 [debug] [Thread-1 (]: SQL status: OK in 0.23000000417232513 seconds
[0m22:00:40.554538 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 1.8461625576019287s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836438.7083764
[0m22:00:40.554538 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 1.8461625576019287s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836438.7083764
[0m22:00:40.554538 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"
[0m22:00:40.554538 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
[0m22:00:40.721038 [debug] [Thread-1 (]: SQL status: OK in 0.17000000178813934 seconds
[0m22:00:40.721038 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 2.012662172317505s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836438.7083764
[0m22:00:40.721038 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 2.012662172317505s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836438.7083764
[0m22:00:40.721038 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"
[0m22:00:40.721038 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m22:00:41.020439 [debug] [Thread-1 (]: SQL status: OK in 0.2800000011920929 seconds
[0m22:00:41.021525 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 2.3131494522094727s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836438.7083764
[0m22:00:41.021525 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 2.3131494522094727s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836438.7083764
[0m22:00:41.021525 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"
[0m22:00:41.021525 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"} */

      describe extended `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
  
[0m22:00:41.221553 [debug] [Thread-1 (]: SQL status: OK in 0.20000000298023224 seconds
[0m22:00:41.221553 [debug] [Thread-1 (]: Writing runtime SQL for node "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"
[0m22:00:41.221553 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 2.5131771564483643s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836438.7083764
[0m22:00:41.221553 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 2.5131771564483643s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836438.7083764
[0m22:00:41.221553 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"
[0m22:00:41.221553 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"} */

          merge into `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` as DBT_INTERNAL_DEST
    
      using `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp` as DBT_INTERNAL_SOURCE
    
    on DBT_INTERNAL_SOURCE.dbt_scd_id = DBT_INTERNAL_DEST.dbt_scd_id
    when matched
     and DBT_INTERNAL_DEST.dbt_valid_to is null
     and DBT_INTERNAL_SOURCE.dbt_change_type in ('update', 'delete')
        then update
        set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to

    when not matched
     and DBT_INTERNAL_SOURCE.dbt_change_type = 'insert'
        then insert *
    ;

      
[0m22:00:45.209010 [debug] [Thread-1 (]: SQL status: OK in 3.9700000286102295 seconds
[0m22:00:45.209706 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
[0m22:00:45.209706 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 6.501330137252808s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836438.7083764
[0m22:00:45.209706 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 6.501330137252808s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836438.7083764
[0m22:00:45.209706 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"
[0m22:00:45.209706 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.salesorderdetail_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.salesorderdetail_snapshot"} */
drop view if exists `hive_metastore`.`snapshots`.`salesorderdetail_snapshot__dbt_tmp`
[0m22:00:45.676799 [debug] [Thread-1 (]: SQL status: OK in 0.4699999988079071 seconds
[0m22:00:45.676799 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m22:00:45.676799 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.salesorderdetail_snapshot (execute): 22:00:38.724322 => 22:00:45.676799
[0m22:00:45.676799 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: _release sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836445.6768
[0m22:00:45.676799 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: _release sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836445.6768
[0m22:00:45.676799 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b4c3fcbb-4f23-4946-8c48-087595f6626d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAA86871D0>]}
[0m22:00:45.676799 [info ] [Thread-1 (]: 6 of 7 OK snapshotted snapshots.salesorderdetail_snapshot ...................... [[32mOK[0m in 6.96s]
[0m22:00:45.676799 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m22:00:45.676799 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.salesorderheader_snapshot
[0m22:00:45.676799 [info ] [Thread-1 (]: 7 of 7 START snapshot snapshots.salesorderheader_snapshot ...................... [RUN]
[0m22:00:45.676799 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836445.6768
[0m22:00:45.692497 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.salesorderdetail_snapshot, now snapshot.medallion_dbt_spark.salesorderheader_snapshot)
[0m22:00:45.693198 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: reusing connection snapshot.medallion_dbt_spark.salesorderdetail_snapshot sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle: 0.01639866828918457s, acqrelcnt: 0, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836445.6768
[0m22:00:45.693198 [debug] [Thread-1 (]: Databricks adapter: On thread (13924, 24776): `hive_metastore`.`snapshots`.`salesorderheader_snapshot` using default compute resource.
[0m22:00:45.693198 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: _acquire sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle: 0.01639866828918457s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836445.6768
[0m22:00:45.693198 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.salesorderheader_snapshot
[0m22:00:45.693198 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.salesorderheader_snapshot (compile): 22:00:45.693198 => 22:00:45.693198
[0m22:00:45.693198 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.salesorderheader_snapshot
[0m22:00:45.726737 [debug] [Thread-1 (]: Writing runtime sql for node "snapshot.medallion_dbt_spark.salesorderheader_snapshot"
[0m22:00:45.742364 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: get_thread_connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle: 0.06556439399719238s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836445.6768
[0m22:00:45.743437 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: idle check connection: sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle: 0.06663703918457031s, acqrelcnt: 1, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836445.6768
[0m22:00:45.743437 [debug] [Thread-1 (]: Using databricks connection "snapshot.medallion_dbt_spark.salesorderheader_snapshot"
[0m22:00:45.743437 [debug] [Thread-1 (]: On snapshot.medallion_dbt_spark.salesorderheader_snapshot: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "snapshot.medallion_dbt_spark.salesorderheader_snapshot"} */

          
  
    
        create or replace table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/salesorderheader/salesorderheader_snapshot'
      
      
      as
      

    select *,
        md5(coalesce(cast(SalesOrderId as string ), '')
         || '|' || coalesce(cast(
    current_timestamp()
 as string ), '')
        ) as dbt_scd_id,
        
    current_timestamp()
 as dbt_updated_at,
        
    current_timestamp()
 as dbt_valid_from,
        nullif(
    current_timestamp()
, 
    current_timestamp()
) as dbt_valid_to
    from (
        



with source_data as (
    select 
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment
    from `hive_metastore`.`saleslt`.`salesorderheader`
)
select *
from source_data
    ) sbq



  
      
[0m22:00:49.681357 [debug] [Thread-1 (]: SQL status: OK in 3.940000057220459 seconds
[0m22:00:49.681357 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m22:00:49.681357 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.salesorderheader_snapshot (execute): 22:00:45.693198 => 22:00:49.681357
[0m22:00:49.681357 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: _release sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836449.6813571
[0m22:00:49.681357 [debug] [Thread-1 (]: Databricks adapter: conn: 1832478041808: _release sess: 7c7def90-cec8-4f7d-b257-e2afe0b5b810, name: snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (13924, 24776), cmpt: ``, lut: 1707836449.6813571
[0m22:00:49.681357 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b4c3fcbb-4f23-4946-8c48-087595f6626d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAA81F0810>]}
[0m22:00:49.681357 [info ] [Thread-1 (]: 7 of 7 OK snapshotted snapshots.salesorderheader_snapshot ...................... [[32mOK[0m in 4.00s]
[0m22:00:49.681357 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.salesorderheader_snapshot
[0m22:00:49.697041 [debug] [MainThread]: Databricks adapter: conn: 1832481329488: idle check connection: sess: None, name: master, idle: 58.53024077415466s, acqrelcnt: 0, lang: None, thrd: (13924, 15656), cmpt: ``, lut: 1707836391.1668007
[0m22:00:49.697689 [debug] [MainThread]: Databricks adapter: conn: 1832481329488: reusing connection master sess: None, name: master, idle: 58.5308883190155s, acqrelcnt: 0, lang: None, thrd: (13924, 15656), cmpt: ``, lut: 1707836391.1668007
[0m22:00:49.697689 [debug] [MainThread]: Databricks adapter: Thread (13924, 15656) using default compute resource.
[0m22:00:49.697689 [debug] [MainThread]: Databricks adapter: conn: 1832481329488: _acquire sess: None, name: master, idle: 58.5308883190155s, acqrelcnt: 1, lang: None, thrd: (13924, 15656), cmpt: ``, lut: 1707836391.1668007
[0m22:00:49.697689 [debug] [MainThread]: Databricks adapter: conn: 1832481329488: get_thread_connection: sess: None, name: master, idle: 58.5308883190155s, acqrelcnt: 1, lang: None, thrd: (13924, 15656), cmpt: ``, lut: 1707836391.1668007
[0m22:00:49.697689 [debug] [MainThread]: Databricks adapter: conn: 1832481329488: idle check connection: sess: None, name: master, idle: 58.5308883190155s, acqrelcnt: 1, lang: None, thrd: (13924, 15656), cmpt: ``, lut: 1707836391.1668007
[0m22:00:49.697689 [debug] [MainThread]: On master: ROLLBACK
[0m22:00:49.697689 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:00:49.981430 [debug] [MainThread]: Databricks adapter: conn: 1832481329488: session opened sess: 87e016a2-5c8e-46bf-8927-111257e54325, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (13924, 15656), cmpt: ``, lut: 1707836449.9814308
[0m22:00:49.981430 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:00:49.981430 [debug] [MainThread]: Databricks adapter: conn: 1832481329488: get_thread_connection: sess: 87e016a2-5c8e-46bf-8927-111257e54325, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (13924, 15656), cmpt: ``, lut: 1707836449.9814308
[0m22:00:49.981430 [debug] [MainThread]: Databricks adapter: conn: 1832481329488: idle check connection: sess: 87e016a2-5c8e-46bf-8927-111257e54325, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (13924, 15656), cmpt: ``, lut: 1707836449.9814308
[0m22:00:49.981430 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:00:49.981430 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:00:49.997068 [debug] [MainThread]: Databricks adapter: conn: 1832481329488: _release sess: 87e016a2-5c8e-46bf-8927-111257e54325, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (13924, 15656), cmpt: ``, lut: 1707836449.9970684
[0m22:00:49.998006 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:00:49.998006 [debug] [MainThread]: On master: ROLLBACK
[0m22:00:49.998006 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:00:49.998006 [debug] [MainThread]: On master: Close
[0m22:00:50.114865 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m22:00:50.114865 [debug] [MainThread]: On list_hive_metastore: Close
[0m22:00:50.198233 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m22:00:50.198233 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m22:00:50.198233 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:00:50.198233 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m22:00:50.281804 [debug] [MainThread]: Connection 'snapshot.medallion_dbt_spark.salesorderheader_snapshot' was properly closed.
[0m22:00:50.281804 [debug] [MainThread]: On snapshot.medallion_dbt_spark.salesorderheader_snapshot: ROLLBACK
[0m22:00:50.298504 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:00:50.298504 [debug] [MainThread]: On snapshot.medallion_dbt_spark.salesorderheader_snapshot: Close
[0m22:00:50.398368 [info ] [MainThread]: 
[0m22:00:50.398368 [info ] [MainThread]: Finished running 7 snapshots in 0 hours 1 minutes and 1.11 seconds (61.11s).
[0m22:00:50.398368 [debug] [MainThread]: Command end result
[0m22:00:50.418379 [info ] [MainThread]: 
[0m22:00:50.419447 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:00:50.420936 [info ] [MainThread]: 
[0m22:00:50.420936 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m22:00:50.420936 [debug] [MainThread]: Command `dbt snapshot` succeeded at 22:00:50.420936 after 62.79 seconds
[0m22:00:50.420936 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AA96068410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AA8E938310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AA8EC163D0>]}
[0m22:00:50.420936 [debug] [MainThread]: Flushing usage events
[0m23:03:43.714554 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017EB2592150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017EB2593A50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017EB2593750>]}


============================== 23:03:43.721429 | 9cafeb2f-d903-47d9-bb45-19ee5243a270 ==============================
[0m23:03:43.721429 [info ] [MainThread]: Running with dbt=1.7.7
[0m23:03:43.723429 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt test', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m23:03:47.496855 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9cafeb2f-d903-47d9-bb45-19ee5243a270', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017EC46C2F90>]}
[0m23:03:47.591650 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9cafeb2f-d903-47d9-bb45-19ee5243a270', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017EB1137C50>]}
[0m23:03:47.592650 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m23:03:47.605558 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m23:03:48.553115 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 7 files added, 0 files changed.
[0m23:03:48.555110 [debug] [MainThread]: Partial parsing: added file: medallion_dbt_spark://models\marts\customer\dim_customer.sql
[0m23:03:48.556115 [debug] [MainThread]: Partial parsing: added file: medallion_dbt_spark://models\marts\product\dim_product.yml
[0m23:03:48.557778 [debug] [MainThread]: Partial parsing: added file: medallion_dbt_spark://models\marts\staging\bronze.yml
[0m23:03:48.558782 [debug] [MainThread]: Partial parsing: added file: medallion_dbt_spark://models\marts\product\dim_product.sql
[0m23:03:48.559867 [debug] [MainThread]: Partial parsing: added file: medallion_dbt_spark://models\marts\sales\dim_sales.yml
[0m23:03:48.560859 [debug] [MainThread]: Partial parsing: added file: medallion_dbt_spark://models\marts\sales\dim_sales.sql
[0m23:03:48.561866 [debug] [MainThread]: Partial parsing: added file: medallion_dbt_spark://models\marts\customer\dim_customer.yml
[0m23:03:49.190914 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_customers' in the 'models' section of file 'models\marts\customer\dim_customer.yml'
[0m23:03:49.214518 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.unique_dim_customers_customer_sk.22a014df62' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m23:03:49.217520 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_customers_customer_sk.8ae5836863' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m23:03:49.219641 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_customers_customerid.209fbdda85' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m23:03:49.221652 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_customers_AddressID.51b992e7e3' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m23:03:49.242987 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9cafeb2f-d903-47d9-bb45-19ee5243a270', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017EC4AA8DD0>]}
[0m23:03:49.282994 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9cafeb2f-d903-47d9-bb45-19ee5243a270', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017EC48F5550>]}
[0m23:03:49.283998 [info ] [MainThread]: Found 5 models, 8 tests, 7 snapshots, 9 sources, 0 exposures, 0 metrics, 535 macros, 0 groups, 0 semantic models
[0m23:03:49.312606 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9cafeb2f-d903-47d9-bb45-19ee5243a270', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017EC487C390>]}
[0m23:03:49.318116 [info ] [MainThread]: 
[0m23:03:49.322521 [debug] [MainThread]: Databricks adapter: conn: 1643977760336: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (16164, 17836), cmpt: ``, lut: None
[0m23:03:49.322521 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:03:49.322521 [debug] [MainThread]: Databricks adapter: Thread (16164, 17836) using default compute resource.
[0m23:03:49.322521 [debug] [MainThread]: Databricks adapter: conn: 1643977760336: _acquire sess: None, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (16164, 17836), cmpt: ``, lut: 1707840229.3225217
[0m23:03:49.333674 [debug] [ThreadPool]: Databricks adapter: conn: 1643980685520: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_snapshots, idle: 0s, acqrelcnt: 0, lang: None, thrd: (16164, 9860), cmpt: ``, lut: None
[0m23:03:49.334655 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_snapshots'
[0m23:03:49.334655 [debug] [ThreadPool]: Databricks adapter: Thread (16164, 9860) using default compute resource.
[0m23:03:49.335653 [debug] [ThreadPool]: Databricks adapter: conn: 1643980685520: _acquire sess: None, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (16164, 9860), cmpt: ``, lut: 1707840229.3356533
[0m23:03:49.343174 [debug] [ThreadPool]: Databricks adapter: conn: 1643980685520: get_thread_connection: sess: None, name: list_hive_metastore_snapshots, idle: 0.006527900695800781s, acqrelcnt: 1, lang: None, thrd: (16164, 9860), cmpt: ``, lut: 1707840229.3356533
[0m23:03:49.344177 [debug] [ThreadPool]: Databricks adapter: conn: 1643980685520: idle check connection: sess: None, name: list_hive_metastore_snapshots, idle: 0.007521390914916992s, acqrelcnt: 1, lang: None, thrd: (16164, 9860), cmpt: ``, lut: 1707840229.3356533
[0m23:03:49.344177 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m23:03:49.345173 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m23:03:49.346170 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:03:50.156605 [debug] [ThreadPool]: Databricks adapter: conn: 1643980685520: session opened sess: aad1d507-0e3d-48d2-adb7-0354f4a30e51, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (16164, 9860), cmpt: ``, lut: 1707840230.1556032
[0m23:03:50.627630 [debug] [ThreadPool]: SQL status: OK in 1.2799999713897705 seconds
[0m23:03:50.659065 [debug] [ThreadPool]: Databricks adapter: conn: 1643980685520: get_thread_connection: sess: aad1d507-0e3d-48d2-adb7-0354f4a30e51, name: list_hive_metastore_snapshots, idle: 0.5034627914428711s, acqrelcnt: 1, lang: None, thrd: (16164, 9860), cmpt: ``, lut: 1707840230.1556032
[0m23:03:50.660070 [debug] [ThreadPool]: Databricks adapter: conn: 1643980685520: idle check connection: sess: aad1d507-0e3d-48d2-adb7-0354f4a30e51, name: list_hive_metastore_snapshots, idle: 0.504467248916626s, acqrelcnt: 1, lang: None, thrd: (16164, 9860), cmpt: ``, lut: 1707840230.1556032
[0m23:03:50.660070 [debug] [ThreadPool]: Databricks adapter: conn: 1643980685520: get_thread_connection: sess: aad1d507-0e3d-48d2-adb7-0354f4a30e51, name: list_hive_metastore_snapshots, idle: 0.504467248916626s, acqrelcnt: 1, lang: None, thrd: (16164, 9860), cmpt: ``, lut: 1707840230.1556032
[0m23:03:50.661071 [debug] [ThreadPool]: Databricks adapter: conn: 1643980685520: idle check connection: sess: aad1d507-0e3d-48d2-adb7-0354f4a30e51, name: list_hive_metastore_snapshots, idle: 0.5054683685302734s, acqrelcnt: 1, lang: None, thrd: (16164, 9860), cmpt: ``, lut: 1707840230.1556032
[0m23:03:50.661071 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:03:50.662147 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m23:03:50.663153 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m23:03:50.956330 [debug] [ThreadPool]: SQL status: OK in 0.28999999165534973 seconds
[0m23:03:50.969074 [debug] [ThreadPool]: Databricks adapter: conn: 1643980685520: get_thread_connection: sess: aad1d507-0e3d-48d2-adb7-0354f4a30e51, name: list_hive_metastore_snapshots, idle: 0.8134710788726807s, acqrelcnt: 1, lang: None, thrd: (16164, 9860), cmpt: ``, lut: 1707840230.1556032
[0m23:03:50.970138 [debug] [ThreadPool]: Databricks adapter: conn: 1643980685520: idle check connection: sess: aad1d507-0e3d-48d2-adb7-0354f4a30e51, name: list_hive_metastore_snapshots, idle: 0.81453537940979s, acqrelcnt: 1, lang: None, thrd: (16164, 9860), cmpt: ``, lut: 1707840230.1556032
[0m23:03:50.970138 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m23:03:50.971075 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m23:03:51.138231 [debug] [ThreadPool]: SQL status: OK in 0.17000000178813934 seconds
[0m23:03:51.144318 [debug] [ThreadPool]: Databricks adapter: conn: 1643980685520: _release sess: aad1d507-0e3d-48d2-adb7-0354f4a30e51, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (16164, 9860), cmpt: ``, lut: 1707840231.1433258
[0m23:03:51.146318 [debug] [ThreadPool]: Databricks adapter: conn: 1643980685520: idle check connection: sess: aad1d507-0e3d-48d2-adb7-0354f4a30e51, name: list_hive_metastore_snapshots, idle: 0.0029931068420410156s, acqrelcnt: 0, lang: None, thrd: (16164, 9860), cmpt: ``, lut: 1707840231.1433258
[0m23:03:51.148917 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now list_hive_metastore_saleslt)
[0m23:03:51.148917 [debug] [ThreadPool]: Databricks adapter: conn: 1643980685520: reusing connection list_hive_metastore_snapshots sess: aad1d507-0e3d-48d2-adb7-0354f4a30e51, name: list_hive_metastore_saleslt, idle: 0.005591869354248047s, acqrelcnt: 0, lang: None, thrd: (16164, 9860), cmpt: ``, lut: 1707840231.1433258
[0m23:03:51.150203 [debug] [ThreadPool]: Databricks adapter: Thread (16164, 9860) using default compute resource.
[0m23:03:51.150203 [debug] [ThreadPool]: Databricks adapter: conn: 1643980685520: _acquire sess: aad1d507-0e3d-48d2-adb7-0354f4a30e51, name: list_hive_metastore_saleslt, idle: 0.0068781375885009766s, acqrelcnt: 1, lang: None, thrd: (16164, 9860), cmpt: ``, lut: 1707840231.1433258
[0m23:03:51.153207 [debug] [ThreadPool]: Databricks adapter: conn: 1643980685520: get_thread_connection: sess: aad1d507-0e3d-48d2-adb7-0354f4a30e51, name: list_hive_metastore_saleslt, idle: 0.009881734848022461s, acqrelcnt: 1, lang: None, thrd: (16164, 9860), cmpt: ``, lut: 1707840231.1433258
[0m23:03:51.154207 [debug] [ThreadPool]: Databricks adapter: conn: 1643980685520: idle check connection: sess: aad1d507-0e3d-48d2-adb7-0354f4a30e51, name: list_hive_metastore_saleslt, idle: 0.009881734848022461s, acqrelcnt: 1, lang: None, thrd: (16164, 9860), cmpt: ``, lut: 1707840231.1433258
[0m23:03:51.154207 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:03:51.154207 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m23:03:51.270879 [debug] [ThreadPool]: SQL status: OK in 0.11999999731779099 seconds
[0m23:03:51.281190 [debug] [ThreadPool]: Databricks adapter: conn: 1643980685520: get_thread_connection: sess: aad1d507-0e3d-48d2-adb7-0354f4a30e51, name: list_hive_metastore_saleslt, idle: 0.13685989379882812s, acqrelcnt: 1, lang: None, thrd: (16164, 9860), cmpt: ``, lut: 1707840231.1433258
[0m23:03:51.282308 [debug] [ThreadPool]: Databricks adapter: conn: 1643980685520: idle check connection: sess: aad1d507-0e3d-48d2-adb7-0354f4a30e51, name: list_hive_metastore_saleslt, idle: 0.13885998725891113s, acqrelcnt: 1, lang: None, thrd: (16164, 9860), cmpt: ``, lut: 1707840231.1433258
[0m23:03:51.282308 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:03:51.283310 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m23:03:51.416396 [debug] [ThreadPool]: SQL status: OK in 0.12999999523162842 seconds
[0m23:03:51.429007 [debug] [ThreadPool]: Databricks adapter: conn: 1643980685520: get_thread_connection: sess: aad1d507-0e3d-48d2-adb7-0354f4a30e51, name: list_hive_metastore_saleslt, idle: 0.28467464447021484s, acqrelcnt: 1, lang: None, thrd: (16164, 9860), cmpt: ``, lut: 1707840231.1433258
[0m23:03:51.430090 [debug] [ThreadPool]: Databricks adapter: conn: 1643980685520: idle check connection: sess: aad1d507-0e3d-48d2-adb7-0354f4a30e51, name: list_hive_metastore_saleslt, idle: 0.28676486015319824s, acqrelcnt: 1, lang: None, thrd: (16164, 9860), cmpt: ``, lut: 1707840231.1433258
[0m23:03:51.432055 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:03:51.432234 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m23:03:51.630232 [debug] [ThreadPool]: SQL status: OK in 0.20000000298023224 seconds
[0m23:03:51.633632 [debug] [ThreadPool]: Databricks adapter: conn: 1643980685520: _release sess: aad1d507-0e3d-48d2-adb7-0354f4a30e51, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (16164, 9860), cmpt: ``, lut: 1707840231.6336327
[0m23:03:51.637663 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9cafeb2f-d903-47d9-bb45-19ee5243a270', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017EC4A6ED10>]}
[0m23:03:51.637663 [debug] [MainThread]: Databricks adapter: conn: 1643977760336: get_thread_connection: sess: None, name: master, idle: 2.3151419162750244s, acqrelcnt: 1, lang: None, thrd: (16164, 17836), cmpt: ``, lut: 1707840229.3225217
[0m23:03:51.638856 [debug] [MainThread]: Databricks adapter: conn: 1643977760336: idle check connection: sess: None, name: master, idle: 2.3163347244262695s, acqrelcnt: 1, lang: None, thrd: (16164, 17836), cmpt: ``, lut: 1707840229.3225217
[0m23:03:51.638856 [debug] [MainThread]: Databricks adapter: conn: 1643977760336: get_thread_connection: sess: None, name: master, idle: 2.3163347244262695s, acqrelcnt: 1, lang: None, thrd: (16164, 17836), cmpt: ``, lut: 1707840229.3225217
[0m23:03:51.639847 [debug] [MainThread]: Databricks adapter: conn: 1643977760336: idle check connection: sess: None, name: master, idle: 2.3173258304595947s, acqrelcnt: 1, lang: None, thrd: (16164, 17836), cmpt: ``, lut: 1707840229.3225217
[0m23:03:51.639847 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:03:51.640945 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:03:51.640945 [debug] [MainThread]: Databricks adapter: conn: 1643977760336: _release sess: None, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (16164, 17836), cmpt: ``, lut: 1707840231.640945
[0m23:03:51.641842 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:03:51.642930 [info ] [MainThread]: 
[0m23:03:51.651838 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_name.af2288f4de
[0m23:03:51.653845 [info ] [Thread-1 (]: 1 of 8 START test not_null_dim_product_name .................................... [RUN]
[0m23:03:51.655761 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: Creating DatabricksDBTConnection sess: None, name: test.medallion_dbt_spark.not_null_dim_product_name.af2288f4de, idle: 0s, acqrelcnt: 0, lang: None, thrd: (16164, 16356), cmpt: ``, lut: None
[0m23:03:51.655761 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.medallion_dbt_spark.not_null_dim_product_name.af2288f4de'
[0m23:03:51.656848 [debug] [Thread-1 (]: Databricks adapter: On thread (16164, 16356): None using default compute resource.
[0m23:03:51.656848 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: _acquire sess: None, name: test.medallion_dbt_spark.not_null_dim_product_name.af2288f4de, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840231.6568482
[0m23:03:51.657823 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_name.af2288f4de
[0m23:03:51.683141 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_name.af2288f4de"
[0m23:03:51.687805 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_name.af2288f4de (compile): 23:03:51.657823 => 23:03:51.686143
[0m23:03:51.688763 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_name.af2288f4de
[0m23:03:51.706993 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_name.af2288f4de"
[0m23:03:51.709042 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_name.af2288f4de, idle: 0.05219388008117676s, acqrelcnt: 1, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840231.6568482
[0m23:03:51.710125 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_name.af2288f4de, idle: 0.05327773094177246s, acqrelcnt: 1, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840231.6568482
[0m23:03:51.711122 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_name.af2288f4de, idle: 0.05427432060241699s, acqrelcnt: 1, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840231.6568482
[0m23:03:51.711122 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_name.af2288f4de, idle: 0.05427432060241699s, acqrelcnt: 1, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840231.6568482
[0m23:03:51.712120 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m23:03:51.712120 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_name.af2288f4de"
[0m23:03:51.713131 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_name.af2288f4de: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_name.af2288f4de"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select name
from `hive_metastore`.`saleslt`.`dim_product`
where name is null



      
    ) dbt_internal_test
[0m23:03:51.713131 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:03:53.258293 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: session opened sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_dim_product_name.af2288f4de, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840233.258294
[0m23:03:53.506253 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_name.af2288f4de"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select name
from `hive_metastore`.`saleslt`.`dim_product`
where name is null



      
    ) dbt_internal_test
[0m23:03:53.508856 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:03:53.510944 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:671)
	... 35 more

[0m23:03:53.512857 [debug] [Thread-1 (]: Databricks adapter: operation-id: e8885ed7-2de8-45de-ae0d-552b8b24176f
[0m23:03:53.514942 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_name.af2288f4de (execute): 23:03:51.689764 => 23:03:53.513932
[0m23:03:53.515886 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: _release sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_dim_product_name.af2288f4de, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840233.5158863
[0m23:03:53.647746 [debug] [Thread-1 (]: Runtime Error in test not_null_dim_product_name (models\marts\product\dim_product.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:03:53.649262 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: _release sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_dim_product_name.af2288f4de, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840233.648253
[0m23:03:53.649262 [error] [Thread-1 (]: 1 of 8 ERROR not_null_dim_product_name ......................................... [[31mERROR[0m in 1.99s]
[0m23:03:53.651271 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_name.af2288f4de
[0m23:03:53.651271 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m23:03:53.652315 [info ] [Thread-1 (]: 2 of 8 START test not_null_dim_product_product_sk .............................. [RUN]
[0m23:03:53.654270 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: idle check connection: sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_dim_product_name.af2288f4de, idle: 0.006017208099365234s, acqrelcnt: 0, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840233.648253
[0m23:03:53.654270 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_name.af2288f4de, now test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417)
[0m23:03:53.655312 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: reusing connection test.medallion_dbt_spark.not_null_dim_product_name.af2288f4de sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.007059574127197266s, acqrelcnt: 0, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840233.648253
[0m23:03:53.655312 [debug] [Thread-1 (]: Databricks adapter: On thread (16164, 16356): None using default compute resource.
[0m23:03:53.656304 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: _acquire sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.008051872253417969s, acqrelcnt: 1, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840233.648253
[0m23:03:53.656304 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m23:03:53.660991 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m23:03:53.662906 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (compile): 23:03:53.656304 => 23:03:53.662906
[0m23:03:53.663903 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m23:03:53.669053 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m23:03:53.670127 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: get_thread_connection: sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.02187490463256836s, acqrelcnt: 1, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840233.648253
[0m23:03:53.671050 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: idle check connection: sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.022797822952270508s, acqrelcnt: 1, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840233.648253
[0m23:03:53.672049 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m23:03:53.672049 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_sk
from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is null



      
    ) dbt_internal_test
[0m23:03:54.478777 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_sk
from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is null



      
    ) dbt_internal_test
[0m23:03:54.479763 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:03:54.480762 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:671)
	... 35 more

[0m23:03:54.480762 [debug] [Thread-1 (]: Databricks adapter: operation-id: a94a923a-cb3e-49be-9aee-3376c2458c07
[0m23:03:54.481762 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (execute): 23:03:53.663903 => 23:03:54.481762
[0m23:03:54.482763 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: _release sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840234.4827635
[0m23:03:54.486201 [debug] [Thread-1 (]: Runtime Error in test not_null_dim_product_product_sk (models\marts\product\dim_product.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:03:54.486201 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: _release sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840234.4862015
[0m23:03:54.487773 [error] [Thread-1 (]: 2 of 8 ERROR not_null_dim_product_product_sk ................................... [[31mERROR[0m in 0.83s]
[0m23:03:54.489741 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m23:03:54.490750 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m23:03:54.491754 [info ] [Thread-1 (]: 3 of 8 START test not_null_dim_product_sellstartdate ........................... [RUN]
[0m23:03:54.494776 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: idle check connection: sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0075664520263671875s, acqrelcnt: 0, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840234.4862015
[0m23:03:54.495779 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, now test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546)
[0m23:03:54.496775 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.009577751159667969s, acqrelcnt: 0, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840234.4862015
[0m23:03:54.496775 [debug] [Thread-1 (]: Databricks adapter: On thread (16164, 16356): None using default compute resource.
[0m23:03:54.497776 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: _acquire sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.011574506759643555s, acqrelcnt: 1, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840234.4862015
[0m23:03:54.498891 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m23:03:54.506150 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m23:03:54.508940 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (compile): 23:03:54.498891 => 23:03:54.507938
[0m23:03:54.509493 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m23:03:54.514550 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m23:03:54.515499 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: get_thread_connection: sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.02929830551147461s, acqrelcnt: 1, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840234.4862015
[0m23:03:54.516568 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: idle check connection: sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.03036642074584961s, acqrelcnt: 1, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840234.4862015
[0m23:03:54.516568 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m23:03:54.518073 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select sellstartdate
from `hive_metastore`.`saleslt`.`dim_product`
where sellstartdate is null



      
    ) dbt_internal_test
[0m23:03:54.745054 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select sellstartdate
from `hive_metastore`.`saleslt`.`dim_product`
where sellstartdate is null



      
    ) dbt_internal_test
[0m23:03:54.745054 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:03:54.746054 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:671)
	... 35 more

[0m23:03:54.747054 [debug] [Thread-1 (]: Databricks adapter: operation-id: ca18a81d-1155-4316-b0f8-9d45265952f3
[0m23:03:54.748141 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (execute): 23:03:54.509493 => 23:03:54.747054
[0m23:03:54.749740 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: _release sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840234.7487338
[0m23:03:54.753852 [debug] [Thread-1 (]: Runtime Error in test not_null_dim_product_sellstartdate (models\marts\product\dim_product.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:03:54.754837 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: _release sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840234.7548373
[0m23:03:54.754837 [error] [Thread-1 (]: 3 of 8 ERROR not_null_dim_product_sellstartdate ................................ [[31mERROR[0m in 0.26s]
[0m23:03:54.756741 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m23:03:54.756741 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m23:03:54.757798 [info ] [Thread-1 (]: 4 of 8 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m23:03:54.759407 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: idle check connection: sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0045697689056396484s, acqrelcnt: 0, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840234.7548373
[0m23:03:54.759407 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, now test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710)
[0m23:03:54.760407 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: reusing connection test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.005569934844970703s, acqrelcnt: 0, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840234.7548373
[0m23:03:54.760407 [debug] [Thread-1 (]: Databricks adapter: On thread (16164, 16356): None using default compute resource.
[0m23:03:54.761406 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: _acquire sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.006569385528564453s, acqrelcnt: 1, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840234.7548373
[0m23:03:54.761406 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m23:03:54.766506 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m23:03:54.771020 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 23:03:54.762461 => 23:03:54.771020
[0m23:03:54.772025 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m23:03:54.780411 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m23:03:54.784336 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: get_thread_connection: sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.02850818634033203s, acqrelcnt: 1, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840234.7548373
[0m23:03:54.785335 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: idle check connection: sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.029498577117919922s, acqrelcnt: 1, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840234.7548373
[0m23:03:54.785335 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m23:03:54.786335 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m23:03:54.955493 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m23:03:54.955493 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:03:54.957891 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:671)
	... 35 more

[0m23:03:54.958796 [debug] [Thread-1 (]: Databricks adapter: operation-id: 193bd959-9c41-468f-b4a1-d820d645dbe3
[0m23:03:54.959797 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 23:03:54.773102 => 23:03:54.958796
[0m23:03:54.960798 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: _release sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840234.9607983
[0m23:03:54.964796 [debug] [Thread-1 (]: Runtime Error in test not_null_my_first_dbt_model_id (models\example\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:03:54.965797 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: _release sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840234.9657972
[0m23:03:54.966797 [error] [Thread-1 (]: 4 of 8 ERROR not_null_my_first_dbt_model_id .................................... [[31mERROR[0m in 0.21s]
[0m23:03:54.967797 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m23:03:54.969985 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m23:03:54.971985 [info ] [Thread-1 (]: 5 of 8 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m23:03:54.974221 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: idle check connection: sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.008424043655395508s, acqrelcnt: 0, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840234.9657972
[0m23:03:54.975011 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, now test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778)
[0m23:03:54.975011 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: reusing connection test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.009214162826538086s, acqrelcnt: 0, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840234.9657972
[0m23:03:54.976138 [debug] [Thread-1 (]: Databricks adapter: On thread (16164, 16356): None using default compute resource.
[0m23:03:54.976138 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: _acquire sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.010341405868530273s, acqrelcnt: 1, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840234.9657972
[0m23:03:54.976138 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m23:03:54.983100 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m23:03:54.984109 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (compile): 23:03:54.977648 => 23:03:54.984109
[0m23:03:54.985106 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m23:03:54.990643 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m23:03:54.992675 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: get_thread_connection: sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.02584671974182129s, acqrelcnt: 1, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840234.9657972
[0m23:03:54.993642 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: idle check connection: sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.02784585952758789s, acqrelcnt: 1, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840234.9657972
[0m23:03:54.993642 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m23:03:54.994693 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m23:03:55.327633 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m23:03:55.328726 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:03:55.329641 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:671)
	... 35 more

[0m23:03:55.329641 [debug] [Thread-1 (]: Databricks adapter: operation-id: dec818ed-b841-4d37-8581-6ffd4af17b81
[0m23:03:55.331098 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (execute): 23:03:54.985106 => 23:03:55.329641
[0m23:03:55.331098 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: _release sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840235.3310983
[0m23:03:55.334404 [debug] [Thread-1 (]: Runtime Error in test not_null_my_second_dbt_model_id (models\example\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:03:55.335424 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: _release sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840235.3354242
[0m23:03:55.336338 [error] [Thread-1 (]: 5 of 8 ERROR not_null_my_second_dbt_model_id ................................... [[31mERROR[0m in 0.36s]
[0m23:03:55.341855 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m23:03:55.341855 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m23:03:55.342923 [info ] [Thread-1 (]: 6 of 8 START test unique_dim_product_product_sk ................................ [RUN]
[0m23:03:55.344854 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: idle check connection: sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.009430646896362305s, acqrelcnt: 0, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840235.3354242
[0m23:03:55.344854 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, now test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c)
[0m23:03:55.345859 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: reusing connection test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.010434865951538086s, acqrelcnt: 0, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840235.3354242
[0m23:03:55.345859 [debug] [Thread-1 (]: Databricks adapter: On thread (16164, 16356): None using default compute resource.
[0m23:03:55.346854 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: _acquire sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.01143026351928711s, acqrelcnt: 1, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840235.3354242
[0m23:03:55.347856 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m23:03:55.358973 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m23:03:55.361002 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (compile): 23:03:55.347856 => 23:03:55.361002
[0m23:03:55.361914 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m23:03:55.365909 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m23:03:55.366913 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: get_thread_connection: sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.03148961067199707s, acqrelcnt: 1, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840235.3354242
[0m23:03:55.366913 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: idle check connection: sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.03148961067199707s, acqrelcnt: 1, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840235.3354242
[0m23:03:55.367912 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m23:03:55.368424 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    product_sk as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is not null
group by product_sk
having count(*) > 1



      
    ) dbt_internal_test
[0m23:03:55.525674 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    product_sk as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is not null
group by product_sk
having count(*) > 1



      
    ) dbt_internal_test
[0m23:03:55.526760 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
[0m23:03:55.526760 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:671)
	... 35 more

[0m23:03:55.527719 [debug] [Thread-1 (]: Databricks adapter: operation-id: 6685fa5e-dc6b-49e8-9e0a-f68771bf93ba
[0m23:03:55.529181 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (execute): 23:03:55.361914 => 23:03:55.527719
[0m23:03:55.529181 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: _release sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840235.5291817
[0m23:03:55.533676 [debug] [Thread-1 (]: Runtime Error in test unique_dim_product_product_sk (models\marts\product\dim_product.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
[0m23:03:55.534676 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: _release sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840235.5346768
[0m23:03:55.535690 [error] [Thread-1 (]: 6 of 8 ERROR unique_dim_product_product_sk ..................................... [[31mERROR[0m in 0.19s]
[0m23:03:55.537664 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m23:03:55.539269 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m23:03:55.539269 [info ] [Thread-1 (]: 7 of 8 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m23:03:55.541300 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: idle check connection: sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.006623268127441406s, acqrelcnt: 0, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840235.5346768
[0m23:03:55.542337 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, now test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321)
[0m23:03:55.542337 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: reusing connection test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.007661104202270508s, acqrelcnt: 0, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840235.5346768
[0m23:03:55.543320 [debug] [Thread-1 (]: Databricks adapter: On thread (16164, 16356): None using default compute resource.
[0m23:03:55.543320 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: _acquire sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.008643388748168945s, acqrelcnt: 1, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840235.5346768
[0m23:03:55.544269 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m23:03:55.550977 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m23:03:55.552882 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (compile): 23:03:55.544269 => 23:03:55.551966
[0m23:03:55.552882 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m23:03:55.557643 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m23:03:55.558652 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: get_thread_connection: sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.023975610733032227s, acqrelcnt: 1, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840235.5346768
[0m23:03:55.559653 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: idle check connection: sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.024976253509521484s, acqrelcnt: 1, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840235.5346768
[0m23:03:55.560662 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m23:03:55.560662 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m23:03:55.709971 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m23:03:55.710992 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
[0m23:03:55.710992 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:671)
	... 35 more

[0m23:03:55.712043 [debug] [Thread-1 (]: Databricks adapter: operation-id: ccd35136-acc7-4f5d-8311-63a69a1598a7
[0m23:03:55.714055 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (execute): 23:03:55.554123 => 23:03:55.713064
[0m23:03:55.714055 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: _release sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840235.714056
[0m23:03:55.719564 [debug] [Thread-1 (]: Runtime Error in test unique_my_first_dbt_model_id (models\example\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
[0m23:03:55.720571 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: _release sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840235.720572
[0m23:03:55.721571 [error] [Thread-1 (]: 7 of 8 ERROR unique_my_first_dbt_model_id ...................................... [[31mERROR[0m in 0.18s]
[0m23:03:55.723571 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m23:03:55.724617 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m23:03:55.725576 [info ] [Thread-1 (]: 8 of 8 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m23:03:55.726573 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: idle check connection: sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.006001710891723633s, acqrelcnt: 0, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840235.720572
[0m23:03:55.727665 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, now test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493)
[0m23:03:55.727665 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: reusing connection test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.007093191146850586s, acqrelcnt: 0, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840235.720572
[0m23:03:55.727665 [debug] [Thread-1 (]: Databricks adapter: On thread (16164, 16356): None using default compute resource.
[0m23:03:55.729160 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: _acquire sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.008588790893554688s, acqrelcnt: 1, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840235.720572
[0m23:03:55.729160 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m23:03:55.734351 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m23:03:55.736308 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (compile): 23:03:55.730169 => 23:03:55.735307
[0m23:03:55.736308 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m23:03:55.740824 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m23:03:55.741827 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: get_thread_connection: sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.021255016326904297s, acqrelcnt: 1, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840235.720572
[0m23:03:55.743312 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: idle check connection: sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.02274036407470703s, acqrelcnt: 1, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840235.720572
[0m23:03:55.744320 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m23:03:55.744320 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m23:03:55.890072 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m23:03:55.891047 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
[0m23:03:55.891047 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:671)
	... 35 more

[0m23:03:55.892092 [debug] [Thread-1 (]: Databricks adapter: operation-id: 3dc9a88e-bfa4-4ca8-acc0-acf3fbff773d
[0m23:03:55.893046 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (execute): 23:03:55.737815 => 23:03:55.892092
[0m23:03:55.893046 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: _release sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840235.8930464
[0m23:03:55.896141 [debug] [Thread-1 (]: Runtime Error in test unique_my_second_dbt_model_id (models\example\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
[0m23:03:55.896141 [debug] [Thread-1 (]: Databricks adapter: conn: 1643980541840: _release sess: beaaec20-d4e5-4f6d-b505-13fd764806c3, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16164, 16356), cmpt: ``, lut: 1707840235.896141
[0m23:03:55.896141 [error] [Thread-1 (]: 8 of 8 ERROR unique_my_second_dbt_model_id ..................................... [[31mERROR[0m in 0.17s]
[0m23:03:55.897836 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m23:03:55.899758 [debug] [MainThread]: Databricks adapter: conn: 1643977760336: idle check connection: sess: None, name: master, idle: 4.2588136196136475s, acqrelcnt: 0, lang: None, thrd: (16164, 17836), cmpt: ``, lut: 1707840231.640945
[0m23:03:55.901760 [debug] [MainThread]: Databricks adapter: conn: 1643977760336: reusing connection master sess: None, name: master, idle: 4.259814023971558s, acqrelcnt: 0, lang: None, thrd: (16164, 17836), cmpt: ``, lut: 1707840231.640945
[0m23:03:55.901760 [debug] [MainThread]: Databricks adapter: Thread (16164, 17836) using default compute resource.
[0m23:03:55.902759 [debug] [MainThread]: Databricks adapter: conn: 1643977760336: _acquire sess: None, name: master, idle: 4.2618138790130615s, acqrelcnt: 1, lang: None, thrd: (16164, 17836), cmpt: ``, lut: 1707840231.640945
[0m23:03:55.903758 [debug] [MainThread]: Databricks adapter: conn: 1643977760336: get_thread_connection: sess: None, name: master, idle: 4.262813329696655s, acqrelcnt: 1, lang: None, thrd: (16164, 17836), cmpt: ``, lut: 1707840231.640945
[0m23:03:55.904757 [debug] [MainThread]: Databricks adapter: conn: 1643977760336: idle check connection: sess: None, name: master, idle: 4.262813329696655s, acqrelcnt: 1, lang: None, thrd: (16164, 17836), cmpt: ``, lut: 1707840231.640945
[0m23:03:55.904757 [debug] [MainThread]: On master: ROLLBACK
[0m23:03:55.904757 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:03:56.256270 [debug] [MainThread]: Databricks adapter: conn: 1643977760336: session opened sess: 9dfc74d8-aeb5-4831-bfba-66bcf02f16f6, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (16164, 17836), cmpt: ``, lut: 1707840236.2562702
[0m23:03:56.256270 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:03:56.257836 [debug] [MainThread]: Databricks adapter: conn: 1643977760336: get_thread_connection: sess: 9dfc74d8-aeb5-4831-bfba-66bcf02f16f6, name: master, idle: 0.0015666484832763672s, acqrelcnt: 1, lang: None, thrd: (16164, 17836), cmpt: ``, lut: 1707840236.2562702
[0m23:03:56.258204 [debug] [MainThread]: Databricks adapter: conn: 1643977760336: idle check connection: sess: 9dfc74d8-aeb5-4831-bfba-66bcf02f16f6, name: master, idle: 0.0019338130950927734s, acqrelcnt: 1, lang: None, thrd: (16164, 17836), cmpt: ``, lut: 1707840236.2562702
[0m23:03:56.259254 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:03:56.259254 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:03:56.260410 [debug] [MainThread]: Databricks adapter: conn: 1643977760336: _release sess: 9dfc74d8-aeb5-4831-bfba-66bcf02f16f6, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (16164, 17836), cmpt: ``, lut: 1707840236.2592545
[0m23:03:56.261418 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:03:56.261418 [debug] [MainThread]: On master: ROLLBACK
[0m23:03:56.262418 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:03:56.262418 [debug] [MainThread]: On master: Close
[0m23:03:56.346792 [debug] [MainThread]: Connection 'list_hive_metastore_saleslt' was properly closed.
[0m23:03:56.347804 [debug] [MainThread]: On list_hive_metastore_saleslt: ROLLBACK
[0m23:03:56.349751 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:03:56.349751 [debug] [MainThread]: On list_hive_metastore_saleslt: Close
[0m23:03:56.456098 [debug] [MainThread]: Connection 'test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493' was properly closed.
[0m23:03:56.456098 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: ROLLBACK
[0m23:03:56.456098 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:03:56.457604 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m23:03:56.535672 [info ] [MainThread]: 
[0m23:03:56.536672 [info ] [MainThread]: Finished running 8 tests in 0 hours 0 minutes and 7.21 seconds (7.21s).
[0m23:03:56.539743 [debug] [MainThread]: Command end result
[0m23:03:56.558905 [info ] [MainThread]: 
[0m23:03:56.559902 [info ] [MainThread]: [31mCompleted with 8 errors and 0 warnings:[0m
[0m23:03:56.560901 [info ] [MainThread]: 
[0m23:03:56.561901 [error] [MainThread]:   Runtime Error in test not_null_dim_product_name (models\marts\product\dim_product.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:03:56.562901 [info ] [MainThread]: 
[0m23:03:56.564293 [error] [MainThread]:   Runtime Error in test not_null_dim_product_product_sk (models\marts\product\dim_product.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:03:56.566301 [info ] [MainThread]: 
[0m23:03:56.566301 [error] [MainThread]:   Runtime Error in test not_null_dim_product_sellstartdate (models\marts\product\dim_product.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:03:56.568820 [info ] [MainThread]: 
[0m23:03:56.569821 [error] [MainThread]:   Runtime Error in test not_null_my_first_dbt_model_id (models\example\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:03:56.570819 [info ] [MainThread]: 
[0m23:03:56.571818 [error] [MainThread]:   Runtime Error in test not_null_my_second_dbt_model_id (models\example\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:03:56.573819 [info ] [MainThread]: 
[0m23:03:56.574818 [error] [MainThread]:   Runtime Error in test unique_dim_product_product_sk (models\marts\product\dim_product.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
[0m23:03:56.576412 [info ] [MainThread]: 
[0m23:03:56.577979 [error] [MainThread]:   Runtime Error in test unique_my_first_dbt_model_id (models\example\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
[0m23:03:56.580987 [info ] [MainThread]: 
[0m23:03:56.581988 [error] [MainThread]:   Runtime Error in test unique_my_second_dbt_model_id (models\example\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
[0m23:03:56.583066 [info ] [MainThread]: 
[0m23:03:56.584079 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=8 SKIP=0 TOTAL=8
[0m23:03:56.586135 [debug] [MainThread]: Command `dbt test` failed at 23:03:56.586135 after 12.94 seconds
[0m23:03:56.587646 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017EB225BCD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017EB225B510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017EAB0E63D0>]}
[0m23:03:56.587646 [debug] [MainThread]: Flushing usage events
[0m23:28:06.853414 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DBC94C4D90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DBC912CB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DBC8E67B50>]}


============================== 23:28:06.859653 | a8a004e4-a086-415e-a3ba-5a6c79c4889b ==============================
[0m23:28:06.859653 [info ] [MainThread]: Running with dbt=1.7.7
[0m23:28:06.860655 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt test', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m23:28:08.499248 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a8a004e4-a086-415e-a3ba-5a6c79c4889b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DBDB5D1D50>]}
[0m23:28:08.568560 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a8a004e4-a086-415e-a3ba-5a6c79c4889b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DBC8E64C90>]}
[0m23:28:08.569557 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m23:28:08.579682 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m23:28:08.628483 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading medallion_dbt_spark: marts\product\dim_product.yml - Runtime Error
    Syntax error near line 22
    ------------------------------
    19 |     - name: weight
    20 |       description: The weight of the product
    21 |     - name:category
    22 |       description: The product category id
    23 |     - name: model
    24 |       description: The product model id
    25 |     - name: description
    
    Raw Error:
    ------------------------------
    mapping values are not allowed in this context
      in "<unicode string>", line 22, column 18
[0m23:28:08.633085 [debug] [MainThread]: Command `dbt test` failed at 23:28:08.633085 after 1.82 seconds
[0m23:28:08.634094 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DBC9515F10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DBC95137D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DBC91780D0>]}
[0m23:28:08.634094 [debug] [MainThread]: Flushing usage events
[0m23:29:14.566052 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000199DE402E10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000199DE3CC690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000199DE3CC3D0>]}


============================== 23:29:14.570064 | 53a1bce0-3316-4843-950c-27b19d8946ab ==============================
[0m23:29:14.570064 [info ] [MainThread]: Running with dbt=1.7.7
[0m23:29:14.571312 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt test', 'send_anonymous_usage_stats': 'True'}
[0m23:29:15.879588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '53a1bce0-3316-4843-950c-27b19d8946ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000199DE76C5D0>]}
[0m23:29:15.953146 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '53a1bce0-3316-4843-950c-27b19d8946ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000199F0A365D0>]}
[0m23:29:15.954149 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m23:29:15.963474 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m23:29:16.087000 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 5 files changed.
[0m23:29:16.087508 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://models\marts\sales\dim_sales.yml
[0m23:29:16.088516 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://models\marts\product\dim_product.yml
[0m23:29:16.088516 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://models\marts\customer\dim_customer.sql
[0m23:29:16.089517 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://models\marts\sales\dim_sales.sql
[0m23:29:16.089517 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://models\marts\product\dim_product.sql
[0m23:29:16.336766 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '53a1bce0-3316-4843-950c-27b19d8946ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000199F109FC50>]}
[0m23:29:16.352231 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '53a1bce0-3316-4843-950c-27b19d8946ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000199F0F48A90>]}
[0m23:29:16.353231 [info ] [MainThread]: Found 5 models, 8 tests, 7 snapshots, 9 sources, 0 exposures, 0 metrics, 535 macros, 0 groups, 0 semantic models
[0m23:29:16.354195 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '53a1bce0-3316-4843-950c-27b19d8946ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000199F0D8B250>]}
[0m23:29:16.356190 [info ] [MainThread]: 
[0m23:29:16.357345 [debug] [MainThread]: Databricks adapter: conn: 1760681174800: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (16728, 19348), cmpt: ``, lut: None
[0m23:29:16.358347 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:29:16.358347 [debug] [MainThread]: Databricks adapter: Thread (16728, 19348) using default compute resource.
[0m23:29:16.358347 [debug] [MainThread]: Databricks adapter: conn: 1760681174800: _acquire sess: None, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (16728, 19348), cmpt: ``, lut: 1707841756.358348
[0m23:29:16.360918 [debug] [ThreadPool]: Databricks adapter: conn: 1760686028176: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_saleslt, idle: 0s, acqrelcnt: 0, lang: None, thrd: (16728, 21484), cmpt: ``, lut: None
[0m23:29:16.361902 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m23:29:16.361902 [debug] [ThreadPool]: Databricks adapter: Thread (16728, 21484) using default compute resource.
[0m23:29:16.362901 [debug] [ThreadPool]: Databricks adapter: conn: 1760686028176: _acquire sess: None, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (16728, 21484), cmpt: ``, lut: 1707841756.361902
[0m23:29:16.366900 [debug] [ThreadPool]: Databricks adapter: conn: 1760686028176: get_thread_connection: sess: None, name: list_hive_metastore_saleslt, idle: 0.004998683929443359s, acqrelcnt: 1, lang: None, thrd: (16728, 21484), cmpt: ``, lut: 1707841756.361902
[0m23:29:16.367908 [debug] [ThreadPool]: Databricks adapter: conn: 1760686028176: idle check connection: sess: None, name: list_hive_metastore_saleslt, idle: 0.006006002426147461s, acqrelcnt: 1, lang: None, thrd: (16728, 21484), cmpt: ``, lut: 1707841756.361902
[0m23:29:16.368904 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:29:16.369411 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m23:29:16.369411 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:29:16.977113 [debug] [ThreadPool]: Databricks adapter: conn: 1760686028176: session opened sess: d0c6afcb-92c1-460d-9ffc-26ad824acef9, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (16728, 21484), cmpt: ``, lut: 1707841756.9771135
[0m23:29:17.462939 [debug] [ThreadPool]: SQL status: OK in 1.090000033378601 seconds
[0m23:29:17.489973 [debug] [ThreadPool]: Databricks adapter: conn: 1760686028176: get_thread_connection: sess: d0c6afcb-92c1-460d-9ffc-26ad824acef9, name: list_hive_metastore_saleslt, idle: 0.5128600597381592s, acqrelcnt: 1, lang: None, thrd: (16728, 21484), cmpt: ``, lut: 1707841756.9771135
[0m23:29:17.490975 [debug] [ThreadPool]: Databricks adapter: conn: 1760686028176: idle check connection: sess: d0c6afcb-92c1-460d-9ffc-26ad824acef9, name: list_hive_metastore_saleslt, idle: 0.5138614177703857s, acqrelcnt: 1, lang: None, thrd: (16728, 21484), cmpt: ``, lut: 1707841756.9771135
[0m23:29:17.490975 [debug] [ThreadPool]: Databricks adapter: conn: 1760686028176: get_thread_connection: sess: d0c6afcb-92c1-460d-9ffc-26ad824acef9, name: list_hive_metastore_saleslt, idle: 0.5138614177703857s, acqrelcnt: 1, lang: None, thrd: (16728, 21484), cmpt: ``, lut: 1707841756.9771135
[0m23:29:17.491977 [debug] [ThreadPool]: Databricks adapter: conn: 1760686028176: idle check connection: sess: d0c6afcb-92c1-460d-9ffc-26ad824acef9, name: list_hive_metastore_saleslt, idle: 0.5148637294769287s, acqrelcnt: 1, lang: None, thrd: (16728, 21484), cmpt: ``, lut: 1707841756.9771135
[0m23:29:17.491977 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:29:17.492974 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:29:17.492974 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m23:29:17.675272 [debug] [ThreadPool]: SQL status: OK in 0.18000000715255737 seconds
[0m23:29:17.682274 [debug] [ThreadPool]: Databricks adapter: conn: 1760686028176: get_thread_connection: sess: d0c6afcb-92c1-460d-9ffc-26ad824acef9, name: list_hive_metastore_saleslt, idle: 0.704077959060669s, acqrelcnt: 1, lang: None, thrd: (16728, 21484), cmpt: ``, lut: 1707841756.9771135
[0m23:29:17.682414 [debug] [ThreadPool]: Databricks adapter: conn: 1760686028176: idle check connection: sess: d0c6afcb-92c1-460d-9ffc-26ad824acef9, name: list_hive_metastore_saleslt, idle: 0.70530104637146s, acqrelcnt: 1, lang: None, thrd: (16728, 21484), cmpt: ``, lut: 1707841756.9771135
[0m23:29:17.682414 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:29:17.683423 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m23:29:17.921340 [debug] [ThreadPool]: SQL status: OK in 0.23999999463558197 seconds
[0m23:29:17.924541 [debug] [ThreadPool]: Databricks adapter: conn: 1760686028176: _release sess: d0c6afcb-92c1-460d-9ffc-26ad824acef9, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (16728, 21484), cmpt: ``, lut: 1707841757.9245415
[0m23:29:17.926543 [debug] [ThreadPool]: Databricks adapter: conn: 1760686028176: idle check connection: sess: d0c6afcb-92c1-460d-9ffc-26ad824acef9, name: list_hive_metastore_saleslt, idle: 0.0010039806365966797s, acqrelcnt: 0, lang: None, thrd: (16728, 21484), cmpt: ``, lut: 1707841757.9245415
[0m23:29:17.928544 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m23:29:17.929542 [debug] [ThreadPool]: Databricks adapter: conn: 1760686028176: reusing connection list_hive_metastore_saleslt sess: d0c6afcb-92c1-460d-9ffc-26ad824acef9, name: list_hive_metastore_snapshots, idle: 0.005001068115234375s, acqrelcnt: 0, lang: None, thrd: (16728, 21484), cmpt: ``, lut: 1707841757.9245415
[0m23:29:17.929542 [debug] [ThreadPool]: Databricks adapter: Thread (16728, 21484) using default compute resource.
[0m23:29:17.930542 [debug] [ThreadPool]: Databricks adapter: conn: 1760686028176: _acquire sess: d0c6afcb-92c1-460d-9ffc-26ad824acef9, name: list_hive_metastore_snapshots, idle: 0.00600123405456543s, acqrelcnt: 1, lang: None, thrd: (16728, 21484), cmpt: ``, lut: 1707841757.9245415
[0m23:29:17.932775 [debug] [ThreadPool]: Databricks adapter: conn: 1760686028176: get_thread_connection: sess: d0c6afcb-92c1-460d-9ffc-26ad824acef9, name: list_hive_metastore_snapshots, idle: 0.008234500885009766s, acqrelcnt: 1, lang: None, thrd: (16728, 21484), cmpt: ``, lut: 1707841757.9245415
[0m23:29:17.933778 [debug] [ThreadPool]: Databricks adapter: conn: 1760686028176: idle check connection: sess: d0c6afcb-92c1-460d-9ffc-26ad824acef9, name: list_hive_metastore_snapshots, idle: 0.009236812591552734s, acqrelcnt: 1, lang: None, thrd: (16728, 21484), cmpt: ``, lut: 1707841757.9245415
[0m23:29:17.933778 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m23:29:17.934774 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m23:29:18.120197 [debug] [ThreadPool]: SQL status: OK in 0.1899999976158142 seconds
[0m23:29:18.126484 [debug] [ThreadPool]: Databricks adapter: conn: 1760686028176: get_thread_connection: sess: d0c6afcb-92c1-460d-9ffc-26ad824acef9, name: list_hive_metastore_snapshots, idle: 0.20194292068481445s, acqrelcnt: 1, lang: None, thrd: (16728, 21484), cmpt: ``, lut: 1707841757.9245415
[0m23:29:18.127488 [debug] [ThreadPool]: Databricks adapter: conn: 1760686028176: idle check connection: sess: d0c6afcb-92c1-460d-9ffc-26ad824acef9, name: list_hive_metastore_snapshots, idle: 0.20294713973999023s, acqrelcnt: 1, lang: None, thrd: (16728, 21484), cmpt: ``, lut: 1707841757.9245415
[0m23:29:18.127488 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m23:29:18.128488 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m23:29:18.255565 [debug] [ThreadPool]: SQL status: OK in 0.12999999523162842 seconds
[0m23:29:18.266030 [debug] [ThreadPool]: Databricks adapter: conn: 1760686028176: get_thread_connection: sess: d0c6afcb-92c1-460d-9ffc-26ad824acef9, name: list_hive_metastore_snapshots, idle: 0.3414890766143799s, acqrelcnt: 1, lang: None, thrd: (16728, 21484), cmpt: ``, lut: 1707841757.9245415
[0m23:29:18.267027 [debug] [ThreadPool]: Databricks adapter: conn: 1760686028176: idle check connection: sess: d0c6afcb-92c1-460d-9ffc-26ad824acef9, name: list_hive_metastore_snapshots, idle: 0.3424854278564453s, acqrelcnt: 1, lang: None, thrd: (16728, 21484), cmpt: ``, lut: 1707841757.9245415
[0m23:29:18.268026 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m23:29:18.268026 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m23:29:18.412132 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m23:29:18.417145 [debug] [ThreadPool]: Databricks adapter: conn: 1760686028176: _release sess: d0c6afcb-92c1-460d-9ffc-26ad824acef9, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (16728, 21484), cmpt: ``, lut: 1707841758.4161305
[0m23:29:18.419083 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '53a1bce0-3316-4843-950c-27b19d8946ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000199F0F45A50>]}
[0m23:29:18.420047 [debug] [MainThread]: Databricks adapter: conn: 1760681174800: get_thread_connection: sess: None, name: master, idle: 2.061699151992798s, acqrelcnt: 1, lang: None, thrd: (16728, 19348), cmpt: ``, lut: 1707841756.358348
[0m23:29:18.421057 [debug] [MainThread]: Databricks adapter: conn: 1760681174800: idle check connection: sess: None, name: master, idle: 2.062709093093872s, acqrelcnt: 1, lang: None, thrd: (16728, 19348), cmpt: ``, lut: 1707841756.358348
[0m23:29:18.421057 [debug] [MainThread]: Databricks adapter: conn: 1760681174800: get_thread_connection: sess: None, name: master, idle: 2.062709093093872s, acqrelcnt: 1, lang: None, thrd: (16728, 19348), cmpt: ``, lut: 1707841756.358348
[0m23:29:18.422048 [debug] [MainThread]: Databricks adapter: conn: 1760681174800: idle check connection: sess: None, name: master, idle: 2.062709093093872s, acqrelcnt: 1, lang: None, thrd: (16728, 19348), cmpt: ``, lut: 1707841756.358348
[0m23:29:18.422048 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:29:18.422048 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:29:18.423047 [debug] [MainThread]: Databricks adapter: conn: 1760681174800: _release sess: None, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (16728, 19348), cmpt: ``, lut: 1707841758.423047
[0m23:29:18.424046 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:29:18.425199 [info ] [MainThread]: 
[0m23:29:18.432629 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m23:29:18.433652 [info ] [Thread-1 (]: 1 of 8 START test not_null_dim_product_product_name ............................ [RUN]
[0m23:29:18.434628 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: Creating DatabricksDBTConnection sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0s, acqrelcnt: 0, lang: None, thrd: (16728, 21756), cmpt: ``, lut: None
[0m23:29:18.435634 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5'
[0m23:29:18.435634 [debug] [Thread-1 (]: Databricks adapter: On thread (16728, 21756): None using default compute resource.
[0m23:29:18.436631 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: _acquire sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841758.436631
[0m23:29:18.436631 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m23:29:18.459913 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m23:29:18.461913 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (compile): 23:29:18.437630 => 23:29:18.461913
[0m23:29:18.462898 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m23:29:18.485274 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m23:29:18.487273 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.05064249038696289s, acqrelcnt: 1, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841758.436631
[0m23:29:18.488271 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.05164074897766113s, acqrelcnt: 1, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841758.436631
[0m23:29:18.489272 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.052641868591308594s, acqrelcnt: 1, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841758.436631
[0m23:29:18.490275 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.052641868591308594s, acqrelcnt: 1, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841758.436631
[0m23:29:18.490275 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m23:29:18.491274 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m23:29:18.492295 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_name
from `hive_metastore`.`saleslt`.`dim_product`
where product_name is null



      
    ) dbt_internal_test
[0m23:29:18.493275 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:29:18.747795 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: session opened sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841758.747795
[0m23:29:18.855352 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_name
from `hive_metastore`.`saleslt`.`dim_product`
where product_name is null



      
    ) dbt_internal_test
[0m23:29:18.856364 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:29:18.856364 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:671)
	... 35 more

[0m23:29:18.857350 [debug] [Thread-1 (]: Databricks adapter: operation-id: 3101df06-f064-4a3e-adb7-a7d3f1f0a498
[0m23:29:18.857350 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (execute): 23:29:18.462898 => 23:29:18.857350
[0m23:29:18.858312 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: _release sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841758.8583126
[0m23:29:18.874820 [debug] [Thread-1 (]: Runtime Error in test not_null_dim_product_product_name (models\marts\product\dim_product.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:29:18.874820 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: _release sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841758.87482
[0m23:29:18.875912 [error] [Thread-1 (]: 1 of 8 ERROR not_null_dim_product_product_name ................................. [[31mERROR[0m in 0.44s]
[0m23:29:18.876829 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m23:29:18.877837 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m23:29:18.877837 [info ] [Thread-1 (]: 2 of 8 START test not_null_dim_product_product_sk .............................. [RUN]
[0m23:29:18.879821 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: idle check connection: sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.00500178337097168s, acqrelcnt: 0, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841758.87482
[0m23:29:18.879821 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, now test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417)
[0m23:29:18.880832 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.006012916564941406s, acqrelcnt: 0, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841758.87482
[0m23:29:18.880832 [debug] [Thread-1 (]: Databricks adapter: On thread (16728, 21756): None using default compute resource.
[0m23:29:18.881820 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: _acquire sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.006012916564941406s, acqrelcnt: 1, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841758.87482
[0m23:29:18.881947 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m23:29:18.887968 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m23:29:18.888951 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (compile): 23:29:18.881947 => 23:29:18.888951
[0m23:29:18.889951 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m23:29:18.893031 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m23:29:18.893954 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: get_thread_connection: sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.019134044647216797s, acqrelcnt: 1, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841758.87482
[0m23:29:18.893954 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: idle check connection: sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.019134044647216797s, acqrelcnt: 1, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841758.87482
[0m23:29:18.894995 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m23:29:18.894995 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_sk
from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is null



      
    ) dbt_internal_test
[0m23:29:19.010293 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_sk
from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is null



      
    ) dbt_internal_test
[0m23:29:19.011331 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:29:19.012244 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:671)
	... 35 more

[0m23:29:19.012244 [debug] [Thread-1 (]: Databricks adapter: operation-id: 9fc56749-6eee-441d-9ba1-6b72738c9128
[0m23:29:19.013241 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (execute): 23:29:18.889951 => 23:29:19.012244
[0m23:29:19.013241 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: _release sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.0132418
[0m23:29:19.016338 [debug] [Thread-1 (]: Runtime Error in test not_null_dim_product_product_sk (models\marts\product\dim_product.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:29:19.016338 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: _release sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.016338
[0m23:29:19.017329 [error] [Thread-1 (]: 2 of 8 ERROR not_null_dim_product_product_sk ................................... [[31mERROR[0m in 0.14s]
[0m23:29:19.018241 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m23:29:19.019242 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m23:29:19.019242 [info ] [Thread-1 (]: 3 of 8 START test not_null_dim_product_sellstartdate ........................... [RUN]
[0m23:29:19.021310 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: idle check connection: sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.003958940505981445s, acqrelcnt: 0, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.016338
[0m23:29:19.021310 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, now test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546)
[0m23:29:19.022299 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.005961179733276367s, acqrelcnt: 0, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.016338
[0m23:29:19.022299 [debug] [Thread-1 (]: Databricks adapter: On thread (16728, 21756): None using default compute resource.
[0m23:29:19.023352 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: _acquire sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.005961179733276367s, acqrelcnt: 1, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.016338
[0m23:29:19.023352 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m23:29:19.029614 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m23:29:19.030616 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (compile): 23:29:19.023352 => 23:29:19.030616
[0m23:29:19.031773 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m23:29:19.035247 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m23:29:19.037242 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: get_thread_connection: sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.019908666610717773s, acqrelcnt: 1, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.016338
[0m23:29:19.038243 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: idle check connection: sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0209047794342041s, acqrelcnt: 1, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.016338
[0m23:29:19.038243 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m23:29:19.039272 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select sellstartdate
from `hive_metastore`.`saleslt`.`dim_product`
where sellstartdate is null



      
    ) dbt_internal_test
[0m23:29:19.137706 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select sellstartdate
from `hive_metastore`.`saleslt`.`dim_product`
where sellstartdate is null



      
    ) dbt_internal_test
[0m23:29:19.137706 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:29:19.138710 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:671)
	... 35 more

[0m23:29:19.138710 [debug] [Thread-1 (]: Databricks adapter: operation-id: 44fc7a8a-4845-4e32-8d31-1e340f9a2c6d
[0m23:29:19.139706 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (execute): 23:29:19.032231 => 23:29:19.139706
[0m23:29:19.140706 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: _release sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.1397061
[0m23:29:19.142706 [debug] [Thread-1 (]: Runtime Error in test not_null_dim_product_sellstartdate (models\marts\product\dim_product.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:29:19.143706 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: _release sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0010001659393310547s, acqrelcnt: 0, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.142706
[0m23:29:19.143706 [error] [Thread-1 (]: 3 of 8 ERROR not_null_dim_product_sellstartdate ................................ [[31mERROR[0m in 0.12s]
[0m23:29:19.145713 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m23:29:19.145713 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m23:29:19.146710 [info ] [Thread-1 (]: 4 of 8 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m23:29:19.147711 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: idle check connection: sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0050051212310791016s, acqrelcnt: 0, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.142706
[0m23:29:19.148725 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, now test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710)
[0m23:29:19.148725 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: reusing connection test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.00601959228515625s, acqrelcnt: 0, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.142706
[0m23:29:19.149709 [debug] [Thread-1 (]: Databricks adapter: On thread (16728, 21756): None using default compute resource.
[0m23:29:19.149926 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: _acquire sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.00722050666809082s, acqrelcnt: 1, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.142706
[0m23:29:19.149926 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m23:29:19.154929 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m23:29:19.155928 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 23:29:19.149926 => 23:29:19.154929
[0m23:29:19.156929 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m23:29:19.159197 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m23:29:19.160197 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: get_thread_connection: sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.01749134063720703s, acqrelcnt: 1, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.142706
[0m23:29:19.160197 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: idle check connection: sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.01749134063720703s, acqrelcnt: 1, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.142706
[0m23:29:19.161195 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m23:29:19.161195 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m23:29:19.262825 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m23:29:19.264731 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:29:19.265750 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:671)
	... 35 more

[0m23:29:19.267727 [debug] [Thread-1 (]: Databricks adapter: operation-id: b54aadda-b5a5-4ebe-bf84-056e346ad03e
[0m23:29:19.268751 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 23:29:19.157193 => 23:29:19.268751
[0m23:29:19.269801 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: _release sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.2698011
[0m23:29:19.272837 [debug] [Thread-1 (]: Runtime Error in test not_null_my_first_dbt_model_id (models\example\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:29:19.273728 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: _release sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.2728379
[0m23:29:19.273972 [error] [Thread-1 (]: 4 of 8 ERROR not_null_my_first_dbt_model_id .................................... [[31mERROR[0m in 0.13s]
[0m23:29:19.274989 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m23:29:19.275977 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m23:29:19.275977 [info ] [Thread-1 (]: 5 of 8 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m23:29:19.277138 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: idle check connection: sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.004300355911254883s, acqrelcnt: 0, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.2728379
[0m23:29:19.278235 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, now test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778)
[0m23:29:19.278235 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: reusing connection test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.005397796630859375s, acqrelcnt: 0, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.2728379
[0m23:29:19.279198 [debug] [Thread-1 (]: Databricks adapter: On thread (16728, 21756): None using default compute resource.
[0m23:29:19.279198 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: _acquire sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.006360292434692383s, acqrelcnt: 1, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.2728379
[0m23:29:19.280159 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m23:29:19.283616 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m23:29:19.284612 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (compile): 23:29:19.280159 => 23:29:19.284612
[0m23:29:19.285536 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m23:29:19.287603 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m23:29:19.288604 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: get_thread_connection: sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.01576709747314453s, acqrelcnt: 1, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.2728379
[0m23:29:19.289529 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: idle check connection: sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.01576709747314453s, acqrelcnt: 1, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.2728379
[0m23:29:19.289529 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m23:29:19.289529 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m23:29:19.391948 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m23:29:19.391948 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:29:19.392947 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:671)
	... 35 more

[0m23:29:19.393948 [debug] [Thread-1 (]: Databricks adapter: operation-id: 7651bb2d-101e-467e-befd-1f52a825a563
[0m23:29:19.393948 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (execute): 23:29:19.285536 => 23:29:19.393948
[0m23:29:19.394947 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: _release sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.3949475
[0m23:29:19.397946 [debug] [Thread-1 (]: Runtime Error in test not_null_my_second_dbt_model_id (models\example\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:29:19.397946 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: _release sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.3979468
[0m23:29:19.398947 [error] [Thread-1 (]: 5 of 8 ERROR not_null_my_second_dbt_model_id ................................... [[31mERROR[0m in 0.12s]
[0m23:29:19.400159 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m23:29:19.401246 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m23:29:19.402163 [info ] [Thread-1 (]: 6 of 8 START test unique_dim_product_product_sk ................................ [RUN]
[0m23:29:19.403413 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: idle check connection: sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0054662227630615234s, acqrelcnt: 0, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.3979468
[0m23:29:19.404419 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, now test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c)
[0m23:29:19.404419 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: reusing connection test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.006473064422607422s, acqrelcnt: 0, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.3979468
[0m23:29:19.404419 [debug] [Thread-1 (]: Databricks adapter: On thread (16728, 21756): None using default compute resource.
[0m23:29:19.405419 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: _acquire sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0074727535247802734s, acqrelcnt: 1, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.3979468
[0m23:29:19.405419 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m23:29:19.411791 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m23:29:19.412725 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (compile): 23:29:19.406426 => 23:29:19.411791
[0m23:29:19.412725 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m23:29:19.415794 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m23:29:19.416719 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: get_thread_connection: sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.018772363662719727s, acqrelcnt: 1, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.3979468
[0m23:29:19.416719 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: idle check connection: sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.018772363662719727s, acqrelcnt: 1, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.3979468
[0m23:29:19.417716 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m23:29:19.417716 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    product_sk as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is not null
group by product_sk
having count(*) > 1



      
    ) dbt_internal_test
[0m23:29:19.520675 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    product_sk as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is not null
group by product_sk
having count(*) > 1



      
    ) dbt_internal_test
[0m23:29:19.521675 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
[0m23:29:19.521675 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:671)
	... 35 more

[0m23:29:19.522674 [debug] [Thread-1 (]: Databricks adapter: operation-id: 9c6480fe-21c6-4df6-a79f-aefdf40a2489
[0m23:29:19.522674 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (execute): 23:29:19.412725 => 23:29:19.522674
[0m23:29:19.523989 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: _release sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.5238185
[0m23:29:19.526993 [debug] [Thread-1 (]: Runtime Error in test unique_dim_product_product_sk (models\marts\product\dim_product.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
[0m23:29:19.527994 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: _release sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.5269935
[0m23:29:19.527994 [error] [Thread-1 (]: 6 of 8 ERROR unique_dim_product_product_sk ..................................... [[31mERROR[0m in 0.12s]
[0m23:29:19.533118 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m23:29:19.534121 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m23:29:19.535118 [info ] [Thread-1 (]: 7 of 8 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m23:29:19.537165 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: idle check connection: sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.010171890258789062s, acqrelcnt: 0, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.5269935
[0m23:29:19.537165 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, now test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321)
[0m23:29:19.538119 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: reusing connection test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.010171890258789062s, acqrelcnt: 0, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.5269935
[0m23:29:19.538119 [debug] [Thread-1 (]: Databricks adapter: On thread (16728, 21756): None using default compute resource.
[0m23:29:19.539116 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: _acquire sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.011126279830932617s, acqrelcnt: 1, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.5269935
[0m23:29:19.539116 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m23:29:19.545208 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m23:29:19.546121 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (compile): 23:29:19.539116 => 23:29:19.546121
[0m23:29:19.547117 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m23:29:19.550319 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m23:29:19.552312 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: get_thread_connection: sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.02531886100769043s, acqrelcnt: 1, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.5269935
[0m23:29:19.553315 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: idle check connection: sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.026322126388549805s, acqrelcnt: 1, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.5269935
[0m23:29:19.554412 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m23:29:19.554412 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m23:29:19.666443 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m23:29:19.667526 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
[0m23:29:19.668468 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:671)
	... 35 more

[0m23:29:19.669443 [debug] [Thread-1 (]: Databricks adapter: operation-id: a8890712-851e-4230-8426-4c48bac9f8ad
[0m23:29:19.669443 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (execute): 23:29:19.548151 => 23:29:19.669443
[0m23:29:19.670486 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: _release sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.670486
[0m23:29:19.672527 [debug] [Thread-1 (]: Runtime Error in test unique_my_first_dbt_model_id (models\example\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
[0m23:29:19.673529 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: _release sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.6735291
[0m23:29:19.673529 [error] [Thread-1 (]: 7 of 8 ERROR unique_my_first_dbt_model_id ...................................... [[31mERROR[0m in 0.14s]
[0m23:29:19.674653 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m23:29:19.675659 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m23:29:19.675659 [info ] [Thread-1 (]: 8 of 8 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m23:29:19.677657 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: idle check connection: sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.004128217697143555s, acqrelcnt: 0, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.6735291
[0m23:29:19.677657 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, now test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493)
[0m23:29:19.678655 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: reusing connection test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.005126953125s, acqrelcnt: 0, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.6735291
[0m23:29:19.678655 [debug] [Thread-1 (]: Databricks adapter: On thread (16728, 21756): None using default compute resource.
[0m23:29:19.679659 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: _acquire sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.006129741668701172s, acqrelcnt: 1, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.6735291
[0m23:29:19.679659 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m23:29:19.684361 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m23:29:19.685279 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (compile): 23:29:19.680701 => 23:29:19.685279
[0m23:29:19.685279 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m23:29:19.688357 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m23:29:19.689271 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: get_thread_connection: sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.015742063522338867s, acqrelcnt: 1, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.6735291
[0m23:29:19.689271 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: idle check connection: sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.015742063522338867s, acqrelcnt: 1, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.6735291
[0m23:29:19.690270 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m23:29:19.690270 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m23:29:19.793973 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m23:29:19.794973 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
[0m23:29:19.795970 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:671)
	... 35 more

[0m23:29:19.795970 [debug] [Thread-1 (]: Databricks adapter: operation-id: 1e5afde5-b6d7-4384-8947-20ba374c87dc
[0m23:29:19.796973 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (execute): 23:29:19.686272 => 23:29:19.796973
[0m23:29:19.796973 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: _release sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.7969737
[0m23:29:19.799974 [debug] [Thread-1 (]: Runtime Error in test unique_my_second_dbt_model_id (models\example\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
[0m23:29:19.800973 [debug] [Thread-1 (]: Databricks adapter: conn: 1760684533328: _release sess: 1eef3a0b-50c6-4b9e-8773-e689252426dd, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16728, 21756), cmpt: ``, lut: 1707841759.8009734
[0m23:29:19.800973 [error] [Thread-1 (]: 8 of 8 ERROR unique_my_second_dbt_model_id ..................................... [[31mERROR[0m in 0.12s]
[0m23:29:19.802973 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m23:29:19.804025 [debug] [MainThread]: Databricks adapter: conn: 1760681174800: idle check connection: sess: None, name: master, idle: 1.3809788227081299s, acqrelcnt: 0, lang: None, thrd: (16728, 19348), cmpt: ``, lut: 1707841758.423047
[0m23:29:19.804974 [debug] [MainThread]: Databricks adapter: conn: 1760681174800: reusing connection master sess: None, name: master, idle: 1.381927728652954s, acqrelcnt: 0, lang: None, thrd: (16728, 19348), cmpt: ``, lut: 1707841758.423047
[0m23:29:19.804974 [debug] [MainThread]: Databricks adapter: Thread (16728, 19348) using default compute resource.
[0m23:29:19.805973 [debug] [MainThread]: Databricks adapter: conn: 1760681174800: _acquire sess: None, name: master, idle: 1.3829267024993896s, acqrelcnt: 1, lang: None, thrd: (16728, 19348), cmpt: ``, lut: 1707841758.423047
[0m23:29:19.805973 [debug] [MainThread]: Databricks adapter: conn: 1760681174800: get_thread_connection: sess: None, name: master, idle: 1.3829267024993896s, acqrelcnt: 1, lang: None, thrd: (16728, 19348), cmpt: ``, lut: 1707841758.423047
[0m23:29:19.805973 [debug] [MainThread]: Databricks adapter: conn: 1760681174800: idle check connection: sess: None, name: master, idle: 1.3829267024993896s, acqrelcnt: 1, lang: None, thrd: (16728, 19348), cmpt: ``, lut: 1707841758.423047
[0m23:29:19.806973 [debug] [MainThread]: On master: ROLLBACK
[0m23:29:19.806973 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:29:20.049817 [debug] [MainThread]: Databricks adapter: conn: 1760681174800: session opened sess: 123f4e69-4c7a-46c7-92cd-54fceed033f0, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (16728, 19348), cmpt: ``, lut: 1707841760.0498173
[0m23:29:20.049817 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:29:20.050909 [debug] [MainThread]: Databricks adapter: conn: 1760681174800: get_thread_connection: sess: 123f4e69-4c7a-46c7-92cd-54fceed033f0, name: master, idle: 0.0010917186737060547s, acqrelcnt: 1, lang: None, thrd: (16728, 19348), cmpt: ``, lut: 1707841760.0498173
[0m23:29:20.050909 [debug] [MainThread]: Databricks adapter: conn: 1760681174800: idle check connection: sess: 123f4e69-4c7a-46c7-92cd-54fceed033f0, name: master, idle: 0.0010917186737060547s, acqrelcnt: 1, lang: None, thrd: (16728, 19348), cmpt: ``, lut: 1707841760.0498173
[0m23:29:20.051819 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:29:20.051819 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:29:20.051819 [debug] [MainThread]: Databricks adapter: conn: 1760681174800: _release sess: 123f4e69-4c7a-46c7-92cd-54fceed033f0, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (16728, 19348), cmpt: ``, lut: 1707841760.0518198
[0m23:29:20.052821 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:29:20.053874 [debug] [MainThread]: On master: ROLLBACK
[0m23:29:20.053874 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:29:20.054818 [debug] [MainThread]: On master: Close
[0m23:29:20.140005 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m23:29:20.140994 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m23:29:20.142973 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:29:20.143919 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m23:29:20.217149 [debug] [MainThread]: Connection 'test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493' was properly closed.
[0m23:29:20.218174 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: ROLLBACK
[0m23:29:20.218174 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:29:20.219088 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m23:29:20.298509 [info ] [MainThread]: 
[0m23:29:20.299764 [info ] [MainThread]: Finished running 8 tests in 0 hours 0 minutes and 3.94 seconds (3.94s).
[0m23:29:20.301765 [debug] [MainThread]: Command end result
[0m23:29:20.313106 [info ] [MainThread]: 
[0m23:29:20.315019 [info ] [MainThread]: [31mCompleted with 8 errors and 0 warnings:[0m
[0m23:29:20.316021 [info ] [MainThread]: 
[0m23:29:20.317019 [error] [MainThread]:   Runtime Error in test not_null_dim_product_product_name (models\marts\product\dim_product.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:29:20.318022 [info ] [MainThread]: 
[0m23:29:20.319104 [error] [MainThread]:   Runtime Error in test not_null_dim_product_product_sk (models\marts\product\dim_product.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:29:20.321114 [info ] [MainThread]: 
[0m23:29:20.322114 [error] [MainThread]:   Runtime Error in test not_null_dim_product_sellstartdate (models\marts\product\dim_product.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:29:20.324206 [info ] [MainThread]: 
[0m23:29:20.325347 [error] [MainThread]:   Runtime Error in test not_null_my_first_dbt_model_id (models\example\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:29:20.327352 [info ] [MainThread]: 
[0m23:29:20.328346 [error] [MainThread]:   Runtime Error in test not_null_my_second_dbt_model_id (models\example\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:29:20.329346 [info ] [MainThread]: 
[0m23:29:20.330346 [error] [MainThread]:   Runtime Error in test unique_dim_product_product_sk (models\marts\product\dim_product.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
[0m23:29:20.331494 [info ] [MainThread]: 
[0m23:29:20.332502 [error] [MainThread]:   Runtime Error in test unique_my_first_dbt_model_id (models\example\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
[0m23:29:20.333504 [info ] [MainThread]: 
[0m23:29:20.334498 [error] [MainThread]:   Runtime Error in test unique_my_second_dbt_model_id (models\example\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
[0m23:29:20.336500 [info ] [MainThread]: 
[0m23:29:20.337504 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=8 SKIP=0 TOTAL=8
[0m23:29:20.340500 [debug] [MainThread]: Command `dbt test` failed at 23:29:20.339499 after 5.81 seconds
[0m23:29:20.340500 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000199DE3ED050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000199DE76C910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000199D752D010>]}
[0m23:29:20.341499 [debug] [MainThread]: Flushing usage events
[0m23:32:54.958005 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F20192310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F1FFB56D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F20757150>]}


============================== 23:32:54.962076 | 0511681d-05b8-445c-8b39-0b4895d3b4ae ==============================
[0m23:32:54.962076 [info ] [MainThread]: Running with dbt=1.7.7
[0m23:32:54.963080 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt test', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m23:32:56.184970 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0511681d-05b8-445c-8b39-0b4895d3b4ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F328B4D10>]}
[0m23:32:56.282218 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0511681d-05b8-445c-8b39-0b4895d3b4ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F327C7F90>]}
[0m23:32:56.283224 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m23:32:56.296981 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m23:32:56.313177 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m23:32:56.314085 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '0511681d-05b8-445c-8b39-0b4895d3b4ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F32B221D0>]}
[0m23:32:57.702173 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'dim_customers' in the 'models' section of file 'models\marts\customer\dim_customer.yml'
[0m23:32:57.784364 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.unique_dim_customers_customer_sk.22a014df62' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m23:32:57.785291 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_customers_customer_sk.8ae5836863' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m23:32:57.786814 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_customers_customerid.209fbdda85' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m23:32:57.787328 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.medallion_dbt_spark.not_null_dim_customers_AddressID.51b992e7e3' (models\marts\customer\dim_customer.yml) depends on a node named 'dim_customers' in package '' which was not found
[0m23:32:57.822354 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0511681d-05b8-445c-8b39-0b4895d3b4ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F32C4B5D0>]}
[0m23:32:57.840855 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0511681d-05b8-445c-8b39-0b4895d3b4ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F32F9CC50>]}
[0m23:32:57.841853 [info ] [MainThread]: Found 5 models, 7 snapshots, 8 tests, 9 sources, 0 exposures, 0 metrics, 535 macros, 0 groups, 0 semantic models
[0m23:32:57.842854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0511681d-05b8-445c-8b39-0b4895d3b4ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F329ACD90>]}
[0m23:32:57.845855 [info ] [MainThread]: 
[0m23:32:57.847361 [debug] [MainThread]: Databricks adapter: conn: 2264302907728: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (16492, 25872), cmpt: ``, lut: None
[0m23:32:57.848373 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:32:57.848373 [debug] [MainThread]: Databricks adapter: Thread (16492, 25872) using default compute resource.
[0m23:32:57.848373 [debug] [MainThread]: Databricks adapter: conn: 2264302907728: _acquire sess: None, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (16492, 25872), cmpt: ``, lut: 1707841977.848373
[0m23:32:57.850705 [debug] [ThreadPool]: Databricks adapter: conn: 2264302998864: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_saleslt, idle: 0s, acqrelcnt: 0, lang: None, thrd: (16492, 24988), cmpt: ``, lut: None
[0m23:32:57.851719 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m23:32:57.851719 [debug] [ThreadPool]: Databricks adapter: Thread (16492, 24988) using default compute resource.
[0m23:32:57.852712 [debug] [ThreadPool]: Databricks adapter: conn: 2264302998864: _acquire sess: None, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (16492, 24988), cmpt: ``, lut: 1707841977.8527126
[0m23:32:57.858846 [debug] [ThreadPool]: Databricks adapter: conn: 2264302998864: get_thread_connection: sess: None, name: list_hive_metastore_saleslt, idle: 0.0051326751708984375s, acqrelcnt: 1, lang: None, thrd: (16492, 24988), cmpt: ``, lut: 1707841977.8527126
[0m23:32:57.858846 [debug] [ThreadPool]: Databricks adapter: conn: 2264302998864: idle check connection: sess: None, name: list_hive_metastore_saleslt, idle: 0.0061337947845458984s, acqrelcnt: 1, lang: None, thrd: (16492, 24988), cmpt: ``, lut: 1707841977.8527126
[0m23:32:57.859843 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:32:57.859843 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m23:32:57.860844 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:32:58.287015 [debug] [ThreadPool]: Databricks adapter: conn: 2264302998864: session opened sess: d9488a1c-4f95-4a12-9dd5-b66c21cd2b1c, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (16492, 24988), cmpt: ``, lut: 1707841978.2870152
[0m23:32:58.708239 [debug] [ThreadPool]: SQL status: OK in 0.8500000238418579 seconds
[0m23:32:58.722396 [debug] [ThreadPool]: Databricks adapter: conn: 2264302998864: get_thread_connection: sess: d9488a1c-4f95-4a12-9dd5-b66c21cd2b1c, name: list_hive_metastore_saleslt, idle: 0.4353814125061035s, acqrelcnt: 1, lang: None, thrd: (16492, 24988), cmpt: ``, lut: 1707841978.2870152
[0m23:32:58.723286 [debug] [ThreadPool]: Databricks adapter: conn: 2264302998864: idle check connection: sess: d9488a1c-4f95-4a12-9dd5-b66c21cd2b1c, name: list_hive_metastore_saleslt, idle: 0.4353814125061035s, acqrelcnt: 1, lang: None, thrd: (16492, 24988), cmpt: ``, lut: 1707841978.2870152
[0m23:32:58.723286 [debug] [ThreadPool]: Databricks adapter: conn: 2264302998864: get_thread_connection: sess: d9488a1c-4f95-4a12-9dd5-b66c21cd2b1c, name: list_hive_metastore_saleslt, idle: 0.43627142906188965s, acqrelcnt: 1, lang: None, thrd: (16492, 24988), cmpt: ``, lut: 1707841978.2870152
[0m23:32:58.723286 [debug] [ThreadPool]: Databricks adapter: conn: 2264302998864: idle check connection: sess: d9488a1c-4f95-4a12-9dd5-b66c21cd2b1c, name: list_hive_metastore_saleslt, idle: 0.43627142906188965s, acqrelcnt: 1, lang: None, thrd: (16492, 24988), cmpt: ``, lut: 1707841978.2870152
[0m23:32:58.724565 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:32:58.724565 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:32:58.724565 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m23:32:58.926632 [debug] [ThreadPool]: SQL status: OK in 0.20000000298023224 seconds
[0m23:32:58.933952 [debug] [ThreadPool]: Databricks adapter: conn: 2264302998864: get_thread_connection: sess: d9488a1c-4f95-4a12-9dd5-b66c21cd2b1c, name: list_hive_metastore_saleslt, idle: 0.6458487510681152s, acqrelcnt: 1, lang: None, thrd: (16492, 24988), cmpt: ``, lut: 1707841978.2870152
[0m23:32:58.933952 [debug] [ThreadPool]: Databricks adapter: conn: 2264302998864: idle check connection: sess: d9488a1c-4f95-4a12-9dd5-b66c21cd2b1c, name: list_hive_metastore_saleslt, idle: 0.6469376087188721s, acqrelcnt: 1, lang: None, thrd: (16492, 24988), cmpt: ``, lut: 1707841978.2870152
[0m23:32:58.934866 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:32:58.934866 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m23:32:59.092295 [debug] [ThreadPool]: SQL status: OK in 0.1599999964237213 seconds
[0m23:32:59.095295 [debug] [ThreadPool]: Databricks adapter: conn: 2264302998864: _release sess: d9488a1c-4f95-4a12-9dd5-b66c21cd2b1c, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (16492, 24988), cmpt: ``, lut: 1707841979.095296
[0m23:32:59.097007 [debug] [ThreadPool]: Databricks adapter: conn: 2264302998864: idle check connection: sess: d9488a1c-4f95-4a12-9dd5-b66c21cd2b1c, name: list_hive_metastore_saleslt, idle: 0.0016012191772460938s, acqrelcnt: 0, lang: None, thrd: (16492, 24988), cmpt: ``, lut: 1707841979.095296
[0m23:32:59.098990 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m23:32:59.098990 [debug] [ThreadPool]: Databricks adapter: conn: 2264302998864: reusing connection list_hive_metastore_saleslt sess: d9488a1c-4f95-4a12-9dd5-b66c21cd2b1c, name: list_hive_metastore_snapshots, idle: 0.0036950111389160156s, acqrelcnt: 0, lang: None, thrd: (16492, 24988), cmpt: ``, lut: 1707841979.095296
[0m23:32:59.100129 [debug] [ThreadPool]: Databricks adapter: Thread (16492, 24988) using default compute resource.
[0m23:32:59.100129 [debug] [ThreadPool]: Databricks adapter: conn: 2264302998864: _acquire sess: d9488a1c-4f95-4a12-9dd5-b66c21cd2b1c, name: list_hive_metastore_snapshots, idle: 0.0048334598541259766s, acqrelcnt: 1, lang: None, thrd: (16492, 24988), cmpt: ``, lut: 1707841979.095296
[0m23:32:59.102138 [debug] [ThreadPool]: Databricks adapter: conn: 2264302998864: get_thread_connection: sess: d9488a1c-4f95-4a12-9dd5-b66c21cd2b1c, name: list_hive_metastore_snapshots, idle: 0.006842136383056641s, acqrelcnt: 1, lang: None, thrd: (16492, 24988), cmpt: ``, lut: 1707841979.095296
[0m23:32:59.103222 [debug] [ThreadPool]: Databricks adapter: conn: 2264302998864: idle check connection: sess: d9488a1c-4f95-4a12-9dd5-b66c21cd2b1c, name: list_hive_metastore_snapshots, idle: 0.00792694091796875s, acqrelcnt: 1, lang: None, thrd: (16492, 24988), cmpt: ``, lut: 1707841979.095296
[0m23:32:59.103222 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m23:32:59.104131 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m23:32:59.251938 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m23:32:59.257486 [debug] [ThreadPool]: Databricks adapter: conn: 2264302998864: get_thread_connection: sess: d9488a1c-4f95-4a12-9dd5-b66c21cd2b1c, name: list_hive_metastore_snapshots, idle: 0.16219019889831543s, acqrelcnt: 1, lang: None, thrd: (16492, 24988), cmpt: ``, lut: 1707841979.095296
[0m23:32:59.258063 [debug] [ThreadPool]: Databricks adapter: conn: 2264302998864: idle check connection: sess: d9488a1c-4f95-4a12-9dd5-b66c21cd2b1c, name: list_hive_metastore_snapshots, idle: 0.16276788711547852s, acqrelcnt: 1, lang: None, thrd: (16492, 24988), cmpt: ``, lut: 1707841979.095296
[0m23:32:59.258063 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m23:32:59.259107 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m23:32:59.418132 [debug] [ThreadPool]: SQL status: OK in 0.1599999964237213 seconds
[0m23:32:59.422053 [debug] [ThreadPool]: Databricks adapter: conn: 2264302998864: get_thread_connection: sess: d9488a1c-4f95-4a12-9dd5-b66c21cd2b1c, name: list_hive_metastore_snapshots, idle: 0.32675743103027344s, acqrelcnt: 1, lang: None, thrd: (16492, 24988), cmpt: ``, lut: 1707841979.095296
[0m23:32:59.423056 [debug] [ThreadPool]: Databricks adapter: conn: 2264302998864: idle check connection: sess: d9488a1c-4f95-4a12-9dd5-b66c21cd2b1c, name: list_hive_metastore_snapshots, idle: 0.3277609348297119s, acqrelcnt: 1, lang: None, thrd: (16492, 24988), cmpt: ``, lut: 1707841979.095296
[0m23:32:59.423056 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m23:32:59.423056 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m23:32:59.570685 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m23:32:59.574818 [debug] [ThreadPool]: Databricks adapter: conn: 2264302998864: _release sess: d9488a1c-4f95-4a12-9dd5-b66c21cd2b1c, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (16492, 24988), cmpt: ``, lut: 1707841979.5746808
[0m23:32:59.577836 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0511681d-05b8-445c-8b39-0b4895d3b4ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F32D71C90>]}
[0m23:32:59.577836 [debug] [MainThread]: Databricks adapter: conn: 2264302907728: get_thread_connection: sess: None, name: master, idle: 1.7294635772705078s, acqrelcnt: 1, lang: None, thrd: (16492, 25872), cmpt: ``, lut: 1707841977.848373
[0m23:32:59.578823 [debug] [MainThread]: Databricks adapter: conn: 2264302907728: idle check connection: sess: None, name: master, idle: 1.7304506301879883s, acqrelcnt: 1, lang: None, thrd: (16492, 25872), cmpt: ``, lut: 1707841977.848373
[0m23:32:59.578823 [debug] [MainThread]: Databricks adapter: conn: 2264302907728: get_thread_connection: sess: None, name: master, idle: 1.7304506301879883s, acqrelcnt: 1, lang: None, thrd: (16492, 25872), cmpt: ``, lut: 1707841977.848373
[0m23:32:59.579822 [debug] [MainThread]: Databricks adapter: conn: 2264302907728: idle check connection: sess: None, name: master, idle: 1.7314491271972656s, acqrelcnt: 1, lang: None, thrd: (16492, 25872), cmpt: ``, lut: 1707841977.848373
[0m23:32:59.579822 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:32:59.580822 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:32:59.580822 [debug] [MainThread]: Databricks adapter: conn: 2264302907728: _release sess: None, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (16492, 25872), cmpt: ``, lut: 1707841979.5808225
[0m23:32:59.582001 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:32:59.583008 [info ] [MainThread]: 
[0m23:32:59.587002 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m23:32:59.588094 [info ] [Thread-1 (]: 1 of 8 START test not_null_dim_product_product_name ............................ [RUN]
[0m23:32:59.590003 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: Creating DatabricksDBTConnection sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0s, acqrelcnt: 0, lang: None, thrd: (16492, 4336), cmpt: ``, lut: None
[0m23:32:59.590003 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5'
[0m23:32:59.591004 [debug] [Thread-1 (]: Databricks adapter: On thread (16492, 4336): None using default compute resource.
[0m23:32:59.591004 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: _acquire sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841979.5910046
[0m23:32:59.592003 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m23:32:59.612244 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m23:32:59.614753 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (compile): 23:32:59.592003 => 23:32:59.613243
[0m23:32:59.615764 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m23:32:59.633284 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m23:32:59.635251 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.043247222900390625s, acqrelcnt: 1, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841979.5910046
[0m23:32:59.635251 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.04424691200256348s, acqrelcnt: 1, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841979.5910046
[0m23:32:59.636250 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.045246124267578125s, acqrelcnt: 1, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841979.5910046
[0m23:32:59.636250 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.045246124267578125s, acqrelcnt: 1, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841979.5910046
[0m23:32:59.637253 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m23:32:59.637253 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m23:32:59.638250 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_name
from `hive_metastore`.`saleslt`.`dim_product`
where product_name is null



      
    ) dbt_internal_test
[0m23:32:59.638250 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:32:59.842592 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: session opened sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841979.842592
[0m23:32:59.952367 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_name
from `hive_metastore`.`saleslt`.`dim_product`
where product_name is null



      
    ) dbt_internal_test
[0m23:32:59.954290 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:32:59.955298 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:671)
	... 35 more

[0m23:32:59.956374 [debug] [Thread-1 (]: Databricks adapter: operation-id: c0564193-afb8-4d42-b98d-642d07bdaabd
[0m23:32:59.957315 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (execute): 23:32:59.615764 => 23:32:59.956374
[0m23:32:59.957555 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: _release sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841979.9575558
[0m23:32:59.961636 [debug] [Thread-1 (]: Runtime Error in test not_null_dim_product_product_name (models\marts\product\dim_product.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:32:59.962646 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: _release sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841979.9626467
[0m23:32:59.962646 [error] [Thread-1 (]: 1 of 8 ERROR not_null_dim_product_product_name ................................. [[31mERROR[0m in 0.37s]
[0m23:32:59.964567 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m23:32:59.964567 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m23:32:59.965557 [info ] [Thread-1 (]: 2 of 8 START test not_null_dim_product_product_sk .............................. [RUN]
[0m23:32:59.966556 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: idle check connection: sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.003910064697265625s, acqrelcnt: 0, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841979.9626467
[0m23:32:59.967631 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, now test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417)
[0m23:32:59.967631 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.00498509407043457s, acqrelcnt: 0, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841979.9626467
[0m23:32:59.968566 [debug] [Thread-1 (]: Databricks adapter: On thread (16492, 4336): None using default compute resource.
[0m23:32:59.968566 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: _acquire sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.005919694900512695s, acqrelcnt: 1, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841979.9626467
[0m23:32:59.968566 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m23:32:59.974564 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m23:32:59.974906 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (compile): 23:32:59.969560 => 23:32:59.974906
[0m23:32:59.975943 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m23:32:59.978910 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m23:32:59.980909 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: get_thread_connection: sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.018262863159179688s, acqrelcnt: 1, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841979.9626467
[0m23:32:59.980909 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: idle check connection: sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.018262863159179688s, acqrelcnt: 1, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841979.9626467
[0m23:32:59.982049 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m23:32:59.982049 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_sk
from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is null



      
    ) dbt_internal_test
[0m23:33:00.096649 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_sk
from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is null



      
    ) dbt_internal_test
[0m23:33:00.096649 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:33:00.097654 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:671)
	... 35 more

[0m23:33:00.097654 [debug] [Thread-1 (]: Databricks adapter: operation-id: 062b43f8-9d54-4e97-b4f1-c833f8524fe7
[0m23:33:00.098650 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (execute): 23:32:59.976907 => 23:33:00.098650
[0m23:33:00.098650 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: _release sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.0986502
[0m23:33:00.102099 [debug] [Thread-1 (]: Runtime Error in test not_null_dim_product_product_sk (models\marts\product\dim_product.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:33:00.102099 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: _release sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.1020992
[0m23:33:00.103099 [error] [Thread-1 (]: 2 of 8 ERROR not_null_dim_product_product_sk ................................... [[31mERROR[0m in 0.14s]
[0m23:33:00.104099 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m23:33:00.104099 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m23:33:00.105099 [info ] [Thread-1 (]: 3 of 8 START test not_null_dim_product_sellstartdate ........................... [RUN]
[0m23:33:00.106098 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: idle check connection: sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0039997100830078125s, acqrelcnt: 0, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.1020992
[0m23:33:00.107096 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, now test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546)
[0m23:33:00.107096 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0049974918365478516s, acqrelcnt: 0, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.1020992
[0m23:33:00.108098 [debug] [Thread-1 (]: Databricks adapter: On thread (16492, 4336): None using default compute resource.
[0m23:33:00.108309 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: _acquire sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0062105655670166016s, acqrelcnt: 1, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.1020992
[0m23:33:00.108309 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m23:33:00.114972 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m23:33:00.115977 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (compile): 23:33:00.108309 => 23:33:00.114972
[0m23:33:00.115977 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m23:33:00.119057 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m23:33:00.119978 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: get_thread_connection: sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.01787877082824707s, acqrelcnt: 1, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.1020992
[0m23:33:00.119978 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: idle check connection: sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.01787877082824707s, acqrelcnt: 1, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.1020992
[0m23:33:00.120976 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m23:33:00.120976 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select sellstartdate
from `hive_metastore`.`saleslt`.`dim_product`
where sellstartdate is null



      
    ) dbt_internal_test
[0m23:33:00.227741 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select sellstartdate
from `hive_metastore`.`saleslt`.`dim_product`
where sellstartdate is null



      
    ) dbt_internal_test
[0m23:33:00.227741 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:33:00.228741 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:671)
	... 35 more

[0m23:33:00.228741 [debug] [Thread-1 (]: Databricks adapter: operation-id: 2f57f00c-db1a-4a6d-a8bb-7ad447e2562a
[0m23:33:00.229741 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (execute): 23:33:00.116976 => 23:33:00.229741
[0m23:33:00.230745 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: _release sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.2297416
[0m23:33:00.232829 [debug] [Thread-1 (]: Runtime Error in test not_null_dim_product_sellstartdate (models\marts\product\dim_product.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:33:00.233850 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: _release sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.2338507
[0m23:33:00.234743 [error] [Thread-1 (]: 3 of 8 ERROR not_null_dim_product_sellstartdate ................................ [[31mERROR[0m in 0.13s]
[0m23:33:00.235745 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m23:33:00.235745 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m23:33:00.236792 [info ] [Thread-1 (]: 4 of 8 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m23:33:00.238818 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: idle check connection: sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0038940906524658203s, acqrelcnt: 0, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.2338507
[0m23:33:00.238818 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, now test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710)
[0m23:33:00.239746 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: reusing connection test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.005895376205444336s, acqrelcnt: 0, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.2338507
[0m23:33:00.240835 [debug] [Thread-1 (]: Databricks adapter: On thread (16492, 4336): None using default compute resource.
[0m23:33:00.241747 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: _acquire sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.006985187530517578s, acqrelcnt: 1, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.2338507
[0m23:33:00.241747 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m23:33:00.250210 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m23:33:00.251207 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 23:33:00.242778 => 23:33:00.251207
[0m23:33:00.252240 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m23:33:00.257634 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m23:33:00.259638 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: get_thread_connection: sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.024861574172973633s, acqrelcnt: 1, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.2338507
[0m23:33:00.259638 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: idle check connection: sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0257875919342041s, acqrelcnt: 1, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.2338507
[0m23:33:00.260684 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m23:33:00.261638 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m23:33:00.365138 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m23:33:00.366182 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:33:00.367086 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:671)
	... 35 more

[0m23:33:00.368115 [debug] [Thread-1 (]: Databricks adapter: operation-id: ccfb0d73-2080-479f-a367-deb08794a570
[0m23:33:00.369141 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 23:33:00.253303 => 23:33:00.369141
[0m23:33:00.370101 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: _release sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.3691416
[0m23:33:00.374178 [debug] [Thread-1 (]: Runtime Error in test not_null_my_first_dbt_model_id (models\example\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:33:00.375085 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: _release sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.3741786
[0m23:33:00.375085 [error] [Thread-1 (]: 4 of 8 ERROR not_null_my_first_dbt_model_id .................................... [[31mERROR[0m in 0.14s]
[0m23:33:00.377087 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m23:33:00.377087 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m23:33:00.378129 [info ] [Thread-1 (]: 5 of 8 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m23:33:00.379084 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: idle check connection: sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0049054622650146484s, acqrelcnt: 0, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.3741786
[0m23:33:00.380176 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, now test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778)
[0m23:33:00.380176 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: reusing connection test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.005998134613037109s, acqrelcnt: 0, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.3741786
[0m23:33:00.381088 [debug] [Thread-1 (]: Databricks adapter: On thread (16492, 4336): None using default compute resource.
[0m23:33:00.382300 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: _acquire sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.008013725280761719s, acqrelcnt: 1, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.3741786
[0m23:33:00.382300 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m23:33:00.386398 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m23:33:00.387424 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (compile): 23:33:00.382300 => 23:33:00.387424
[0m23:33:00.388303 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m23:33:00.390387 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m23:33:00.391368 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: get_thread_connection: sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.017189979553222656s, acqrelcnt: 1, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.3741786
[0m23:33:00.392302 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: idle check connection: sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.018123388290405273s, acqrelcnt: 1, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.3741786
[0m23:33:00.392302 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m23:33:00.393301 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m23:33:00.489310 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m23:33:00.489310 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:33:00.490325 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:671)
	... 35 more

[0m23:33:00.490325 [debug] [Thread-1 (]: Databricks adapter: operation-id: 3202e845-7a42-49c6-9d91-fb5b04e73212
[0m23:33:00.491215 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (execute): 23:33:00.388303 => 23:33:00.491215
[0m23:33:00.491215 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: _release sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.491216
[0m23:33:00.494299 [debug] [Thread-1 (]: Runtime Error in test not_null_my_second_dbt_model_id (models\example\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:33:00.495218 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: _release sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.495218
[0m23:33:00.495218 [error] [Thread-1 (]: 5 of 8 ERROR not_null_my_second_dbt_model_id ................................... [[31mERROR[0m in 0.12s]
[0m23:33:00.497218 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m23:33:00.497218 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m23:33:00.498216 [info ] [Thread-1 (]: 6 of 8 START test unique_dim_product_product_sk ................................ [RUN]
[0m23:33:00.499218 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: idle check connection: sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.004000425338745117s, acqrelcnt: 0, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.495218
[0m23:33:00.500219 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, now test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c)
[0m23:33:00.501229 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: reusing connection test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0050008296966552734s, acqrelcnt: 0, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.495218
[0m23:33:00.501229 [debug] [Thread-1 (]: Databricks adapter: On thread (16492, 4336): None using default compute resource.
[0m23:33:00.502229 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: _acquire sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0060117244720458984s, acqrelcnt: 1, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.495218
[0m23:33:00.502229 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m23:33:00.510228 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m23:33:00.511229 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (compile): 23:33:00.502229 => 23:33:00.510228
[0m23:33:00.512228 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m23:33:00.515736 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m23:33:00.516744 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: get_thread_connection: sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.021526575088500977s, acqrelcnt: 1, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.495218
[0m23:33:00.517747 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: idle check connection: sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.02252960205078125s, acqrelcnt: 1, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.495218
[0m23:33:00.518823 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m23:33:00.519745 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    product_sk as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is not null
group by product_sk
having count(*) > 1



      
    ) dbt_internal_test
[0m23:33:00.627452 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    product_sk as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is not null
group by product_sk
having count(*) > 1



      
    ) dbt_internal_test
[0m23:33:00.628449 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
[0m23:33:00.630368 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:671)
	... 35 more

[0m23:33:00.631448 [debug] [Thread-1 (]: Databricks adapter: operation-id: e7132eef-c7f4-4bb3-9fa6-a18bdfa91b94
[0m23:33:00.632583 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (execute): 23:33:00.512228 => 23:33:00.632583
[0m23:33:00.633668 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: _release sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.633669
[0m23:33:00.637664 [debug] [Thread-1 (]: Runtime Error in test unique_dim_product_product_sk (models\marts\product\dim_product.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
[0m23:33:00.637664 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: _release sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.6376648
[0m23:33:00.638648 [error] [Thread-1 (]: 6 of 8 ERROR unique_dim_product_product_sk ..................................... [[31mERROR[0m in 0.14s]
[0m23:33:00.639458 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m23:33:00.640500 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m23:33:00.641466 [info ] [Thread-1 (]: 7 of 8 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m23:33:00.642981 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: idle check connection: sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0043032169342041016s, acqrelcnt: 0, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.6376648
[0m23:33:00.642981 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, now test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321)
[0m23:33:00.642981 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: reusing connection test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.005316257476806641s, acqrelcnt: 0, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.6376648
[0m23:33:00.643978 [debug] [Thread-1 (]: Databricks adapter: On thread (16492, 4336): None using default compute resource.
[0m23:33:00.643978 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: _acquire sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.006313800811767578s, acqrelcnt: 1, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.6376648
[0m23:33:00.644978 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m23:33:00.648978 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m23:33:00.650130 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (compile): 23:33:00.644978 => 23:33:00.648978
[0m23:33:00.650130 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m23:33:00.653225 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m23:33:00.654215 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: get_thread_connection: sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.016550302505493164s, acqrelcnt: 1, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.6376648
[0m23:33:00.655138 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: idle check connection: sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.017473220825195312s, acqrelcnt: 1, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.6376648
[0m23:33:00.656132 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m23:33:00.656132 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m23:33:00.761020 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m23:33:00.761020 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
[0m23:33:00.762019 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:671)
	... 35 more

[0m23:33:00.762930 [debug] [Thread-1 (]: Databricks adapter: operation-id: 26ee05fa-edc9-4a42-92a6-1f5668ce4839
[0m23:33:00.762930 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (execute): 23:33:00.650130 => 23:33:00.762930
[0m23:33:00.763930 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: _release sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.7629306
[0m23:33:00.767019 [debug] [Thread-1 (]: Runtime Error in test unique_my_first_dbt_model_id (models\example\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
[0m23:33:00.767933 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: _release sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.7670197
[0m23:33:00.767933 [error] [Thread-1 (]: 7 of 8 ERROR unique_my_first_dbt_model_id ...................................... [[31mERROR[0m in 0.13s]
[0m23:33:00.768931 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m23:33:00.769995 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m23:33:00.769995 [info ] [Thread-1 (]: 8 of 8 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m23:33:00.771936 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: idle check connection: sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.004916667938232422s, acqrelcnt: 0, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.7670197
[0m23:33:00.771936 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, now test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493)
[0m23:33:00.772936 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: reusing connection test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.005917072296142578s, acqrelcnt: 0, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.7670197
[0m23:33:00.772936 [debug] [Thread-1 (]: Databricks adapter: On thread (16492, 4336): None using default compute resource.
[0m23:33:00.773935 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: _acquire sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.00691533088684082s, acqrelcnt: 1, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.7670197
[0m23:33:00.773935 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m23:33:00.781935 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m23:33:00.782934 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (compile): 23:33:00.774935 => 23:33:00.781935
[0m23:33:00.782934 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m23:33:00.786935 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m23:33:00.788940 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: get_thread_connection: sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.02091813087463379s, acqrelcnt: 1, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.7670197
[0m23:33:00.789937 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: idle check connection: sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.021921157836914062s, acqrelcnt: 1, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.7670197
[0m23:33:00.789937 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m23:33:00.790943 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m23:33:00.894374 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m23:33:00.894374 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
[0m23:33:00.895374 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:671)
	... 35 more

[0m23:33:00.895374 [debug] [Thread-1 (]: Databricks adapter: operation-id: ff1ed4cb-9b07-457b-bdea-7968c53e8645
[0m23:33:00.896373 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (execute): 23:33:00.783935 => 23:33:00.896373
[0m23:33:00.896373 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: _release sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.8963737
[0m23:33:00.899373 [debug] [Thread-1 (]: Runtime Error in test unique_my_second_dbt_model_id (models\example\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
[0m23:33:00.899373 [debug] [Thread-1 (]: Databricks adapter: conn: 2264303343504: _release sess: 63f4cf6b-11d6-4c0f-bd1a-74d045fc1201, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16492, 4336), cmpt: ``, lut: 1707841980.899373
[0m23:33:00.900616 [error] [Thread-1 (]: 8 of 8 ERROR unique_my_second_dbt_model_id ..................................... [[31mERROR[0m in 0.13s]
[0m23:33:00.901620 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m23:33:00.903617 [debug] [MainThread]: Databricks adapter: conn: 2264302907728: idle check connection: sess: None, name: master, idle: 1.3227951526641846s, acqrelcnt: 0, lang: None, thrd: (16492, 25872), cmpt: ``, lut: 1707841979.5808225
[0m23:33:00.904615 [debug] [MainThread]: Databricks adapter: conn: 2264302907728: reusing connection master sess: None, name: master, idle: 1.3237931728363037s, acqrelcnt: 0, lang: None, thrd: (16492, 25872), cmpt: ``, lut: 1707841979.5808225
[0m23:33:00.904615 [debug] [MainThread]: Databricks adapter: Thread (16492, 25872) using default compute resource.
[0m23:33:00.904615 [debug] [MainThread]: Databricks adapter: conn: 2264302907728: _acquire sess: None, name: master, idle: 1.3237931728363037s, acqrelcnt: 1, lang: None, thrd: (16492, 25872), cmpt: ``, lut: 1707841979.5808225
[0m23:33:00.905617 [debug] [MainThread]: Databricks adapter: conn: 2264302907728: get_thread_connection: sess: None, name: master, idle: 1.3247950077056885s, acqrelcnt: 1, lang: None, thrd: (16492, 25872), cmpt: ``, lut: 1707841979.5808225
[0m23:33:00.905617 [debug] [MainThread]: Databricks adapter: conn: 2264302907728: idle check connection: sess: None, name: master, idle: 1.3247950077056885s, acqrelcnt: 1, lang: None, thrd: (16492, 25872), cmpt: ``, lut: 1707841979.5808225
[0m23:33:00.906695 [debug] [MainThread]: On master: ROLLBACK
[0m23:33:00.906831 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:33:01.163152 [debug] [MainThread]: Databricks adapter: conn: 2264302907728: session opened sess: b5d1501a-ac73-4aa2-99d5-ff29e946fc37, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (16492, 25872), cmpt: ``, lut: 1707841981.1631527
[0m23:33:01.163152 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:33:01.164152 [debug] [MainThread]: Databricks adapter: conn: 2264302907728: get_thread_connection: sess: b5d1501a-ac73-4aa2-99d5-ff29e946fc37, name: master, idle: 0.00099945068359375s, acqrelcnt: 1, lang: None, thrd: (16492, 25872), cmpt: ``, lut: 1707841981.1631527
[0m23:33:01.164152 [debug] [MainThread]: Databricks adapter: conn: 2264302907728: idle check connection: sess: b5d1501a-ac73-4aa2-99d5-ff29e946fc37, name: master, idle: 0.00099945068359375s, acqrelcnt: 1, lang: None, thrd: (16492, 25872), cmpt: ``, lut: 1707841981.1631527
[0m23:33:01.165152 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:33:01.165152 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:33:01.165152 [debug] [MainThread]: Databricks adapter: conn: 2264302907728: _release sess: b5d1501a-ac73-4aa2-99d5-ff29e946fc37, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (16492, 25872), cmpt: ``, lut: 1707841981.1651525
[0m23:33:01.166152 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:33:01.167156 [debug] [MainThread]: On master: ROLLBACK
[0m23:33:01.167156 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:33:01.167156 [debug] [MainThread]: On master: Close
[0m23:33:01.257234 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m23:33:01.257407 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m23:33:01.257407 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:33:01.258424 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m23:33:01.344899 [debug] [MainThread]: Connection 'test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493' was properly closed.
[0m23:33:01.345907 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: ROLLBACK
[0m23:33:01.345907 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:33:01.346909 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m23:33:01.431175 [info ] [MainThread]: 
[0m23:33:01.432525 [info ] [MainThread]: Finished running 8 tests in 0 hours 0 minutes and 3.58 seconds (3.58s).
[0m23:33:01.435685 [debug] [MainThread]: Command end result
[0m23:33:01.456962 [info ] [MainThread]: 
[0m23:33:01.457962 [info ] [MainThread]: [31mCompleted with 8 errors and 0 warnings:[0m
[0m23:33:01.458965 [info ] [MainThread]: 
[0m23:33:01.459962 [error] [MainThread]:   Runtime Error in test not_null_dim_product_product_name (models\marts\product\dim_product.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:33:01.461341 [info ] [MainThread]: 
[0m23:33:01.461341 [error] [MainThread]:   Runtime Error in test not_null_dim_product_product_sk (models\marts\product\dim_product.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:33:01.461341 [info ] [MainThread]: 
[0m23:33:01.461341 [error] [MainThread]:   Runtime Error in test not_null_dim_product_sellstartdate (models\marts\product\dim_product.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:33:01.469387 [info ] [MainThread]: 
[0m23:33:01.470804 [error] [MainThread]:   Runtime Error in test not_null_my_first_dbt_model_id (models\example\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:33:01.472811 [info ] [MainThread]: 
[0m23:33:01.473812 [error] [MainThread]:   Runtime Error in test not_null_my_second_dbt_model_id (models\example\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 14 pos 5
[0m23:33:01.474813 [info ] [MainThread]: 
[0m23:33:01.476280 [error] [MainThread]:   Runtime Error in test unique_dim_product_product_sk (models\marts\product\dim_product.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`dim_product` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
[0m23:33:01.479072 [info ] [MainThread]: 
[0m23:33:01.480072 [error] [MainThread]:   Runtime Error in test unique_my_first_dbt_model_id (models\example\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
[0m23:33:01.482478 [info ] [MainThread]: 
[0m23:33:01.482901 [error] [MainThread]:   Runtime Error in test unique_my_second_dbt_model_id (models\example\schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`saleslt`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 15 pos 5
[0m23:33:01.484916 [info ] [MainThread]: 
[0m23:33:01.485924 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=8 SKIP=0 TOTAL=8
[0m23:33:01.487927 [debug] [MainThread]: Command `dbt test` failed at 23:33:01.486923 after 6.61 seconds
[0m23:33:01.487927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F1FFB53D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F200ACAD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F1931AF10>]}
[0m23:33:01.488922 [debug] [MainThread]: Flushing usage events
[0m23:33:34.569002 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BEB2232C50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BEB1E9C690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BEB1E9C050>]}


============================== 23:33:34.573093 | 5d77af87-7953-4c99-944f-e70f40b3d7c0 ==============================
[0m23:33:34.573093 [info ] [MainThread]: Running with dbt=1.7.7
[0m23:33:34.574006 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True'}
[0m23:33:35.813473 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5d77af87-7953-4c99-944f-e70f40b3d7c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BEC4693F50>]}
[0m23:33:35.919780 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5d77af87-7953-4c99-944f-e70f40b3d7c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BEC4533590>]}
[0m23:33:35.920692 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m23:33:35.930189 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m23:33:35.996538 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m23:33:35.997623 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m23:33:36.004641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5d77af87-7953-4c99-944f-e70f40b3d7c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BEC457B3D0>]}
[0m23:33:36.020924 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5d77af87-7953-4c99-944f-e70f40b3d7c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BEC47C7D50>]}
[0m23:33:36.021922 [info ] [MainThread]: Found 5 models, 7 snapshots, 8 tests, 9 sources, 0 exposures, 0 metrics, 535 macros, 0 groups, 0 semantic models
[0m23:33:36.022924 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5d77af87-7953-4c99-944f-e70f40b3d7c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BEC4756CD0>]}
[0m23:33:36.024924 [info ] [MainThread]: 
[0m23:33:36.025923 [debug] [MainThread]: Databricks adapter: conn: 3018362398544: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (25572, 2656), cmpt: ``, lut: None
[0m23:33:36.027241 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:33:36.027241 [debug] [MainThread]: Databricks adapter: Thread (25572, 2656) using default compute resource.
[0m23:33:36.028241 [debug] [MainThread]: Databricks adapter: conn: 3018362398544: _acquire sess: None, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (25572, 2656), cmpt: ``, lut: 1707842016.028241
[0m23:33:36.030242 [debug] [ThreadPool]: Databricks adapter: conn: 3018363531280: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (25572, 23560), cmpt: ``, lut: None
[0m23:33:36.030242 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m23:33:36.031240 [debug] [ThreadPool]: Databricks adapter: Thread (25572, 23560) using default compute resource.
[0m23:33:36.031240 [debug] [ThreadPool]: Databricks adapter: conn: 3018363531280: _acquire sess: None, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (25572, 23560), cmpt: ``, lut: 1707842016.0312405
[0m23:33:36.032241 [debug] [ThreadPool]: Databricks adapter: conn: 3018363531280: get_thread_connection: sess: None, name: list_hive_metastore, idle: 0.0010013580322265625s, acqrelcnt: 1, lang: None, thrd: (25572, 23560), cmpt: ``, lut: 1707842016.0312405
[0m23:33:36.033240 [debug] [ThreadPool]: Databricks adapter: conn: 3018363531280: idle check connection: sess: None, name: list_hive_metastore, idle: 0.0010013580322265625s, acqrelcnt: 1, lang: None, thrd: (25572, 23560), cmpt: ``, lut: 1707842016.0312405
[0m23:33:36.033240 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m23:33:36.033240 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m23:33:36.034801 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:33:36.270951 [debug] [ThreadPool]: Databricks adapter: conn: 3018363531280: session opened sess: a5698af8-e4d4-40d6-8cc2-1ff540f431b3, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (25572, 23560), cmpt: ``, lut: 1707842016.2709517
[0m23:33:36.387600 [debug] [ThreadPool]: SQL status: OK in 0.3499999940395355 seconds
[0m23:33:36.391600 [debug] [ThreadPool]: Databricks adapter: conn: 3018363531280: _release sess: a5698af8-e4d4-40d6-8cc2-1ff540f431b3, name: list_hive_metastore, idle: 0.0010004043579101562s, acqrelcnt: 0, lang: None, thrd: (25572, 23560), cmpt: ``, lut: 1707842016.3906002
[0m23:33:36.393602 [debug] [ThreadPool]: Databricks adapter: conn: 3018363533456: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_saleslt, idle: 0s, acqrelcnt: 0, lang: None, thrd: (25572, 25064), cmpt: ``, lut: None
[0m23:33:36.394603 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m23:33:36.394603 [debug] [ThreadPool]: Databricks adapter: Thread (25572, 25064) using default compute resource.
[0m23:33:36.394603 [debug] [ThreadPool]: Databricks adapter: conn: 3018363533456: _acquire sess: None, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (25572, 25064), cmpt: ``, lut: 1707842016.3946037
[0m23:33:36.400320 [debug] [ThreadPool]: Databricks adapter: conn: 3018363533456: get_thread_connection: sess: None, name: list_hive_metastore_saleslt, idle: 0.005716800689697266s, acqrelcnt: 1, lang: None, thrd: (25572, 25064), cmpt: ``, lut: 1707842016.3946037
[0m23:33:36.401314 [debug] [ThreadPool]: Databricks adapter: conn: 3018363533456: idle check connection: sess: None, name: list_hive_metastore_saleslt, idle: 0.00671076774597168s, acqrelcnt: 1, lang: None, thrd: (25572, 25064), cmpt: ``, lut: 1707842016.3946037
[0m23:33:36.401314 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:33:36.402654 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m23:33:36.402654 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:33:36.617449 [debug] [ThreadPool]: Databricks adapter: conn: 3018363533456: session opened sess: 9cdce3be-a046-47bc-a0c4-f7303f93c132, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (25572, 25064), cmpt: ``, lut: 1707842016.6174498
[0m23:33:36.775200 [debug] [ThreadPool]: SQL status: OK in 0.3700000047683716 seconds
[0m23:33:36.792137 [debug] [ThreadPool]: Databricks adapter: conn: 3018363533456: get_thread_connection: sess: 9cdce3be-a046-47bc-a0c4-f7303f93c132, name: list_hive_metastore_saleslt, idle: 0.17468738555908203s, acqrelcnt: 1, lang: None, thrd: (25572, 25064), cmpt: ``, lut: 1707842016.6174498
[0m23:33:36.793072 [debug] [ThreadPool]: Databricks adapter: conn: 3018363533456: idle check connection: sess: 9cdce3be-a046-47bc-a0c4-f7303f93c132, name: list_hive_metastore_saleslt, idle: 0.17468738555908203s, acqrelcnt: 1, lang: None, thrd: (25572, 25064), cmpt: ``, lut: 1707842016.6174498
[0m23:33:36.793072 [debug] [ThreadPool]: Databricks adapter: conn: 3018363533456: get_thread_connection: sess: 9cdce3be-a046-47bc-a0c4-f7303f93c132, name: list_hive_metastore_saleslt, idle: 0.17562317848205566s, acqrelcnt: 1, lang: None, thrd: (25572, 25064), cmpt: ``, lut: 1707842016.6174498
[0m23:33:36.794113 [debug] [ThreadPool]: Databricks adapter: conn: 3018363533456: idle check connection: sess: 9cdce3be-a046-47bc-a0c4-f7303f93c132, name: list_hive_metastore_saleslt, idle: 0.17562317848205566s, acqrelcnt: 1, lang: None, thrd: (25572, 25064), cmpt: ``, lut: 1707842016.6174498
[0m23:33:36.794113 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:33:36.794113 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:33:36.795066 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m23:33:36.946386 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m23:33:36.956624 [debug] [ThreadPool]: Databricks adapter: conn: 3018363533456: get_thread_connection: sess: 9cdce3be-a046-47bc-a0c4-f7303f93c132, name: list_hive_metastore_saleslt, idle: 0.3391752243041992s, acqrelcnt: 1, lang: None, thrd: (25572, 25064), cmpt: ``, lut: 1707842016.6174498
[0m23:33:36.957531 [debug] [ThreadPool]: Databricks adapter: conn: 3018363533456: idle check connection: sess: 9cdce3be-a046-47bc-a0c4-f7303f93c132, name: list_hive_metastore_saleslt, idle: 0.3399629592895508s, acqrelcnt: 1, lang: None, thrd: (25572, 25064), cmpt: ``, lut: 1707842016.6174498
[0m23:33:36.957760 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:33:36.957760 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m23:33:37.103658 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m23:33:37.106298 [debug] [ThreadPool]: Databricks adapter: conn: 3018363533456: _release sess: 9cdce3be-a046-47bc-a0c4-f7303f93c132, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (25572, 25064), cmpt: ``, lut: 1707842017.1062984
[0m23:33:37.107424 [debug] [ThreadPool]: Databricks adapter: conn: 3018363533456: idle check connection: sess: 9cdce3be-a046-47bc-a0c4-f7303f93c132, name: list_hive_metastore_saleslt, idle: 0.0011260509490966797s, acqrelcnt: 0, lang: None, thrd: (25572, 25064), cmpt: ``, lut: 1707842017.1062984
[0m23:33:37.110427 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m23:33:37.110427 [debug] [ThreadPool]: Databricks adapter: conn: 3018363533456: reusing connection list_hive_metastore_saleslt sess: 9cdce3be-a046-47bc-a0c4-f7303f93c132, name: list_hive_metastore_snapshots, idle: 0.004128932952880859s, acqrelcnt: 0, lang: None, thrd: (25572, 25064), cmpt: ``, lut: 1707842017.1062984
[0m23:33:37.111427 [debug] [ThreadPool]: Databricks adapter: Thread (25572, 25064) using default compute resource.
[0m23:33:37.111427 [debug] [ThreadPool]: Databricks adapter: conn: 3018363533456: _acquire sess: 9cdce3be-a046-47bc-a0c4-f7303f93c132, name: list_hive_metastore_snapshots, idle: 0.0051288604736328125s, acqrelcnt: 1, lang: None, thrd: (25572, 25064), cmpt: ``, lut: 1707842017.1062984
[0m23:33:37.113428 [debug] [ThreadPool]: Databricks adapter: conn: 3018363533456: get_thread_connection: sess: 9cdce3be-a046-47bc-a0c4-f7303f93c132, name: list_hive_metastore_snapshots, idle: 0.007130622863769531s, acqrelcnt: 1, lang: None, thrd: (25572, 25064), cmpt: ``, lut: 1707842017.1062984
[0m23:33:37.113428 [debug] [ThreadPool]: Databricks adapter: conn: 3018363533456: idle check connection: sess: 9cdce3be-a046-47bc-a0c4-f7303f93c132, name: list_hive_metastore_snapshots, idle: 0.007130622863769531s, acqrelcnt: 1, lang: None, thrd: (25572, 25064), cmpt: ``, lut: 1707842017.1062984
[0m23:33:37.114427 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m23:33:37.114427 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m23:33:37.214673 [debug] [ThreadPool]: SQL status: OK in 0.10000000149011612 seconds
[0m23:33:37.221288 [debug] [ThreadPool]: Databricks adapter: conn: 3018363533456: get_thread_connection: sess: 9cdce3be-a046-47bc-a0c4-f7303f93c132, name: list_hive_metastore_snapshots, idle: 0.11398696899414062s, acqrelcnt: 1, lang: None, thrd: (25572, 25064), cmpt: ``, lut: 1707842017.1062984
[0m23:33:37.221288 [debug] [ThreadPool]: Databricks adapter: conn: 3018363533456: idle check connection: sess: 9cdce3be-a046-47bc-a0c4-f7303f93c132, name: list_hive_metastore_snapshots, idle: 0.1149897575378418s, acqrelcnt: 1, lang: None, thrd: (25572, 25064), cmpt: ``, lut: 1707842017.1062984
[0m23:33:37.222240 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m23:33:37.222240 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m23:33:37.353499 [debug] [ThreadPool]: SQL status: OK in 0.12999999523162842 seconds
[0m23:33:37.358241 [debug] [ThreadPool]: Databricks adapter: conn: 3018363533456: get_thread_connection: sess: 9cdce3be-a046-47bc-a0c4-f7303f93c132, name: list_hive_metastore_snapshots, idle: 0.25194334983825684s, acqrelcnt: 1, lang: None, thrd: (25572, 25064), cmpt: ``, lut: 1707842017.1062984
[0m23:33:37.358241 [debug] [ThreadPool]: Databricks adapter: conn: 3018363533456: idle check connection: sess: 9cdce3be-a046-47bc-a0c4-f7303f93c132, name: list_hive_metastore_snapshots, idle: 0.25194334983825684s, acqrelcnt: 1, lang: None, thrd: (25572, 25064), cmpt: ``, lut: 1707842017.1062984
[0m23:33:37.359240 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m23:33:37.359240 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m23:33:37.499423 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m23:33:37.501645 [debug] [ThreadPool]: Databricks adapter: conn: 3018363533456: _release sess: 9cdce3be-a046-47bc-a0c4-f7303f93c132, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (25572, 25064), cmpt: ``, lut: 1707842017.5016456
[0m23:33:37.503560 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5d77af87-7953-4c99-944f-e70f40b3d7c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BEC47EFDD0>]}
[0m23:33:37.504574 [debug] [MainThread]: Databricks adapter: conn: 3018362398544: get_thread_connection: sess: None, name: master, idle: 1.4763340950012207s, acqrelcnt: 1, lang: None, thrd: (25572, 2656), cmpt: ``, lut: 1707842016.028241
[0m23:33:37.504574 [debug] [MainThread]: Databricks adapter: conn: 3018362398544: idle check connection: sess: None, name: master, idle: 1.4763340950012207s, acqrelcnt: 1, lang: None, thrd: (25572, 2656), cmpt: ``, lut: 1707842016.028241
[0m23:33:37.506079 [debug] [MainThread]: Databricks adapter: conn: 3018362398544: get_thread_connection: sess: None, name: master, idle: 1.4778385162353516s, acqrelcnt: 1, lang: None, thrd: (25572, 2656), cmpt: ``, lut: 1707842016.028241
[0m23:33:37.506079 [debug] [MainThread]: Databricks adapter: conn: 3018362398544: idle check connection: sess: None, name: master, idle: 1.4778385162353516s, acqrelcnt: 1, lang: None, thrd: (25572, 2656), cmpt: ``, lut: 1707842016.028241
[0m23:33:37.507301 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:33:37.507301 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:33:37.508303 [debug] [MainThread]: Databricks adapter: conn: 3018362398544: _release sess: None, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (25572, 2656), cmpt: ``, lut: 1707842017.5073016
[0m23:33:37.508303 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:33:37.509388 [info ] [MainThread]: 
[0m23:33:37.514303 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.dim_customer
[0m23:33:37.515301 [info ] [Thread-1 (]: 1 of 5 START sql table model saleslt.dim_customer .............................. [RUN]
[0m23:33:37.516417 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: Creating DatabricksDBTConnection sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0s, acqrelcnt: 0, lang: None, thrd: (25572, 14120), cmpt: ``, lut: None
[0m23:33:37.517817 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.medallion_dbt_spark.dim_customer'
[0m23:33:37.517817 [debug] [Thread-1 (]: Databricks adapter: On thread (25572, 14120): `hive_metastore`.`saleslt`.`dim_customer` using default compute resource.
[0m23:33:37.518865 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: _acquire sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842017.5188658
[0m23:33:37.518865 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.dim_customer
[0m23:33:37.529504 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.dim_customer"
[0m23:33:37.533298 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_customer (compile): 23:33:37.519847 => 23:33:37.533298
[0m23:33:37.534305 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.dim_customer
[0m23:33:37.606005 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_dbt_spark.dim_customer"
[0m23:33:37.607759 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: get_thread_connection: sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0.08889389038085938s, acqrelcnt: 1, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842017.5188658
[0m23:33:37.608759 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: idle check connection: sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0.08989405632019043s, acqrelcnt: 1, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842017.5188658
[0m23:33:37.608759 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: get_thread_connection: sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0.08989405632019043s, acqrelcnt: 1, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842017.5188658
[0m23:33:37.609762 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: idle check connection: sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0.0908970832824707s, acqrelcnt: 1, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842017.5188658
[0m23:33:37.609762 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m23:33:37.610762 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.dim_customer"
[0m23:33:37.610762 [debug] [Thread-1 (]: On model.medallion_dbt_spark.dim_customer: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_customer`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/dim_customer'
      
      
      as
      

with address_snapshot as (
    select 
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
),
customeraddress_snapshot as (
    select 
        CustomerID,
        AddressID,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
),
customer_snapshot as (
    select 
        CustomerId,
        concat(ifnull(FirstName, ''), ' ', ifnull(MiddleName, ''), ' ', ifnull(LastName, '')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
),
transformed as (
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incrementing surrogate key
    customer_snapshot.CustomerID,
    customer_snapshot.FullName,
    customer_snapshot.AddressID,
    customer_snapshot.AddressType,
    customer_snapshot.AddressLine1,
    customer_snapshot.City,
    customer_snapshot.StateProvince,
    customer_snapshot.CountryRegion,
    customer_snapshot.PostalCode,
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerID = customeraddress_snapshot.CustomerID
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)
select * from transformed
  
[0m23:33:37.611762 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:33:37.948971 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: session opened sess: a664bcb8-fd4e-4f72-ac45-c9f06a04ad2f, name: model.medallion_dbt_spark.dim_customer, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842017.9489715
[0m23:33:38.119897 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_customer`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/dim_customer'
      
      
      as
      

with address_snapshot as (
    select 
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
),
customeraddress_snapshot as (
    select 
        CustomerID,
        AddressID,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
),
customer_snapshot as (
    select 
        CustomerId,
        concat(ifnull(FirstName, ''), ' ', ifnull(MiddleName, ''), ' ', ifnull(LastName, '')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
),
transformed as (
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incrementing surrogate key
    customer_snapshot.CustomerID,
    customer_snapshot.FullName,
    customer_snapshot.AddressID,
    customer_snapshot.AddressType,
    customer_snapshot.AddressLine1,
    customer_snapshot.City,
    customer_snapshot.StateProvince,
    customer_snapshot.CountryRegion,
    customer_snapshot.PostalCode,
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerID = customeraddress_snapshot.CustomerID
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)
select * from transformed
  
[0m23:33:38.120908 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'inner'.(line 57, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_customer`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/dim_customer'
      
      
      as
      

with address_snapshot as (
    select 
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
),
customeraddress_snapshot as (
    select 
        CustomerID,
        AddressID,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
),
customer_snapshot as (
    select 
        CustomerId,
        concat(ifnull(FirstName, ''), ' ', ifnull(MiddleName, ''), ' ', ifnull(LastName, '')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
),
transformed as (
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incrementing surrogate key
    customer_snapshot.CustomerID,
    customer_snapshot.FullName,
    customer_snapshot.AddressID,
    customer_snapshot.AddressType,
    customer_snapshot.AddressLine1,
    customer_snapshot.City,
    customer_snapshot.StateProvince,
    customer_snapshot.CountryRegion,
    customer_snapshot.PostalCode,
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerID = customeraddress_snapshot.CustomerID
----^^^
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)
select * from transformed
  

[0m23:33:38.121905 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'inner'.(line 57, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_customer`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/dim_customer'
      
      
      as
      

with address_snapshot as (
    select 
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
),
customeraddress_snapshot as (
    select 
        CustomerID,
        AddressID,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
),
customer_snapshot as (
    select 
        CustomerId,
        concat(ifnull(FirstName, ''), ' ', ifnull(MiddleName, ''), ' ', ifnull(LastName, '')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
),
transformed as (
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incrementing surrogate key
    customer_snapshot.CustomerID,
    customer_snapshot.FullName,
    customer_snapshot.AddressID,
    customer_snapshot.AddressType,
    customer_snapshot.AddressLine1,
    customer_snapshot.City,
    customer_snapshot.StateProvince,
    customer_snapshot.CountryRegion,
    customer_snapshot.PostalCode,
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerID = customeraddress_snapshot.CustomerID
----^^^
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)
select * from transformed
  

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'inner'.(line 57, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_customer`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/dim_customer'
      
      
      as
      

with address_snapshot as (
    select 
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
),
customeraddress_snapshot as (
    select 
        CustomerID,
        AddressID,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
),
customer_snapshot as (
    select 
        CustomerId,
        concat(ifnull(FirstName, ''), ' ', ifnull(MiddleName, ''), ' ', ifnull(LastName, '')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
),
transformed as (
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incrementing surrogate key
    customer_snapshot.CustomerID,
    customer_snapshot.FullName,
    customer_snapshot.AddressID,
    customer_snapshot.AddressType,
    customer_snapshot.AddressLine1,
    customer_snapshot.City,
    customer_snapshot.StateProvince,
    customer_snapshot.CountryRegion,
    customer_snapshot.PostalCode,
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerID = customeraddress_snapshot.CustomerID
----^^^
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)
select * from transformed
  

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:267)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:101)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:111)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:87)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:541)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:538)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1127)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:526)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:607)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:624)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:607)
	... 35 more

[0m23:33:38.122907 [debug] [Thread-1 (]: Databricks adapter: operation-id: a024a246-7a66-47a7-98b2-b5342cd66902
[0m23:33:38.123905 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_customer (execute): 23:33:37.535306 => 23:33:38.123905
[0m23:33:38.123905 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: _release sess: a664bcb8-fd4e-4f72-ac45-c9f06a04ad2f, name: model.medallion_dbt_spark.dim_customer, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842018.123905
[0m23:33:38.148151 [debug] [Thread-1 (]: Runtime Error in model dim_customer (models\marts\customer\dim_customer.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'inner'.(line 57, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_customer"} */
  
    
      
          create or replace table `hive_metastore`.`saleslt`.`dim_customer`
        
        
      using delta
        
        
        
        
        
      location '/mnt/silver/customer/dim_customer'
        
        
        as
        
  
  with address_snapshot as (
      select 
          AddressID,
          AddressLine1,
          AddressLine2,
          City,
          StateProvince,
          CountryRegion,
          PostalCode
      from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
  ),
  customeraddress_snapshot as (
      select 
          CustomerID,
          AddressID,
          AddressType
      from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
  ),
  customer_snapshot as (
      select 
          CustomerId,
          concat(ifnull(FirstName, ''), ' ', ifnull(MiddleName, ''), ' ', ifnull(LastName, '')) as FullName
      from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
  ),
  transformed as (
      select
      row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incrementing surrogate key
      customer_snapshot.CustomerID,
      customer_snapshot.FullName,
      customer_snapshot.AddressID,
      customer_snapshot.AddressType,
      customer_snapshot.AddressLine1,
      customer_snapshot.City,
      customer_snapshot.StateProvince,
      customer_snapshot.CountryRegion,
      customer_snapshot.PostalCode,
      from customer_snapshot
      inner join customeraddress_snapshot on customer_snapshot.CustomerID = customeraddress_snapshot.CustomerID
  ----^^^
      inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
  )
  select * from transformed
    
  
[0m23:33:38.149447 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: _release sess: a664bcb8-fd4e-4f72-ac45-c9f06a04ad2f, name: model.medallion_dbt_spark.dim_customer, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842018.149448
[0m23:33:38.149447 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d77af87-7953-4c99-944f-e70f40b3d7c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BEC4A12CD0>]}
[0m23:33:38.150450 [error] [Thread-1 (]: 1 of 5 ERROR creating sql table model saleslt.dim_customer ..................... [[31mERROR[0m in 0.63s]
[0m23:33:38.151449 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.dim_customer
[0m23:33:38.152527 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.dim_product
[0m23:33:38.153449 [info ] [Thread-1 (]: 2 of 5 START sql table model saleslt.dim_product ............................... [RUN]
[0m23:33:38.154038 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: idle check connection: sess: a664bcb8-fd4e-4f72-ac45-c9f06a04ad2f, name: model.medallion_dbt_spark.dim_customer, idle: 0.004590272903442383s, acqrelcnt: 0, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842018.149448
[0m23:33:38.155044 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.dim_customer, now model.medallion_dbt_spark.dim_product)
[0m23:33:38.155044 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: reusing connection model.medallion_dbt_spark.dim_customer sess: a664bcb8-fd4e-4f72-ac45-c9f06a04ad2f, name: model.medallion_dbt_spark.dim_product, idle: 0.005596637725830078s, acqrelcnt: 0, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842018.149448
[0m23:33:38.156044 [debug] [Thread-1 (]: Databricks adapter: On thread (25572, 14120): `hive_metastore`.`saleslt`.`dim_product` using default compute resource.
[0m23:33:38.156044 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: _acquire sess: a664bcb8-fd4e-4f72-ac45-c9f06a04ad2f, name: model.medallion_dbt_spark.dim_product, idle: 0.006596088409423828s, acqrelcnt: 1, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842018.149448
[0m23:33:38.157125 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.dim_product
[0m23:33:38.160355 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.dim_product"
[0m23:33:38.161324 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_product (compile): 23:33:38.157249 => 23:33:38.161324
[0m23:33:38.162332 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.dim_product
[0m23:33:38.167430 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_dbt_spark.dim_product"
[0m23:33:38.168332 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: get_thread_connection: sess: a664bcb8-fd4e-4f72-ac45-c9f06a04ad2f, name: model.medallion_dbt_spark.dim_product, idle: 0.018884658813476562s, acqrelcnt: 1, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842018.149448
[0m23:33:38.169333 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: idle check connection: sess: a664bcb8-fd4e-4f72-ac45-c9f06a04ad2f, name: model.medallion_dbt_spark.dim_product, idle: 0.019885540008544922s, acqrelcnt: 1, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842018.149448
[0m23:33:38.170435 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.dim_product"
[0m23:33:38.171366 [debug] [Thread-1 (]: On model.medallion_dbt_spark.dim_product: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/dim_product'
      
      
      as
      
with product_snapshot as (
    select 
        ProductId,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate
    from `hive_metastore`.`snapshots`.`product_snapshot` where dbt_valid_to is null
)
productmodel_snapshot as (
    select 
        ProductModelID,
        Name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot` where dbt_valid_to is null
),
transformed as (
    select
    row_number() over (order by product_snapshot.ProductId) as product_sk, -- auto-incrementing surrogate key
    product_snapshot.Name as product_name,
    product_snapshot.StandardCost,
    product_snapshot.ListPrice,
    product_snapshot.Size,
    product_snapshot.Weight,
    productmodel_snapshot.Name as model,
    productmodel_snapshot.CatalogDescription as description,
    product_snapshot.SellStartDate,
    product_snapshot.SellEndDate,
    product_snapshot.DiscontinuedDate
    from product_snapshot
    left join productmodel_snapshot on product_snapshot.ProductModelID = productmodel_snapshot.ProductModelID
)
select * from transformed
  
[0m23:33:38.276192 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/dim_product'
      
      
      as
      
with product_snapshot as (
    select 
        ProductId,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate
    from `hive_metastore`.`snapshots`.`product_snapshot` where dbt_valid_to is null
)
productmodel_snapshot as (
    select 
        ProductModelID,
        Name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot` where dbt_valid_to is null
),
transformed as (
    select
    row_number() over (order by product_snapshot.ProductId) as product_sk, -- auto-incrementing surrogate key
    product_snapshot.Name as product_name,
    product_snapshot.StandardCost,
    product_snapshot.ListPrice,
    product_snapshot.Size,
    product_snapshot.Weight,
    productmodel_snapshot.Name as model,
    productmodel_snapshot.CatalogDescription as description,
    product_snapshot.SellStartDate,
    product_snapshot.SellEndDate,
    product_snapshot.DiscontinuedDate
    from product_snapshot
    left join productmodel_snapshot on product_snapshot.ProductModelID = productmodel_snapshot.ProductModelID
)
select * from transformed
  
[0m23:33:38.277194 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'productmodel_snapshot'.(line 36, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/dim_product'
      
      
      as
      
with product_snapshot as (
    select 
        ProductId,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate
    from `hive_metastore`.`snapshots`.`product_snapshot` where dbt_valid_to is null
)
productmodel_snapshot as (
^^^
    select 
        ProductModelID,
        Name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot` where dbt_valid_to is null
),
transformed as (
    select
    row_number() over (order by product_snapshot.ProductId) as product_sk, -- auto-incrementing surrogate key
    product_snapshot.Name as product_name,
    product_snapshot.StandardCost,
    product_snapshot.ListPrice,
    product_snapshot.Size,
    product_snapshot.Weight,
    productmodel_snapshot.Name as model,
    productmodel_snapshot.CatalogDescription as description,
    product_snapshot.SellStartDate,
    product_snapshot.SellEndDate,
    product_snapshot.DiscontinuedDate
    from product_snapshot
    left join productmodel_snapshot on product_snapshot.ProductModelID = productmodel_snapshot.ProductModelID
)
select * from transformed
  

[0m23:33:38.279289 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'productmodel_snapshot'.(line 36, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/dim_product'
      
      
      as
      
with product_snapshot as (
    select 
        ProductId,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate
    from `hive_metastore`.`snapshots`.`product_snapshot` where dbt_valid_to is null
)
productmodel_snapshot as (
^^^
    select 
        ProductModelID,
        Name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot` where dbt_valid_to is null
),
transformed as (
    select
    row_number() over (order by product_snapshot.ProductId) as product_sk, -- auto-incrementing surrogate key
    product_snapshot.Name as product_name,
    product_snapshot.StandardCost,
    product_snapshot.ListPrice,
    product_snapshot.Size,
    product_snapshot.Weight,
    productmodel_snapshot.Name as model,
    productmodel_snapshot.CatalogDescription as description,
    product_snapshot.SellStartDate,
    product_snapshot.SellEndDate,
    product_snapshot.DiscontinuedDate
    from product_snapshot
    left join productmodel_snapshot on product_snapshot.ProductModelID = productmodel_snapshot.ProductModelID
)
select * from transformed
  

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'productmodel_snapshot'.(line 36, pos 0)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/dim_product'
      
      
      as
      
with product_snapshot as (
    select 
        ProductId,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate
    from `hive_metastore`.`snapshots`.`product_snapshot` where dbt_valid_to is null
)
productmodel_snapshot as (
^^^
    select 
        ProductModelID,
        Name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot` where dbt_valid_to is null
),
transformed as (
    select
    row_number() over (order by product_snapshot.ProductId) as product_sk, -- auto-incrementing surrogate key
    product_snapshot.Name as product_name,
    product_snapshot.StandardCost,
    product_snapshot.ListPrice,
    product_snapshot.Size,
    product_snapshot.Weight,
    productmodel_snapshot.Name as model,
    productmodel_snapshot.CatalogDescription as description,
    product_snapshot.SellStartDate,
    product_snapshot.SellEndDate,
    product_snapshot.DiscontinuedDate
    from product_snapshot
    left join productmodel_snapshot on product_snapshot.ProductModelID = productmodel_snapshot.ProductModelID
)
select * from transformed
  

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:267)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:101)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:111)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:87)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:541)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:538)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1127)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:526)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:607)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:624)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:607)
	... 35 more

[0m23:33:38.281284 [debug] [Thread-1 (]: Databricks adapter: operation-id: 4ecda9ea-fd11-4cdf-afaa-4485406da4b2
[0m23:33:38.282273 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_product (execute): 23:33:38.162332 => 23:33:38.282273
[0m23:33:38.283305 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: _release sess: a664bcb8-fd4e-4f72-ac45-c9f06a04ad2f, name: model.medallion_dbt_spark.dim_product, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842018.2822735
[0m23:33:38.286273 [debug] [Thread-1 (]: Runtime Error in model dim_product (models\marts\product\dim_product.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'productmodel_snapshot'.(line 36, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_product"} */
  
    
      
          create or replace table `hive_metastore`.`saleslt`.`dim_product`
        
        
      using delta
        
        
        
        
        
      location '/mnt/silver/customer/dim_product'
        
        
        as
        
  with product_snapshot as (
      select 
          ProductId,
          Name,
          ProductNumber,
          Color,
          StandardCost,
          ListPrice,
          Size,
          Weight,
          ProductCategoryID,
          ProductModelID,
          SellStartDate,
          SellEndDate,
          DiscontinuedDate
      from `hive_metastore`.`snapshots`.`product_snapshot` where dbt_valid_to is null
  )
  productmodel_snapshot as (
  ^^^
      select 
          ProductModelID,
          Name,
          CatalogDescription,
          row_number() over (order by name) as model_id
      from `hive_metastore`.`snapshots`.`productmodel_snapshot` where dbt_valid_to is null
  ),
  transformed as (
      select
      row_number() over (order by product_snapshot.ProductId) as product_sk, -- auto-incrementing surrogate key
      product_snapshot.Name as product_name,
      product_snapshot.StandardCost,
      product_snapshot.ListPrice,
      product_snapshot.Size,
      product_snapshot.Weight,
      productmodel_snapshot.Name as model,
      productmodel_snapshot.CatalogDescription as description,
      product_snapshot.SellStartDate,
      product_snapshot.SellEndDate,
      product_snapshot.DiscontinuedDate
      from product_snapshot
      left join productmodel_snapshot on product_snapshot.ProductModelID = productmodel_snapshot.ProductModelID
  )
  select * from transformed
    
  
[0m23:33:38.286273 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: _release sess: a664bcb8-fd4e-4f72-ac45-c9f06a04ad2f, name: model.medallion_dbt_spark.dim_product, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842018.2862732
[0m23:33:38.287195 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d77af87-7953-4c99-944f-e70f40b3d7c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BEC4881390>]}
[0m23:33:38.288194 [error] [Thread-1 (]: 2 of 5 ERROR creating sql table model saleslt.dim_product ...................... [[31mERROR[0m in 0.13s]
[0m23:33:38.289190 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.dim_product
[0m23:33:38.289190 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.dim_sales
[0m23:33:38.290187 [info ] [Thread-1 (]: 3 of 5 START sql table model saleslt.dim_sales ................................. [RUN]
[0m23:33:38.292403 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: idle check connection: sess: a664bcb8-fd4e-4f72-ac45-c9f06a04ad2f, name: model.medallion_dbt_spark.dim_product, idle: 0.0061304569244384766s, acqrelcnt: 0, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842018.2862732
[0m23:33:38.292403 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.dim_product, now model.medallion_dbt_spark.dim_sales)
[0m23:33:38.293402 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: reusing connection model.medallion_dbt_spark.dim_product sess: a664bcb8-fd4e-4f72-ac45-c9f06a04ad2f, name: model.medallion_dbt_spark.dim_sales, idle: 0.0061304569244384766s, acqrelcnt: 0, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842018.2862732
[0m23:33:38.293402 [debug] [Thread-1 (]: Databricks adapter: On thread (25572, 14120): `hive_metastore`.`saleslt`.`dim_sales` using default compute resource.
[0m23:33:38.293402 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: _acquire sess: a664bcb8-fd4e-4f72-ac45-c9f06a04ad2f, name: model.medallion_dbt_spark.dim_sales, idle: 0.007129192352294922s, acqrelcnt: 1, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842018.2862732
[0m23:33:38.294410 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.dim_sales
[0m23:33:38.298402 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.dim_sales"
[0m23:33:38.299551 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_sales (compile): 23:33:38.294410 => 23:33:38.299456
[0m23:33:38.299551 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.dim_sales
[0m23:33:38.304562 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_dbt_spark.dim_sales"
[0m23:33:38.308557 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: get_thread_connection: sess: a664bcb8-fd4e-4f72-ac45-c9f06a04ad2f, name: model.medallion_dbt_spark.dim_sales, idle: 0.022284507751464844s, acqrelcnt: 1, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842018.2862732
[0m23:33:38.309554 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: idle check connection: sess: a664bcb8-fd4e-4f72-ac45-c9f06a04ad2f, name: model.medallion_dbt_spark.dim_sales, idle: 0.023280620574951172s, acqrelcnt: 1, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842018.2862732
[0m23:33:38.310944 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.dim_sales"
[0m23:33:38.310944 [debug] [Thread-1 (]: On model.medallion_dbt_spark.dim_sales: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_sales`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/dim_sales'
      
      
      as
      
with product_snapshot as (
    select 
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    from `hive_metastore`.`saleslt`.`product` 
 salesorderdetail_snapshot as (
    select 
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    from `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` 
),
salesorderheader_snapshot as (
    select 
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    from `hive_metastore`.`saleslt`.`salesorderheader` 
),
transformed as (
    select
    product_snapshot.Name,
    product_snapshot.ProductNumber,
    product_snapshot.Color,
    product_snapshot.StandardCost,
    product_snapshot.ListPrice,
    product_snapshot.Size,
    product_snapshot.Weight,
    product_snapshot.SellStartDate,
    product_snapshot.SellEndDate,
    product_snapshot.DiscontinuedDate,
    product_snapshot.ThumbNailPhoto,
    product_snapshot.ThumbnailPhotoFileName,
    salesorderdetail_snapshot.SalesOrderID,
    salesorderdetail_snapshot.SalesOrderDetailID,
    salesorderdetail_snapshot.OrderQty,
    salesorderdetail_snapshot.ProductID,
    salesorderdetail_snapshot.UnitPrice,
    salesorderdetail_snapshot.UnitPriceDiscount,
    salesorderdetail_snapshot.LineTotal,
    salesorderheader_snapshot.RevisionNumber,
    salesorderheader_snapshot.OrderDate,
    salesorderheader_snapshot.DueDate,
    salesorderheader_snapshot.ShipDate,
    salesorderheader_snapshot.Status,
    salesorderheader_snapshot.OnlineOrderFlag,
    salesorderheader_snapshot.SalesOrderNumber,
    salesorderheader_snapshot.PurchaseOrderNumber,
    salesorderheader_snapshot.AccountNumber,
    salesorderheader_snapshot.CustomerID,
    salesorderheader_snapshot.BillToAddressID,
    salesorderheader_snapshot.ShipToAddressID,
    salesorderheader_snapshot.ShipMethod,
    salesorderheader_snapshot.CreditCardApprovalCode,
    salesorderheader_snapshot.SubTotal,
    salesorderheader_snapshot.TaxAmt,
    salesorderheader_snapshot.Freight,
    salesorderheader_snapshot.TotalDue,
    salesorderheader_snapshot.Comment
    from product_snapshot
    left join salesorderdetail_snapshot on product_snapshot.ProductID = salesorderdetail_snapshot.ProductID
    left join salesorderheader_snapshot on salesorderdetail_snapshot.SalesOrderID = salesorderheader_snapshot.SalesOrderID
)

select * from transformed
  
[0m23:33:38.426491 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_sales`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/dim_sales'
      
      
      as
      
with product_snapshot as (
    select 
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    from `hive_metastore`.`saleslt`.`product` 
 salesorderdetail_snapshot as (
    select 
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    from `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` 
),
salesorderheader_snapshot as (
    select 
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    from `hive_metastore`.`saleslt`.`salesorderheader` 
),
transformed as (
    select
    product_snapshot.Name,
    product_snapshot.ProductNumber,
    product_snapshot.Color,
    product_snapshot.StandardCost,
    product_snapshot.ListPrice,
    product_snapshot.Size,
    product_snapshot.Weight,
    product_snapshot.SellStartDate,
    product_snapshot.SellEndDate,
    product_snapshot.DiscontinuedDate,
    product_snapshot.ThumbNailPhoto,
    product_snapshot.ThumbnailPhotoFileName,
    salesorderdetail_snapshot.SalesOrderID,
    salesorderdetail_snapshot.SalesOrderDetailID,
    salesorderdetail_snapshot.OrderQty,
    salesorderdetail_snapshot.ProductID,
    salesorderdetail_snapshot.UnitPrice,
    salesorderdetail_snapshot.UnitPriceDiscount,
    salesorderdetail_snapshot.LineTotal,
    salesorderheader_snapshot.RevisionNumber,
    salesorderheader_snapshot.OrderDate,
    salesorderheader_snapshot.DueDate,
    salesorderheader_snapshot.ShipDate,
    salesorderheader_snapshot.Status,
    salesorderheader_snapshot.OnlineOrderFlag,
    salesorderheader_snapshot.SalesOrderNumber,
    salesorderheader_snapshot.PurchaseOrderNumber,
    salesorderheader_snapshot.AccountNumber,
    salesorderheader_snapshot.CustomerID,
    salesorderheader_snapshot.BillToAddressID,
    salesorderheader_snapshot.ShipToAddressID,
    salesorderheader_snapshot.ShipMethod,
    salesorderheader_snapshot.CreditCardApprovalCode,
    salesorderheader_snapshot.SubTotal,
    salesorderheader_snapshot.TaxAmt,
    salesorderheader_snapshot.Freight,
    salesorderheader_snapshot.TotalDue,
    salesorderheader_snapshot.Comment
    from product_snapshot
    left join salesorderdetail_snapshot on product_snapshot.ProductID = salesorderdetail_snapshot.ProductID
    left join salesorderheader_snapshot on salesorderdetail_snapshot.SalesOrderID = salesorderheader_snapshot.SalesOrderID
)

select * from transformed
  
[0m23:33:38.426491 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'as'.(line 35, pos 27)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_sales`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/dim_sales'
      
      
      as
      
with product_snapshot as (
    select 
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    from `hive_metastore`.`saleslt`.`product` 
 salesorderdetail_snapshot as (
---------------------------^^^
    select 
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    from `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` 
),
salesorderheader_snapshot as (
    select 
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    from `hive_metastore`.`saleslt`.`salesorderheader` 
),
transformed as (
    select
    product_snapshot.Name,
    product_snapshot.ProductNumber,
    product_snapshot.Color,
    product_snapshot.StandardCost,
    product_snapshot.ListPrice,
    product_snapshot.Size,
    product_snapshot.Weight,
    product_snapshot.SellStartDate,
    product_snapshot.SellEndDate,
    product_snapshot.DiscontinuedDate,
    product_snapshot.ThumbNailPhoto,
    product_snapshot.ThumbnailPhotoFileName,
    salesorderdetail_snapshot.SalesOrderID,
    salesorderdetail_snapshot.SalesOrderDetailID,
    salesorderdetail_snapshot.OrderQty,
    salesorderdetail_snapshot.ProductID,
    salesorderdetail_snapshot.UnitPrice,
    salesorderdetail_snapshot.UnitPriceDiscount,
    salesorderdetail_snapshot.LineTotal,
    salesorderheader_snapshot.RevisionNumber,
    salesorderheader_snapshot.OrderDate,
    salesorderheader_snapshot.DueDate,
    salesorderheader_snapshot.ShipDate,
    salesorderheader_snapshot.Status,
    salesorderheader_snapshot.OnlineOrderFlag,
    salesorderheader_snapshot.SalesOrderNumber,
    salesorderheader_snapshot.PurchaseOrderNumber,
    salesorderheader_snapshot.AccountNumber,
    salesorderheader_snapshot.CustomerID,
    salesorderheader_snapshot.BillToAddressID,
    salesorderheader_snapshot.ShipToAddressID,
    salesorderheader_snapshot.ShipMethod,
    salesorderheader_snapshot.CreditCardApprovalCode,
    salesorderheader_snapshot.SubTotal,
    salesorderheader_snapshot.TaxAmt,
    salesorderheader_snapshot.Freight,
    salesorderheader_snapshot.TotalDue,
    salesorderheader_snapshot.Comment
    from product_snapshot
    left join salesorderdetail_snapshot on product_snapshot.ProductID = salesorderdetail_snapshot.ProductID
    left join salesorderheader_snapshot on salesorderdetail_snapshot.SalesOrderID = salesorderheader_snapshot.SalesOrderID
)

select * from transformed
  

[0m23:33:38.427495 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'as'.(line 35, pos 27)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_sales`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/dim_sales'
      
      
      as
      
with product_snapshot as (
    select 
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    from `hive_metastore`.`saleslt`.`product` 
 salesorderdetail_snapshot as (
---------------------------^^^
    select 
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    from `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` 
),
salesorderheader_snapshot as (
    select 
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    from `hive_metastore`.`saleslt`.`salesorderheader` 
),
transformed as (
    select
    product_snapshot.Name,
    product_snapshot.ProductNumber,
    product_snapshot.Color,
    product_snapshot.StandardCost,
    product_snapshot.ListPrice,
    product_snapshot.Size,
    product_snapshot.Weight,
    product_snapshot.SellStartDate,
    product_snapshot.SellEndDate,
    product_snapshot.DiscontinuedDate,
    product_snapshot.ThumbNailPhoto,
    product_snapshot.ThumbnailPhotoFileName,
    salesorderdetail_snapshot.SalesOrderID,
    salesorderdetail_snapshot.SalesOrderDetailID,
    salesorderdetail_snapshot.OrderQty,
    salesorderdetail_snapshot.ProductID,
    salesorderdetail_snapshot.UnitPrice,
    salesorderdetail_snapshot.UnitPriceDiscount,
    salesorderdetail_snapshot.LineTotal,
    salesorderheader_snapshot.RevisionNumber,
    salesorderheader_snapshot.OrderDate,
    salesorderheader_snapshot.DueDate,
    salesorderheader_snapshot.ShipDate,
    salesorderheader_snapshot.Status,
    salesorderheader_snapshot.OnlineOrderFlag,
    salesorderheader_snapshot.SalesOrderNumber,
    salesorderheader_snapshot.PurchaseOrderNumber,
    salesorderheader_snapshot.AccountNumber,
    salesorderheader_snapshot.CustomerID,
    salesorderheader_snapshot.BillToAddressID,
    salesorderheader_snapshot.ShipToAddressID,
    salesorderheader_snapshot.ShipMethod,
    salesorderheader_snapshot.CreditCardApprovalCode,
    salesorderheader_snapshot.SubTotal,
    salesorderheader_snapshot.TaxAmt,
    salesorderheader_snapshot.Freight,
    salesorderheader_snapshot.TotalDue,
    salesorderheader_snapshot.Comment
    from product_snapshot
    left join salesorderdetail_snapshot on product_snapshot.ProductID = salesorderdetail_snapshot.ProductID
    left join salesorderheader_snapshot on salesorderdetail_snapshot.SalesOrderID = salesorderheader_snapshot.SalesOrderID
)

select * from transformed
  

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'as'.(line 35, pos 27)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_sales`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/dim_sales'
      
      
      as
      
with product_snapshot as (
    select 
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    from `hive_metastore`.`saleslt`.`product` 
 salesorderdetail_snapshot as (
---------------------------^^^
    select 
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    from `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` 
),
salesorderheader_snapshot as (
    select 
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    from `hive_metastore`.`saleslt`.`salesorderheader` 
),
transformed as (
    select
    product_snapshot.Name,
    product_snapshot.ProductNumber,
    product_snapshot.Color,
    product_snapshot.StandardCost,
    product_snapshot.ListPrice,
    product_snapshot.Size,
    product_snapshot.Weight,
    product_snapshot.SellStartDate,
    product_snapshot.SellEndDate,
    product_snapshot.DiscontinuedDate,
    product_snapshot.ThumbNailPhoto,
    product_snapshot.ThumbnailPhotoFileName,
    salesorderdetail_snapshot.SalesOrderID,
    salesorderdetail_snapshot.SalesOrderDetailID,
    salesorderdetail_snapshot.OrderQty,
    salesorderdetail_snapshot.ProductID,
    salesorderdetail_snapshot.UnitPrice,
    salesorderdetail_snapshot.UnitPriceDiscount,
    salesorderdetail_snapshot.LineTotal,
    salesorderheader_snapshot.RevisionNumber,
    salesorderheader_snapshot.OrderDate,
    salesorderheader_snapshot.DueDate,
    salesorderheader_snapshot.ShipDate,
    salesorderheader_snapshot.Status,
    salesorderheader_snapshot.OnlineOrderFlag,
    salesorderheader_snapshot.SalesOrderNumber,
    salesorderheader_snapshot.PurchaseOrderNumber,
    salesorderheader_snapshot.AccountNumber,
    salesorderheader_snapshot.CustomerID,
    salesorderheader_snapshot.BillToAddressID,
    salesorderheader_snapshot.ShipToAddressID,
    salesorderheader_snapshot.ShipMethod,
    salesorderheader_snapshot.CreditCardApprovalCode,
    salesorderheader_snapshot.SubTotal,
    salesorderheader_snapshot.TaxAmt,
    salesorderheader_snapshot.Freight,
    salesorderheader_snapshot.TotalDue,
    salesorderheader_snapshot.Comment
    from product_snapshot
    left join salesorderdetail_snapshot on product_snapshot.ProductID = salesorderdetail_snapshot.ProductID
    left join salesorderheader_snapshot on salesorderdetail_snapshot.SalesOrderID = salesorderheader_snapshot.SalesOrderID
)

select * from transformed
  

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:267)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:101)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:111)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:87)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$3(SparkExecuteStatementOperation.scala:541)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:538)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1127)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:526)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:536)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:607)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:624)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:607)
	... 35 more

[0m23:33:38.428402 [debug] [Thread-1 (]: Databricks adapter: operation-id: b52b502d-4ef8-4a56-8ed6-d5a8337cad1b
[0m23:33:38.429402 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_sales (execute): 23:33:38.300643 => 23:33:38.429402
[0m23:33:38.429402 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: _release sess: a664bcb8-fd4e-4f72-ac45-c9f06a04ad2f, name: model.medallion_dbt_spark.dim_sales, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842018.4294024
[0m23:33:38.432684 [debug] [Thread-1 (]: Runtime Error in model dim_sales (models\marts\sales\dim_sales.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'as'.(line 35, pos 27)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_sales"} */
  
    
      
          create or replace table `hive_metastore`.`saleslt`.`dim_sales`
        
        
      using delta
        
        
        
        
        
      location '/mnt/silver/customer/dim_sales'
        
        
        as
        
  with product_snapshot as (
      select 
          ProductID,
          Name,
          ProductNumber,
          Color,
          StandardCost,
          ListPrice,
          Size,
          Weight,
          SellStartDate,
          SellEndDate,
          DiscontinuedDate,
          ThumbNailPhoto,
          ThumbnailPhotoFileName
      from `hive_metastore`.`saleslt`.`product` 
   salesorderdetail_snapshot as (
  ---------------------------^^^
      select 
          SalesOrderID,
          SalesOrderDetailID,
          OrderQty,
          ProductID,
          UnitPrice,
          UnitPriceDiscount,
          LineTotal
      from `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` 
  ),
  salesorderheader_snapshot as (
      select 
          SalesOrderID,
          RevisionNumber,
          OrderDate,
          DueDate,
          ShipDate,
          Status,
          OnlineOrderFlag,
          SalesOrderNumber,
          PurchaseOrderNumber,
          AccountNumber,
          CustomerID,
          ShipToAddressID,
          BillToAddressID,
          ShipMethod,
          CreditCardApprovalCode,
          SubTotal,
          TaxAmt,
          Freight,
          TotalDue,
          Comment,
          row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
      from `hive_metastore`.`saleslt`.`salesorderheader` 
  ),
  transformed as (
      select
      product_snapshot.Name,
      product_snapshot.ProductNumber,
      product_snapshot.Color,
      product_snapshot.StandardCost,
      product_snapshot.ListPrice,
      product_snapshot.Size,
      product_snapshot.Weight,
      product_snapshot.SellStartDate,
      product_snapshot.SellEndDate,
      product_snapshot.DiscontinuedDate,
      product_snapshot.ThumbNailPhoto,
      product_snapshot.ThumbnailPhotoFileName,
      salesorderdetail_snapshot.SalesOrderID,
      salesorderdetail_snapshot.SalesOrderDetailID,
      salesorderdetail_snapshot.OrderQty,
      salesorderdetail_snapshot.ProductID,
      salesorderdetail_snapshot.UnitPrice,
      salesorderdetail_snapshot.UnitPriceDiscount,
      salesorderdetail_snapshot.LineTotal,
      salesorderheader_snapshot.RevisionNumber,
      salesorderheader_snapshot.OrderDate,
      salesorderheader_snapshot.DueDate,
      salesorderheader_snapshot.ShipDate,
      salesorderheader_snapshot.Status,
      salesorderheader_snapshot.OnlineOrderFlag,
      salesorderheader_snapshot.SalesOrderNumber,
      salesorderheader_snapshot.PurchaseOrderNumber,
      salesorderheader_snapshot.AccountNumber,
      salesorderheader_snapshot.CustomerID,
      salesorderheader_snapshot.BillToAddressID,
      salesorderheader_snapshot.ShipToAddressID,
      salesorderheader_snapshot.ShipMethod,
      salesorderheader_snapshot.CreditCardApprovalCode,
      salesorderheader_snapshot.SubTotal,
      salesorderheader_snapshot.TaxAmt,
      salesorderheader_snapshot.Freight,
      salesorderheader_snapshot.TotalDue,
      salesorderheader_snapshot.Comment
      from product_snapshot
      left join salesorderdetail_snapshot on product_snapshot.ProductID = salesorderdetail_snapshot.ProductID
      left join salesorderheader_snapshot on salesorderdetail_snapshot.SalesOrderID = salesorderheader_snapshot.SalesOrderID
  )
  
  select * from transformed
    
  
[0m23:33:38.433700 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: _release sess: a664bcb8-fd4e-4f72-ac45-c9f06a04ad2f, name: model.medallion_dbt_spark.dim_sales, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842018.4337003
[0m23:33:38.433700 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d77af87-7953-4c99-944f-e70f40b3d7c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BEC476CB90>]}
[0m23:33:38.434619 [error] [Thread-1 (]: 3 of 5 ERROR creating sql table model saleslt.dim_sales ........................ [[31mERROR[0m in 0.14s]
[0m23:33:38.436589 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.dim_sales
[0m23:33:38.437093 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.my_first_dbt_model
[0m23:33:38.437093 [info ] [Thread-1 (]: 4 of 5 START sql table model saleslt.my_first_dbt_model ........................ [RUN]
[0m23:33:38.439098 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: idle check connection: sess: a664bcb8-fd4e-4f72-ac45-c9f06a04ad2f, name: model.medallion_dbt_spark.dim_sales, idle: 0.004399299621582031s, acqrelcnt: 0, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842018.4337003
[0m23:33:38.439098 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.dim_sales, now model.medallion_dbt_spark.my_first_dbt_model)
[0m23:33:38.440099 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: reusing connection model.medallion_dbt_spark.dim_sales sess: a664bcb8-fd4e-4f72-ac45-c9f06a04ad2f, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.005398273468017578s, acqrelcnt: 0, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842018.4337003
[0m23:33:38.440099 [debug] [Thread-1 (]: Databricks adapter: On thread (25572, 14120): `hive_metastore`.`saleslt`.`my_first_dbt_model` using default compute resource.
[0m23:33:38.440099 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: _acquire sess: a664bcb8-fd4e-4f72-ac45-c9f06a04ad2f, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.006399631500244141s, acqrelcnt: 1, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842018.4337003
[0m23:33:38.441098 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.my_first_dbt_model
[0m23:33:38.444099 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.my_first_dbt_model"
[0m23:33:38.445129 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.my_first_dbt_model (compile): 23:33:38.441098 => 23:33:38.445129
[0m23:33:38.446099 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.my_first_dbt_model
[0m23:33:38.450328 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_dbt_spark.my_first_dbt_model"
[0m23:33:38.451440 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: get_thread_connection: sess: a664bcb8-fd4e-4f72-ac45-c9f06a04ad2f, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.01774001121520996s, acqrelcnt: 1, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842018.4337003
[0m23:33:38.452332 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: idle check connection: sess: a664bcb8-fd4e-4f72-ac45-c9f06a04ad2f, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.01863241195678711s, acqrelcnt: 1, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842018.4337003
[0m23:33:38.452332 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.my_first_dbt_model"
[0m23:33:38.453331 [debug] [Thread-1 (]: On model.medallion_dbt_spark.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.my_first_dbt_model"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`my_first_dbt_model`
      
      
    using delta
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m23:33:41.139548 [debug] [Thread-1 (]: SQL status: OK in 2.690000057220459 seconds
[0m23:33:41.164247 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.my_first_dbt_model (execute): 23:33:38.446099 => 23:33:41.164247
[0m23:33:41.165163 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: _release sess: a664bcb8-fd4e-4f72-ac45-c9f06a04ad2f, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842021.164247
[0m23:33:41.165163 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: _release sess: a664bcb8-fd4e-4f72-ac45-c9f06a04ad2f, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842021.1651633
[0m23:33:41.166162 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d77af87-7953-4c99-944f-e70f40b3d7c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BEC498D2D0>]}
[0m23:33:41.167168 [info ] [Thread-1 (]: 4 of 5 OK created sql table model saleslt.my_first_dbt_model ................... [[32mOK[0m in 2.73s]
[0m23:33:41.168904 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.my_first_dbt_model
[0m23:33:41.169860 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.my_second_dbt_model
[0m23:33:41.169860 [info ] [Thread-1 (]: 5 of 5 START sql view model saleslt.my_second_dbt_model ........................ [RUN]
[0m23:33:41.171896 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: idle check connection: sess: a664bcb8-fd4e-4f72-ac45-c9f06a04ad2f, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.006732940673828125s, acqrelcnt: 0, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842021.1651633
[0m23:33:41.171896 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.my_first_dbt_model, now model.medallion_dbt_spark.my_second_dbt_model)
[0m23:33:41.172865 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: reusing connection model.medallion_dbt_spark.my_first_dbt_model sess: a664bcb8-fd4e-4f72-ac45-c9f06a04ad2f, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.00770258903503418s, acqrelcnt: 0, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842021.1651633
[0m23:33:41.172865 [debug] [Thread-1 (]: Databricks adapter: On thread (25572, 14120): `hive_metastore`.`saleslt`.`my_second_dbt_model` using default compute resource.
[0m23:33:41.173860 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: _acquire sess: a664bcb8-fd4e-4f72-ac45-c9f06a04ad2f, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.008696794509887695s, acqrelcnt: 1, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842021.1651633
[0m23:33:41.173860 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.my_second_dbt_model
[0m23:33:41.178235 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.my_second_dbt_model"
[0m23:33:41.182080 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.my_second_dbt_model (compile): 23:33:41.175038 => 23:33:41.181070
[0m23:33:41.182080 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.my_second_dbt_model
[0m23:33:41.200714 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_dbt_spark.my_second_dbt_model"
[0m23:33:41.201632 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: get_thread_connection: sess: a664bcb8-fd4e-4f72-ac45-c9f06a04ad2f, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.036469459533691406s, acqrelcnt: 1, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842021.1651633
[0m23:33:41.202631 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: idle check connection: sess: a664bcb8-fd4e-4f72-ac45-c9f06a04ad2f, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.03746795654296875s, acqrelcnt: 1, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842021.1651633
[0m23:33:41.202631 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.my_second_dbt_model"
[0m23:33:41.203634 [debug] [Thread-1 (]: On model.medallion_dbt_spark.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.my_second_dbt_model"} */
create or replace view `hive_metastore`.`saleslt`.`my_second_dbt_model`
  
  
  
  as
    -- Use the `ref` function to select from other models

select *
from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id = 1

[0m23:33:42.210899 [debug] [Thread-1 (]: SQL status: OK in 1.0099999904632568 seconds
[0m23:33:42.213892 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.my_second_dbt_model (execute): 23:33:41.182080 => 23:33:42.212892
[0m23:33:42.213892 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: _release sess: a664bcb8-fd4e-4f72-ac45-c9f06a04ad2f, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842022.213892
[0m23:33:42.214866 [debug] [Thread-1 (]: Databricks adapter: conn: 3018365402320: _release sess: a664bcb8-fd4e-4f72-ac45-c9f06a04ad2f, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (25572, 14120), cmpt: ``, lut: 1707842022.2148666
[0m23:33:42.215889 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d77af87-7953-4c99-944f-e70f40b3d7c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BEC4EB20D0>]}
[0m23:33:42.215889 [info ] [Thread-1 (]: 5 of 5 OK created sql view model saleslt.my_second_dbt_model ................... [[32mOK[0m in 1.05s]
[0m23:33:42.217822 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.my_second_dbt_model
[0m23:33:42.219861 [debug] [MainThread]: Databricks adapter: conn: 3018362398544: idle check connection: sess: None, name: master, idle: 4.712559461593628s, acqrelcnt: 0, lang: None, thrd: (25572, 2656), cmpt: ``, lut: 1707842017.5073016
[0m23:33:42.219861 [debug] [MainThread]: Databricks adapter: conn: 3018362398544: reusing connection master sess: None, name: master, idle: 4.712559461593628s, acqrelcnt: 0, lang: None, thrd: (25572, 2656), cmpt: ``, lut: 1707842017.5073016
[0m23:33:42.220818 [debug] [MainThread]: Databricks adapter: Thread (25572, 2656) using default compute resource.
[0m23:33:42.220818 [debug] [MainThread]: Databricks adapter: conn: 3018362398544: _acquire sess: None, name: master, idle: 4.713516712188721s, acqrelcnt: 1, lang: None, thrd: (25572, 2656), cmpt: ``, lut: 1707842017.5073016
[0m23:33:42.221819 [debug] [MainThread]: Databricks adapter: conn: 3018362398544: get_thread_connection: sess: None, name: master, idle: 4.714517593383789s, acqrelcnt: 1, lang: None, thrd: (25572, 2656), cmpt: ``, lut: 1707842017.5073016
[0m23:33:42.221819 [debug] [MainThread]: Databricks adapter: conn: 3018362398544: idle check connection: sess: None, name: master, idle: 4.714517593383789s, acqrelcnt: 1, lang: None, thrd: (25572, 2656), cmpt: ``, lut: 1707842017.5073016
[0m23:33:42.222816 [debug] [MainThread]: On master: ROLLBACK
[0m23:33:42.223817 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:33:42.435875 [debug] [MainThread]: Databricks adapter: conn: 3018362398544: session opened sess: 27a9ea9c-8a32-48a5-9807-c66bc418cf79, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (25572, 2656), cmpt: ``, lut: 1707842022.4358757
[0m23:33:42.436880 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:33:42.436880 [debug] [MainThread]: Databricks adapter: conn: 3018362398544: get_thread_connection: sess: 27a9ea9c-8a32-48a5-9807-c66bc418cf79, name: master, idle: 0.0010044574737548828s, acqrelcnt: 1, lang: None, thrd: (25572, 2656), cmpt: ``, lut: 1707842022.4358757
[0m23:33:42.437881 [debug] [MainThread]: Databricks adapter: conn: 3018362398544: idle check connection: sess: 27a9ea9c-8a32-48a5-9807-c66bc418cf79, name: master, idle: 0.0010044574737548828s, acqrelcnt: 1, lang: None, thrd: (25572, 2656), cmpt: ``, lut: 1707842022.4358757
[0m23:33:42.437881 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:33:42.437881 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:33:42.438884 [debug] [MainThread]: Databricks adapter: conn: 3018362398544: _release sess: 27a9ea9c-8a32-48a5-9807-c66bc418cf79, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (25572, 2656), cmpt: ``, lut: 1707842022.4388843
[0m23:33:42.440002 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:33:42.440002 [debug] [MainThread]: On master: ROLLBACK
[0m23:33:42.441986 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:33:42.442759 [debug] [MainThread]: On master: Close
[0m23:33:42.562128 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m23:33:42.562128 [debug] [MainThread]: On list_hive_metastore: Close
[0m23:33:42.635347 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m23:33:42.636364 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m23:33:42.636364 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:33:42.637366 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m23:33:42.713828 [debug] [MainThread]: Connection 'model.medallion_dbt_spark.my_second_dbt_model' was properly closed.
[0m23:33:42.713828 [debug] [MainThread]: On model.medallion_dbt_spark.my_second_dbt_model: ROLLBACK
[0m23:33:42.714823 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:33:42.714823 [debug] [MainThread]: On model.medallion_dbt_spark.my_second_dbt_model: Close
[0m23:33:42.796530 [info ] [MainThread]: 
[0m23:33:42.797525 [info ] [MainThread]: Finished running 4 table models, 1 view model in 0 hours 0 minutes and 6.77 seconds (6.77s).
[0m23:33:42.799761 [debug] [MainThread]: Command end result
[0m23:33:42.815924 [info ] [MainThread]: 
[0m23:33:42.817883 [info ] [MainThread]: [31mCompleted with 3 errors and 0 warnings:[0m
[0m23:33:42.818879 [info ] [MainThread]: 
[0m23:33:42.820051 [error] [MainThread]:   Runtime Error in model dim_customer (models\marts\customer\dim_customer.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'inner'.(line 57, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_customer"} */
  
    
      
          create or replace table `hive_metastore`.`saleslt`.`dim_customer`
        
        
      using delta
        
        
        
        
        
      location '/mnt/silver/customer/dim_customer'
        
        
        as
        
  
  with address_snapshot as (
      select 
          AddressID,
          AddressLine1,
          AddressLine2,
          City,
          StateProvince,
          CountryRegion,
          PostalCode
      from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
  ),
  customeraddress_snapshot as (
      select 
          CustomerID,
          AddressID,
          AddressType
      from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
  ),
  customer_snapshot as (
      select 
          CustomerId,
          concat(ifnull(FirstName, ''), ' ', ifnull(MiddleName, ''), ' ', ifnull(LastName, '')) as FullName
      from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
  ),
  transformed as (
      select
      row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incrementing surrogate key
      customer_snapshot.CustomerID,
      customer_snapshot.FullName,
      customer_snapshot.AddressID,
      customer_snapshot.AddressType,
      customer_snapshot.AddressLine1,
      customer_snapshot.City,
      customer_snapshot.StateProvince,
      customer_snapshot.CountryRegion,
      customer_snapshot.PostalCode,
      from customer_snapshot
      inner join customeraddress_snapshot on customer_snapshot.CustomerID = customeraddress_snapshot.CustomerID
  ----^^^
      inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
  )
  select * from transformed
    
  
[0m23:33:42.829267 [info ] [MainThread]: 
[0m23:33:42.830265 [error] [MainThread]:   Runtime Error in model dim_product (models\marts\product\dim_product.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'productmodel_snapshot'.(line 36, pos 0)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_product"} */
  
    
      
          create or replace table `hive_metastore`.`saleslt`.`dim_product`
        
        
      using delta
        
        
        
        
        
      location '/mnt/silver/customer/dim_product'
        
        
        as
        
  with product_snapshot as (
      select 
          ProductId,
          Name,
          ProductNumber,
          Color,
          StandardCost,
          ListPrice,
          Size,
          Weight,
          ProductCategoryID,
          ProductModelID,
          SellStartDate,
          SellEndDate,
          DiscontinuedDate
      from `hive_metastore`.`snapshots`.`product_snapshot` where dbt_valid_to is null
  )
  productmodel_snapshot as (
  ^^^
      select 
          ProductModelID,
          Name,
          CatalogDescription,
          row_number() over (order by name) as model_id
      from `hive_metastore`.`snapshots`.`productmodel_snapshot` where dbt_valid_to is null
  ),
  transformed as (
      select
      row_number() over (order by product_snapshot.ProductId) as product_sk, -- auto-incrementing surrogate key
      product_snapshot.Name as product_name,
      product_snapshot.StandardCost,
      product_snapshot.ListPrice,
      product_snapshot.Size,
      product_snapshot.Weight,
      productmodel_snapshot.Name as model,
      productmodel_snapshot.CatalogDescription as description,
      product_snapshot.SellStartDate,
      product_snapshot.SellEndDate,
      product_snapshot.DiscontinuedDate
      from product_snapshot
      left join productmodel_snapshot on product_snapshot.ProductModelID = productmodel_snapshot.ProductModelID
  )
  select * from transformed
    
  
[0m23:33:42.835071 [info ] [MainThread]: 
[0m23:33:42.836079 [error] [MainThread]:   Runtime Error in model dim_sales (models\marts\sales\dim_sales.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'as'.(line 35, pos 27)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_sales"} */
  
    
      
          create or replace table `hive_metastore`.`saleslt`.`dim_sales`
        
        
      using delta
        
        
        
        
        
      location '/mnt/silver/customer/dim_sales'
        
        
        as
        
  with product_snapshot as (
      select 
          ProductID,
          Name,
          ProductNumber,
          Color,
          StandardCost,
          ListPrice,
          Size,
          Weight,
          SellStartDate,
          SellEndDate,
          DiscontinuedDate,
          ThumbNailPhoto,
          ThumbnailPhotoFileName
      from `hive_metastore`.`saleslt`.`product` 
   salesorderdetail_snapshot as (
  ---------------------------^^^
      select 
          SalesOrderID,
          SalesOrderDetailID,
          OrderQty,
          ProductID,
          UnitPrice,
          UnitPriceDiscount,
          LineTotal
      from `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` 
  ),
  salesorderheader_snapshot as (
      select 
          SalesOrderID,
          RevisionNumber,
          OrderDate,
          DueDate,
          ShipDate,
          Status,
          OnlineOrderFlag,
          SalesOrderNumber,
          PurchaseOrderNumber,
          AccountNumber,
          CustomerID,
          ShipToAddressID,
          BillToAddressID,
          ShipMethod,
          CreditCardApprovalCode,
          SubTotal,
          TaxAmt,
          Freight,
          TotalDue,
          Comment,
          row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
      from `hive_metastore`.`saleslt`.`salesorderheader` 
  ),
  transformed as (
      select
      product_snapshot.Name,
      product_snapshot.ProductNumber,
      product_snapshot.Color,
      product_snapshot.StandardCost,
      product_snapshot.ListPrice,
      product_snapshot.Size,
      product_snapshot.Weight,
      product_snapshot.SellStartDate,
      product_snapshot.SellEndDate,
      product_snapshot.DiscontinuedDate,
      product_snapshot.ThumbNailPhoto,
      product_snapshot.ThumbnailPhotoFileName,
      salesorderdetail_snapshot.SalesOrderID,
      salesorderdetail_snapshot.SalesOrderDetailID,
      salesorderdetail_snapshot.OrderQty,
      salesorderdetail_snapshot.ProductID,
      salesorderdetail_snapshot.UnitPrice,
      salesorderdetail_snapshot.UnitPriceDiscount,
      salesorderdetail_snapshot.LineTotal,
      salesorderheader_snapshot.RevisionNumber,
      salesorderheader_snapshot.OrderDate,
      salesorderheader_snapshot.DueDate,
      salesorderheader_snapshot.ShipDate,
      salesorderheader_snapshot.Status,
      salesorderheader_snapshot.OnlineOrderFlag,
      salesorderheader_snapshot.SalesOrderNumber,
      salesorderheader_snapshot.PurchaseOrderNumber,
      salesorderheader_snapshot.AccountNumber,
      salesorderheader_snapshot.CustomerID,
      salesorderheader_snapshot.BillToAddressID,
      salesorderheader_snapshot.ShipToAddressID,
      salesorderheader_snapshot.ShipMethod,
      salesorderheader_snapshot.CreditCardApprovalCode,
      salesorderheader_snapshot.SubTotal,
      salesorderheader_snapshot.TaxAmt,
      salesorderheader_snapshot.Freight,
      salesorderheader_snapshot.TotalDue,
      salesorderheader_snapshot.Comment
      from product_snapshot
      left join salesorderdetail_snapshot on product_snapshot.ProductID = salesorderdetail_snapshot.ProductID
      left join salesorderheader_snapshot on salesorderdetail_snapshot.SalesOrderID = salesorderheader_snapshot.SalesOrderID
  )
  
  select * from transformed
    
  
[0m23:33:42.843437 [info ] [MainThread]: 
[0m23:33:42.844945 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=3 SKIP=0 TOTAL=5
[0m23:33:42.847207 [debug] [MainThread]: Command `dbt run` failed at 23:33:42.847207 after 8.32 seconds
[0m23:33:42.848206 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BEB173D610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BEB2517150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BEB217CE10>]}
[0m23:33:42.849205 [debug] [MainThread]: Flushing usage events
[0m23:38:08.488125 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001437F1F4F10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001437EF12890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001437F1F4210>]}


============================== 23:38:08.492123 | 316bab98-b196-4780-a0d2-cfe3a572c51d ==============================
[0m23:38:08.492123 [info ] [MainThread]: Running with dbt=1.7.7
[0m23:38:08.494125 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m23:38:09.789652 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '316bab98-b196-4780-a0d2-cfe3a572c51d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000143112F2350>]}
[0m23:38:09.854442 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '316bab98-b196-4780-a0d2-cfe3a572c51d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014311378490>]}
[0m23:38:09.855357 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m23:38:09.866440 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m23:38:09.965534 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 3 files changed.
[0m23:38:09.966535 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://models\marts\sales\dim_sales.sql
[0m23:38:09.966535 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://models\marts\product\dim_product.sql
[0m23:38:09.967534 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://models\marts\customer\dim_customer.sql
[0m23:38:10.284549 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '316bab98-b196-4780-a0d2-cfe3a572c51d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001431194D590>]}
[0m23:38:10.301652 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '316bab98-b196-4780-a0d2-cfe3a572c51d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000143119E8710>]}
[0m23:38:10.301652 [info ] [MainThread]: Found 5 models, 7 snapshots, 8 tests, 9 sources, 0 exposures, 0 metrics, 535 macros, 0 groups, 0 semantic models
[0m23:38:10.302649 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '316bab98-b196-4780-a0d2-cfe3a572c51d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000143114B0410>]}
[0m23:38:10.305649 [info ] [MainThread]: 
[0m23:38:10.306817 [debug] [MainThread]: Databricks adapter: conn: 1387568409936: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (23228, 25200), cmpt: ``, lut: None
[0m23:38:10.306817 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:38:10.307824 [debug] [MainThread]: Databricks adapter: Thread (23228, 25200) using default compute resource.
[0m23:38:10.307824 [debug] [MainThread]: Databricks adapter: conn: 1387568409936: _acquire sess: None, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23228, 25200), cmpt: ``, lut: 1707842290.3078241
[0m23:38:10.309822 [debug] [ThreadPool]: Databricks adapter: conn: 1387570039824: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (23228, 20052), cmpt: ``, lut: None
[0m23:38:10.309822 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m23:38:10.309822 [debug] [ThreadPool]: Databricks adapter: Thread (23228, 20052) using default compute resource.
[0m23:38:10.310821 [debug] [ThreadPool]: Databricks adapter: conn: 1387570039824: _acquire sess: None, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23228, 20052), cmpt: ``, lut: 1707842290.310821
[0m23:38:10.310821 [debug] [ThreadPool]: Databricks adapter: conn: 1387570039824: get_thread_connection: sess: None, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23228, 20052), cmpt: ``, lut: 1707842290.310821
[0m23:38:10.311835 [debug] [ThreadPool]: Databricks adapter: conn: 1387570039824: idle check connection: sess: None, name: list_hive_metastore, idle: 0.0010142326354980469s, acqrelcnt: 1, lang: None, thrd: (23228, 20052), cmpt: ``, lut: 1707842290.310821
[0m23:38:10.311835 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m23:38:10.311835 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m23:38:10.312820 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:38:10.775984 [debug] [ThreadPool]: Databricks adapter: conn: 1387570039824: session opened sess: f7465e17-893a-4ce8-a3e5-92e2ff35c455, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23228, 20052), cmpt: ``, lut: 1707842290.775984
[0m23:38:11.213606 [debug] [ThreadPool]: SQL status: OK in 0.8999999761581421 seconds
[0m23:38:11.218680 [debug] [ThreadPool]: Databricks adapter: conn: 1387570039824: _release sess: f7465e17-893a-4ce8-a3e5-92e2ff35c455, name: list_hive_metastore, idle: 0.0009763240814208984s, acqrelcnt: 0, lang: None, thrd: (23228, 20052), cmpt: ``, lut: 1707842291.2177038
[0m23:38:11.221707 [debug] [ThreadPool]: Databricks adapter: conn: 1387566473232: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_saleslt, idle: 0s, acqrelcnt: 0, lang: None, thrd: (23228, 6524), cmpt: ``, lut: None
[0m23:38:11.222716 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m23:38:11.222716 [debug] [ThreadPool]: Databricks adapter: Thread (23228, 6524) using default compute resource.
[0m23:38:11.223714 [debug] [ThreadPool]: Databricks adapter: conn: 1387566473232: _acquire sess: None, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23228, 6524), cmpt: ``, lut: 1707842291.2227159
[0m23:38:11.227713 [debug] [ThreadPool]: Databricks adapter: conn: 1387566473232: get_thread_connection: sess: None, name: list_hive_metastore_saleslt, idle: 0.00499725341796875s, acqrelcnt: 1, lang: None, thrd: (23228, 6524), cmpt: ``, lut: 1707842291.2227159
[0m23:38:11.228714 [debug] [ThreadPool]: Databricks adapter: conn: 1387566473232: idle check connection: sess: None, name: list_hive_metastore_saleslt, idle: 0.00499725341796875s, acqrelcnt: 1, lang: None, thrd: (23228, 6524), cmpt: ``, lut: 1707842291.2227159
[0m23:38:11.228714 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:38:11.228714 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m23:38:11.229713 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:38:11.463907 [debug] [ThreadPool]: Databricks adapter: conn: 1387566473232: session opened sess: 36e52d6b-099b-47ee-9414-48e9e881e6f9, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23228, 6524), cmpt: ``, lut: 1707842291.4639075
[0m23:38:11.588538 [debug] [ThreadPool]: SQL status: OK in 0.36000001430511475 seconds
[0m23:38:11.669481 [debug] [ThreadPool]: Databricks adapter: conn: 1387566473232: get_thread_connection: sess: 36e52d6b-099b-47ee-9414-48e9e881e6f9, name: list_hive_metastore_saleslt, idle: 0.20557379722595215s, acqrelcnt: 1, lang: None, thrd: (23228, 6524), cmpt: ``, lut: 1707842291.4639075
[0m23:38:11.669481 [debug] [ThreadPool]: Databricks adapter: conn: 1387566473232: idle check connection: sess: 36e52d6b-099b-47ee-9414-48e9e881e6f9, name: list_hive_metastore_saleslt, idle: 0.20557379722595215s, acqrelcnt: 1, lang: None, thrd: (23228, 6524), cmpt: ``, lut: 1707842291.4639075
[0m23:38:11.670476 [debug] [ThreadPool]: Databricks adapter: conn: 1387566473232: get_thread_connection: sess: 36e52d6b-099b-47ee-9414-48e9e881e6f9, name: list_hive_metastore_saleslt, idle: 0.20656943321228027s, acqrelcnt: 1, lang: None, thrd: (23228, 6524), cmpt: ``, lut: 1707842291.4639075
[0m23:38:11.670476 [debug] [ThreadPool]: Databricks adapter: conn: 1387566473232: idle check connection: sess: 36e52d6b-099b-47ee-9414-48e9e881e6f9, name: list_hive_metastore_saleslt, idle: 0.20656943321228027s, acqrelcnt: 1, lang: None, thrd: (23228, 6524), cmpt: ``, lut: 1707842291.4639075
[0m23:38:11.671423 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:38:11.671423 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:38:11.671423 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m23:38:11.871269 [debug] [ThreadPool]: SQL status: OK in 0.20000000298023224 seconds
[0m23:38:11.879528 [debug] [ThreadPool]: Databricks adapter: conn: 1387566473232: get_thread_connection: sess: 36e52d6b-099b-47ee-9414-48e9e881e6f9, name: list_hive_metastore_saleslt, idle: 0.4156205654144287s, acqrelcnt: 1, lang: None, thrd: (23228, 6524), cmpt: ``, lut: 1707842291.4639075
[0m23:38:11.880526 [debug] [ThreadPool]: Databricks adapter: conn: 1387566473232: idle check connection: sess: 36e52d6b-099b-47ee-9414-48e9e881e6f9, name: list_hive_metastore_saleslt, idle: 0.41661906242370605s, acqrelcnt: 1, lang: None, thrd: (23228, 6524), cmpt: ``, lut: 1707842291.4639075
[0m23:38:11.880526 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:38:11.881683 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m23:38:12.220221 [debug] [ThreadPool]: SQL status: OK in 0.3400000035762787 seconds
[0m23:38:12.225443 [debug] [ThreadPool]: Databricks adapter: conn: 1387566473232: _release sess: 36e52d6b-099b-47ee-9414-48e9e881e6f9, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (23228, 6524), cmpt: ``, lut: 1707842292.2243383
[0m23:38:12.226397 [debug] [ThreadPool]: Databricks adapter: conn: 1387566473232: idle check connection: sess: 36e52d6b-099b-47ee-9414-48e9e881e6f9, name: list_hive_metastore_saleslt, idle: 0.0020589828491210938s, acqrelcnt: 0, lang: None, thrd: (23228, 6524), cmpt: ``, lut: 1707842292.2243383
[0m23:38:12.228432 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m23:38:12.229344 [debug] [ThreadPool]: Databricks adapter: conn: 1387566473232: reusing connection list_hive_metastore_saleslt sess: 36e52d6b-099b-47ee-9414-48e9e881e6f9, name: list_hive_metastore_snapshots, idle: 0.005006313323974609s, acqrelcnt: 0, lang: None, thrd: (23228, 6524), cmpt: ``, lut: 1707842292.2243383
[0m23:38:12.229344 [debug] [ThreadPool]: Databricks adapter: Thread (23228, 6524) using default compute resource.
[0m23:38:12.230344 [debug] [ThreadPool]: Databricks adapter: conn: 1387566473232: _acquire sess: 36e52d6b-099b-47ee-9414-48e9e881e6f9, name: list_hive_metastore_snapshots, idle: 0.006006479263305664s, acqrelcnt: 1, lang: None, thrd: (23228, 6524), cmpt: ``, lut: 1707842292.2243383
[0m23:38:12.232609 [debug] [ThreadPool]: Databricks adapter: conn: 1387566473232: get_thread_connection: sess: 36e52d6b-099b-47ee-9414-48e9e881e6f9, name: list_hive_metastore_snapshots, idle: 0.008271455764770508s, acqrelcnt: 1, lang: None, thrd: (23228, 6524), cmpt: ``, lut: 1707842292.2243383
[0m23:38:12.232609 [debug] [ThreadPool]: Databricks adapter: conn: 1387566473232: idle check connection: sess: 36e52d6b-099b-47ee-9414-48e9e881e6f9, name: list_hive_metastore_snapshots, idle: 0.008271455764770508s, acqrelcnt: 1, lang: None, thrd: (23228, 6524), cmpt: ``, lut: 1707842292.2243383
[0m23:38:12.233610 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m23:38:12.233610 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m23:38:12.377008 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m23:38:12.382202 [debug] [ThreadPool]: Databricks adapter: conn: 1387566473232: get_thread_connection: sess: 36e52d6b-099b-47ee-9414-48e9e881e6f9, name: list_hive_metastore_snapshots, idle: 0.15786433219909668s, acqrelcnt: 1, lang: None, thrd: (23228, 6524), cmpt: ``, lut: 1707842292.2243383
[0m23:38:12.382202 [debug] [ThreadPool]: Databricks adapter: conn: 1387566473232: idle check connection: sess: 36e52d6b-099b-47ee-9414-48e9e881e6f9, name: list_hive_metastore_snapshots, idle: 0.15786433219909668s, acqrelcnt: 1, lang: None, thrd: (23228, 6524), cmpt: ``, lut: 1707842292.2243383
[0m23:38:12.383294 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m23:38:12.383294 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m23:38:12.558659 [debug] [ThreadPool]: SQL status: OK in 0.18000000715255737 seconds
[0m23:38:12.563646 [debug] [ThreadPool]: Databricks adapter: conn: 1387566473232: get_thread_connection: sess: 36e52d6b-099b-47ee-9414-48e9e881e6f9, name: list_hive_metastore_snapshots, idle: 0.3393080234527588s, acqrelcnt: 1, lang: None, thrd: (23228, 6524), cmpt: ``, lut: 1707842292.2243383
[0m23:38:12.564646 [debug] [ThreadPool]: Databricks adapter: conn: 1387566473232: idle check connection: sess: 36e52d6b-099b-47ee-9414-48e9e881e6f9, name: list_hive_metastore_snapshots, idle: 0.34030842781066895s, acqrelcnt: 1, lang: None, thrd: (23228, 6524), cmpt: ``, lut: 1707842292.2243383
[0m23:38:12.564646 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m23:38:12.565647 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m23:38:12.712509 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m23:38:12.717406 [debug] [ThreadPool]: Databricks adapter: conn: 1387566473232: _release sess: 36e52d6b-099b-47ee-9414-48e9e881e6f9, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (23228, 6524), cmpt: ``, lut: 1707842292.7174067
[0m23:38:12.720502 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '316bab98-b196-4780-a0d2-cfe3a572c51d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014311570550>]}
[0m23:38:12.721407 [debug] [MainThread]: Databricks adapter: conn: 1387568409936: get_thread_connection: sess: None, name: master, idle: 2.413583517074585s, acqrelcnt: 1, lang: None, thrd: (23228, 25200), cmpt: ``, lut: 1707842290.3078241
[0m23:38:12.722405 [debug] [MainThread]: Databricks adapter: conn: 1387568409936: idle check connection: sess: None, name: master, idle: 2.414581775665283s, acqrelcnt: 1, lang: None, thrd: (23228, 25200), cmpt: ``, lut: 1707842290.3078241
[0m23:38:12.722405 [debug] [MainThread]: Databricks adapter: conn: 1387568409936: get_thread_connection: sess: None, name: master, idle: 2.414581775665283s, acqrelcnt: 1, lang: None, thrd: (23228, 25200), cmpt: ``, lut: 1707842290.3078241
[0m23:38:12.723406 [debug] [MainThread]: Databricks adapter: conn: 1387568409936: idle check connection: sess: None, name: master, idle: 2.4155819416046143s, acqrelcnt: 1, lang: None, thrd: (23228, 25200), cmpt: ``, lut: 1707842290.3078241
[0m23:38:12.723406 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:38:12.723406 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:38:12.724405 [debug] [MainThread]: Databricks adapter: conn: 1387568409936: _release sess: None, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (23228, 25200), cmpt: ``, lut: 1707842292.7244053
[0m23:38:12.725490 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:38:12.725634 [info ] [MainThread]: 
[0m23:38:12.730635 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.dim_customer
[0m23:38:12.731638 [info ] [Thread-1 (]: 1 of 5 START sql table model saleslt.dim_customer .............................. [RUN]
[0m23:38:12.732765 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: Creating DatabricksDBTConnection sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0s, acqrelcnt: 0, lang: None, thrd: (23228, 20152), cmpt: ``, lut: None
[0m23:38:12.733766 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.medallion_dbt_spark.dim_customer'
[0m23:38:12.733766 [debug] [Thread-1 (]: Databricks adapter: On thread (23228, 20152): `hive_metastore`.`saleslt`.`dim_customer` using default compute resource.
[0m23:38:12.734761 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: _acquire sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842292.7347612
[0m23:38:12.734761 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.dim_customer
[0m23:38:12.744759 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.dim_customer"
[0m23:38:12.745759 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_customer (compile): 23:38:12.734761 => 23:38:12.745759
[0m23:38:12.746760 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.dim_customer
[0m23:38:12.814027 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_dbt_spark.dim_customer"
[0m23:38:12.814946 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: get_thread_connection: sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0.0801846981048584s, acqrelcnt: 1, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842292.7347612
[0m23:38:12.815946 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: idle check connection: sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0.0801846981048584s, acqrelcnt: 1, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842292.7347612
[0m23:38:12.815946 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: get_thread_connection: sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0.08118486404418945s, acqrelcnt: 1, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842292.7347612
[0m23:38:12.816944 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: idle check connection: sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0.0821833610534668s, acqrelcnt: 1, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842292.7347612
[0m23:38:12.816944 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m23:38:12.817944 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.dim_customer"
[0m23:38:12.817944 [debug] [Thread-1 (]: On model.medallion_dbt_spark.dim_customer: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_customer`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/dim_customer'
      
      
      as
      

with address_snapshot as (
    select 
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
),
customeraddress_snapshot as (
    select 
        CustomerID,
        AddressID,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
),
customer_snapshot as (
    select 
        CustomerId,
        concat(ifnull(FirstName, ''), ' ', ifnull(MiddleName, ''), ' ', ifnull(LastName, '')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
),
transformed as (
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incrementing surrogate key
    customer_snapshot.CustomerID,
    customer_snapshot.FullName,
    customer_snapshot.AddressID,
    customer_snapshot.AddressType,
    customer_snapshot.AddressLine1,
    customer_snapshot.City,
    customer_snapshot.StateProvince,
    customer_snapshot.CountryRegion,
    customer_snapshot.PostalCode
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerID = customeraddress_snapshot.CustomerID
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)
select * from transformed
  
[0m23:38:12.818945 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:38:13.031585 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: session opened sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.dim_customer, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842293.0315857
[0m23:38:13.736024 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_customer`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/dim_customer'
      
      
      as
      

with address_snapshot as (
    select 
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
),
customeraddress_snapshot as (
    select 
        CustomerID,
        AddressID,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
),
customer_snapshot as (
    select 
        CustomerId,
        concat(ifnull(FirstName, ''), ' ', ifnull(MiddleName, ''), ' ', ifnull(LastName, '')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
),
transformed as (
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incrementing surrogate key
    customer_snapshot.CustomerID,
    customer_snapshot.FullName,
    customer_snapshot.AddressID,
    customer_snapshot.AddressType,
    customer_snapshot.AddressLine1,
    customer_snapshot.City,
    customer_snapshot.StateProvince,
    customer_snapshot.CountryRegion,
    customer_snapshot.PostalCode
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerID = customeraddress_snapshot.CustomerID
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)
select * from transformed
  
[0m23:38:13.738029 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `customer_snapshot`.`AddressID` cannot be resolved. Did you mean one of the following? [`customeraddress_snapshot`.`AddressID`, `address_snapshot`.`AddressID`, `customer_snapshot`.`FullName`, `customer_snapshot`.`CustomerId`, `customeraddress_snapshot`.`AddressType`].; line 49 pos 4
[0m23:38:13.738935 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `customer_snapshot`.`AddressID` cannot be resolved. Did you mean one of the following? [`customeraddress_snapshot`.`AddressID`, `address_snapshot`.`AddressID`, `customer_snapshot`.`FullName`, `customer_snapshot`.`CustomerId`, `customeraddress_snapshot`.`AddressType`].; line 49 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `customer_snapshot`.`AddressID` cannot be resolved. Did you mean one of the following? [`customeraddress_snapshot`.`AddressID`, `address_snapshot`.`AddressID`, `customer_snapshot`.`FullName`, `customer_snapshot`.`CustomerId`, `customeraddress_snapshot`.`AddressType`].; line 49 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:671)
	... 35 more

[0m23:38:13.738935 [debug] [Thread-1 (]: Databricks adapter: operation-id: b9113e82-aec0-4c42-b869-f40b3a2ba315
[0m23:38:13.739935 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_customer (execute): 23:38:12.746760 => 23:38:13.739935
[0m23:38:13.739935 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: _release sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.dim_customer, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842293.7399354
[0m23:38:13.744024 [debug] [Thread-1 (]: Runtime Error in model dim_customer (models\marts\customer\dim_customer.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `customer_snapshot`.`AddressID` cannot be resolved. Did you mean one of the following? [`customeraddress_snapshot`.`AddressID`, `address_snapshot`.`AddressID`, `customer_snapshot`.`FullName`, `customer_snapshot`.`CustomerId`, `customeraddress_snapshot`.`AddressType`].; line 49 pos 4
[0m23:38:13.745030 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: _release sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.dim_customer, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842293.7450306
[0m23:38:13.745030 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '316bab98-b196-4780-a0d2-cfe3a572c51d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000143118460D0>]}
[0m23:38:13.745936 [error] [Thread-1 (]: 1 of 5 ERROR creating sql table model saleslt.dim_customer ..................... [[31mERROR[0m in 1.01s]
[0m23:38:13.746833 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.dim_customer
[0m23:38:13.747880 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.dim_product
[0m23:38:13.748844 [info ] [Thread-1 (]: 2 of 5 START sql table model saleslt.dim_product ............................... [RUN]
[0m23:38:13.750140 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: idle check connection: sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.dim_customer, idle: 0.005110263824462891s, acqrelcnt: 0, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842293.7450306
[0m23:38:13.750140 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.dim_customer, now model.medallion_dbt_spark.dim_product)
[0m23:38:13.751143 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: reusing connection model.medallion_dbt_spark.dim_customer sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.dim_product, idle: 0.006112813949584961s, acqrelcnt: 0, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842293.7450306
[0m23:38:13.751143 [debug] [Thread-1 (]: Databricks adapter: On thread (23228, 20152): `hive_metastore`.`saleslt`.`dim_product` using default compute resource.
[0m23:38:13.752235 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: _acquire sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.dim_product, idle: 0.006112813949584961s, acqrelcnt: 1, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842293.7450306
[0m23:38:13.752235 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.dim_product
[0m23:38:13.757483 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.dim_product"
[0m23:38:13.758553 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_product (compile): 23:38:13.752235 => 23:38:13.758553
[0m23:38:13.758553 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.dim_product
[0m23:38:13.763596 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_dbt_spark.dim_product"
[0m23:38:13.764563 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: get_thread_connection: sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.dim_product, idle: 0.019533395767211914s, acqrelcnt: 1, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842293.7450306
[0m23:38:13.765484 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: idle check connection: sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.dim_product, idle: 0.020453691482543945s, acqrelcnt: 1, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842293.7450306
[0m23:38:13.765484 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.dim_product"
[0m23:38:13.766481 [debug] [Thread-1 (]: On model.medallion_dbt_spark.dim_product: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/dim_product'
      
      
      as
      
with product_snapshot as (
    select 
        ProductId,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate
    from `hive_metastore`.`snapshots`.`product_snapshot` where dbt_valid_to is null
),
productmodel_snapshot as (
    select 
        ProductModelID,
        Name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot` where dbt_valid_to is null
),
transformed as (
    select
    row_number() over (order by product_snapshot.ProductId) as product_sk, -- auto-incrementing surrogate key
    product_snapshot.Name as product_name,
    product_snapshot.StandardCost,
    product_snapshot.ListPrice,
    product_snapshot.Size,
    product_snapshot.Weight,
    productmodel_snapshot.Name as model,
    productmodel_snapshot.CatalogDescription as description,
    product_snapshot.SellStartDate,
    product_snapshot.SellEndDate,
    product_snapshot.DiscontinuedDate
    from product_snapshot
    left join productmodel_snapshot on product_snapshot.ProductModelID = productmodel_snapshot.ProductModelID
)
select * from transformed
  
[0m23:38:19.218250 [debug] [Thread-1 (]: SQL status: OK in 5.449999809265137 seconds
[0m23:38:19.326567 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_product (execute): 23:38:13.759485 => 23:38:19.326567
[0m23:38:19.327586 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: _release sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.dim_product, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842299.3265672
[0m23:38:19.328500 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: _release sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.dim_product, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842299.3285005
[0m23:38:19.328500 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '316bab98-b196-4780-a0d2-cfe3a572c51d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014311844090>]}
[0m23:38:19.329546 [info ] [Thread-1 (]: 2 of 5 OK created sql table model saleslt.dim_product .......................... [[32mOK[0m in 5.58s]
[0m23:38:19.330499 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.dim_product
[0m23:38:19.331757 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.dim_sales
[0m23:38:19.331757 [info ] [Thread-1 (]: 3 of 5 START sql table model saleslt.dim_sales ................................. [RUN]
[0m23:38:19.332976 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: idle check connection: sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.dim_product, idle: 0.004476070404052734s, acqrelcnt: 0, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842299.3285005
[0m23:38:19.334062 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.dim_product, now model.medallion_dbt_spark.dim_sales)
[0m23:38:19.334062 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: reusing connection model.medallion_dbt_spark.dim_product sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.dim_sales, idle: 0.0055615901947021484s, acqrelcnt: 0, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842299.3285005
[0m23:38:19.335028 [debug] [Thread-1 (]: Databricks adapter: On thread (23228, 20152): `hive_metastore`.`saleslt`.`dim_sales` using default compute resource.
[0m23:38:19.335028 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: _acquire sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.dim_sales, idle: 0.006528377532958984s, acqrelcnt: 1, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842299.3285005
[0m23:38:19.336027 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.dim_sales
[0m23:38:19.339073 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.dim_sales"
[0m23:38:19.340062 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_sales (compile): 23:38:19.336027 => 23:38:19.340062
[0m23:38:19.340994 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.dim_sales
[0m23:38:19.345062 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_dbt_spark.dim_sales"
[0m23:38:19.346066 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: get_thread_connection: sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.dim_sales, idle: 0.017565488815307617s, acqrelcnt: 1, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842299.3285005
[0m23:38:19.346987 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: idle check connection: sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.dim_sales, idle: 0.018486976623535156s, acqrelcnt: 1, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842299.3285005
[0m23:38:19.346987 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.dim_sales"
[0m23:38:19.347983 [debug] [Thread-1 (]: On model.medallion_dbt_spark.dim_sales: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_sales`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/dim_sales'
      
      
      as
      
with product_snapshot as (
    select 
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    from `hive_metastore`.`saleslt`.`product` 
),
salesorderdetail_snapshot as (
    select 
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    from `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` 
),
salesorderheader_snapshot as (
    select 
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    from `hive_metastore`.`saleslt`.`salesorderheader` 
),
transformed as (
    select
    product_snapshot.Name,
    product_snapshot.ProductNumber,
    product_snapshot.Color,
    product_snapshot.StandardCost,
    product_snapshot.ListPrice,
    product_snapshot.Size,
    product_snapshot.Weight,
    product_snapshot.SellStartDate,
    product_snapshot.SellEndDate,
    product_snapshot.DiscontinuedDate,
    product_snapshot.ThumbNailPhoto,
    product_snapshot.ThumbnailPhotoFileName,
    salesorderdetail_snapshot.SalesOrderID,
    salesorderdetail_snapshot.SalesOrderDetailID,
    salesorderdetail_snapshot.OrderQty,
    salesorderdetail_snapshot.ProductID,
    salesorderdetail_snapshot.UnitPrice,
    salesorderdetail_snapshot.UnitPriceDiscount,
    salesorderdetail_snapshot.LineTotal,
    salesorderheader_snapshot.RevisionNumber,
    salesorderheader_snapshot.OrderDate,
    salesorderheader_snapshot.DueDate,
    salesorderheader_snapshot.ShipDate,
    salesorderheader_snapshot.Status,
    salesorderheader_snapshot.OnlineOrderFlag,
    salesorderheader_snapshot.SalesOrderNumber,
    salesorderheader_snapshot.PurchaseOrderNumber,
    salesorderheader_snapshot.AccountNumber,
    salesorderheader_snapshot.CustomerID,
    salesorderheader_snapshot.BillToAddressID,
    salesorderheader_snapshot.ShipToAddressID,
    salesorderheader_snapshot.ShipMethod,
    salesorderheader_snapshot.CreditCardApprovalCode,
    salesorderheader_snapshot.SubTotal,
    salesorderheader_snapshot.TaxAmt,
    salesorderheader_snapshot.Freight,
    salesorderheader_snapshot.TotalDue,
    salesorderheader_snapshot.Comment
    from product_snapshot
    left join salesorderdetail_snapshot on product_snapshot.ProductID = salesorderdetail_snapshot.ProductID
    left join salesorderheader_snapshot on salesorderdetail_snapshot.SalesOrderID = salesorderheader_snapshot.SalesOrderID
)

select * from transformed
  
[0m23:38:23.858510 [debug] [Thread-1 (]: SQL status: OK in 4.510000228881836 seconds
[0m23:38:23.860500 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_sales (execute): 23:38:19.340994 => 23:38:23.860500
[0m23:38:23.861501 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: _release sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.dim_sales, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842303.861502
[0m23:38:23.862430 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: _release sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.dim_sales, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842303.86243
[0m23:38:23.862430 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '316bab98-b196-4780-a0d2-cfe3a572c51d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014311680A50>]}
[0m23:38:23.863420 [info ] [Thread-1 (]: 3 of 5 OK created sql table model saleslt.dim_sales ............................ [[32mOK[0m in 4.53s]
[0m23:38:23.864379 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.dim_sales
[0m23:38:23.865465 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.my_first_dbt_model
[0m23:38:23.866386 [info ] [Thread-1 (]: 4 of 5 START sql table model saleslt.my_first_dbt_model ........................ [RUN]
[0m23:38:23.867387 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: idle check connection: sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.dim_sales, idle: 0.0049571990966796875s, acqrelcnt: 0, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842303.86243
[0m23:38:23.867387 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.dim_sales, now model.medallion_dbt_spark.my_first_dbt_model)
[0m23:38:23.868387 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: reusing connection model.medallion_dbt_spark.dim_sales sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.005957603454589844s, acqrelcnt: 0, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842303.86243
[0m23:38:23.868387 [debug] [Thread-1 (]: Databricks adapter: On thread (23228, 20152): `hive_metastore`.`saleslt`.`my_first_dbt_model` using default compute resource.
[0m23:38:23.869385 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: _acquire sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.00695490837097168s, acqrelcnt: 1, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842303.86243
[0m23:38:23.869385 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.my_first_dbt_model
[0m23:38:23.872465 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.my_first_dbt_model"
[0m23:38:23.873386 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.my_first_dbt_model (compile): 23:38:23.869385 => 23:38:23.872465
[0m23:38:23.873386 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.my_first_dbt_model
[0m23:38:23.879683 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: get_thread_connection: sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.017253637313842773s, acqrelcnt: 1, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842303.86243
[0m23:38:23.879683 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: idle check connection: sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.017253637313842773s, acqrelcnt: 1, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842303.86243
[0m23:38:23.880605 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.my_first_dbt_model"
[0m23:38:23.880605 [debug] [Thread-1 (]: On model.medallion_dbt_spark.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.my_first_dbt_model"} */

      describe extended `hive_metastore`.`saleslt`.`my_first_dbt_model`
  
[0m23:38:24.164862 [debug] [Thread-1 (]: SQL status: OK in 0.2800000011920929 seconds
[0m23:38:24.170860 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_dbt_spark.my_first_dbt_model"
[0m23:38:24.171860 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: get_thread_connection: sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.3094305992126465s, acqrelcnt: 1, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842303.86243
[0m23:38:24.172860 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: idle check connection: sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.3094305992126465s, acqrelcnt: 1, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842303.86243
[0m23:38:24.172860 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.my_first_dbt_model"
[0m23:38:24.172860 [debug] [Thread-1 (]: On model.medallion_dbt_spark.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.my_first_dbt_model"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`my_first_dbt_model`
      
      
    using delta
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m23:38:26.396514 [debug] [Thread-1 (]: SQL status: OK in 2.2200000286102295 seconds
[0m23:38:26.405993 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.my_first_dbt_model (execute): 23:38:23.873386 => 23:38:26.405993
[0m23:38:26.407005 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: _release sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842306.4069164
[0m23:38:26.407310 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: _release sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842306.40731
[0m23:38:26.408335 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '316bab98-b196-4780-a0d2-cfe3a572c51d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014311A26190>]}
[0m23:38:26.408335 [info ] [Thread-1 (]: 4 of 5 OK created sql table model saleslt.my_first_dbt_model ................... [[32mOK[0m in 2.54s]
[0m23:38:26.410312 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.my_first_dbt_model
[0m23:38:26.411311 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.my_second_dbt_model
[0m23:38:26.411311 [info ] [Thread-1 (]: 5 of 5 START sql view model saleslt.my_second_dbt_model ........................ [RUN]
[0m23:38:26.413309 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: idle check connection: sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.005999088287353516s, acqrelcnt: 0, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842306.40731
[0m23:38:26.413309 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.my_first_dbt_model, now model.medallion_dbt_spark.my_second_dbt_model)
[0m23:38:26.413309 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: reusing connection model.medallion_dbt_spark.my_first_dbt_model sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.005999088287353516s, acqrelcnt: 0, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842306.40731
[0m23:38:26.413309 [debug] [Thread-1 (]: Databricks adapter: On thread (23228, 20152): `hive_metastore`.`saleslt`.`my_second_dbt_model` using default compute resource.
[0m23:38:26.413309 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: _acquire sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.005999088287353516s, acqrelcnt: 1, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842306.40731
[0m23:38:26.413309 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.my_second_dbt_model
[0m23:38:26.419085 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.my_second_dbt_model"
[0m23:38:26.420103 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.my_second_dbt_model (compile): 23:38:26.413309 => 23:38:26.419085
[0m23:38:26.420103 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.my_second_dbt_model
[0m23:38:26.441167 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_dbt_spark.my_second_dbt_model"
[0m23:38:26.442165 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: get_thread_connection: sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.03485584259033203s, acqrelcnt: 1, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842306.40731
[0m23:38:26.443169 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: idle check connection: sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.03585934638977051s, acqrelcnt: 1, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842306.40731
[0m23:38:26.443169 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.my_second_dbt_model"
[0m23:38:26.444167 [debug] [Thread-1 (]: On model.medallion_dbt_spark.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.my_second_dbt_model"} */
create or replace view `hive_metastore`.`saleslt`.`my_second_dbt_model`
  
  
  
  as
    -- Use the `ref` function to select from other models

select *
from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id = 1

[0m23:38:26.910387 [debug] [Thread-1 (]: SQL status: OK in 0.4699999988079071 seconds
[0m23:38:26.912386 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.my_second_dbt_model (execute): 23:38:26.420103 => 23:38:26.912386
[0m23:38:26.913425 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: _release sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842306.9123867
[0m23:38:26.913425 [debug] [Thread-1 (]: Databricks adapter: conn: 1387568471056: _release sess: 47ae102a-3e04-456e-8190-be6f7c6502f9, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23228, 20152), cmpt: ``, lut: 1707842306.913425
[0m23:38:26.914385 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '316bab98-b196-4780-a0d2-cfe3a572c51d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000143116CC310>]}
[0m23:38:26.915307 [info ] [Thread-1 (]: 5 of 5 OK created sql view model saleslt.my_second_dbt_model ................... [[32mOK[0m in 0.50s]
[0m23:38:26.916315 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.my_second_dbt_model
[0m23:38:26.918302 [debug] [MainThread]: Databricks adapter: conn: 1387568409936: idle check connection: sess: None, name: master, idle: 14.193897724151611s, acqrelcnt: 0, lang: None, thrd: (23228, 25200), cmpt: ``, lut: 1707842292.7244053
[0m23:38:26.919380 [debug] [MainThread]: Databricks adapter: conn: 1387568409936: reusing connection master sess: None, name: master, idle: 14.194974660873413s, acqrelcnt: 0, lang: None, thrd: (23228, 25200), cmpt: ``, lut: 1707842292.7244053
[0m23:38:26.919380 [debug] [MainThread]: Databricks adapter: Thread (23228, 25200) using default compute resource.
[0m23:38:26.920306 [debug] [MainThread]: Databricks adapter: conn: 1387568409936: _acquire sess: None, name: master, idle: 14.195900917053223s, acqrelcnt: 1, lang: None, thrd: (23228, 25200), cmpt: ``, lut: 1707842292.7244053
[0m23:38:26.920306 [debug] [MainThread]: Databricks adapter: conn: 1387568409936: get_thread_connection: sess: None, name: master, idle: 14.195900917053223s, acqrelcnt: 1, lang: None, thrd: (23228, 25200), cmpt: ``, lut: 1707842292.7244053
[0m23:38:26.921334 [debug] [MainThread]: Databricks adapter: conn: 1387568409936: idle check connection: sess: None, name: master, idle: 14.196929216384888s, acqrelcnt: 1, lang: None, thrd: (23228, 25200), cmpt: ``, lut: 1707842292.7244053
[0m23:38:26.921334 [debug] [MainThread]: On master: ROLLBACK
[0m23:38:26.922308 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:38:27.204800 [debug] [MainThread]: Databricks adapter: conn: 1387568409936: session opened sess: acfb35ef-309b-491f-bc0f-2d16e982611a, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23228, 25200), cmpt: ``, lut: 1707842307.204801
[0m23:38:27.205802 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:38:27.206706 [debug] [MainThread]: Databricks adapter: conn: 1387568409936: get_thread_connection: sess: acfb35ef-309b-491f-bc0f-2d16e982611a, name: master, idle: 0.0010008811950683594s, acqrelcnt: 1, lang: None, thrd: (23228, 25200), cmpt: ``, lut: 1707842307.204801
[0m23:38:27.206836 [debug] [MainThread]: Databricks adapter: conn: 1387568409936: idle check connection: sess: acfb35ef-309b-491f-bc0f-2d16e982611a, name: master, idle: 0.0020351409912109375s, acqrelcnt: 1, lang: None, thrd: (23228, 25200), cmpt: ``, lut: 1707842307.204801
[0m23:38:27.206836 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:38:27.207882 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:38:27.207882 [debug] [MainThread]: Databricks adapter: conn: 1387568409936: _release sess: acfb35ef-309b-491f-bc0f-2d16e982611a, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (23228, 25200), cmpt: ``, lut: 1707842307.2078824
[0m23:38:27.208842 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:38:27.208842 [debug] [MainThread]: On master: ROLLBACK
[0m23:38:27.209839 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:38:27.209839 [debug] [MainThread]: On master: Close
[0m23:38:27.301684 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m23:38:27.302664 [debug] [MainThread]: On list_hive_metastore: Close
[0m23:38:27.383585 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m23:38:27.383585 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m23:38:27.384585 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:38:27.384585 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m23:38:27.469795 [debug] [MainThread]: Connection 'model.medallion_dbt_spark.my_second_dbt_model' was properly closed.
[0m23:38:27.470822 [debug] [MainThread]: On model.medallion_dbt_spark.my_second_dbt_model: ROLLBACK
[0m23:38:27.470822 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:38:27.471730 [debug] [MainThread]: On model.medallion_dbt_spark.my_second_dbt_model: Close
[0m23:38:27.551300 [info ] [MainThread]: 
[0m23:38:27.552301 [info ] [MainThread]: Finished running 4 table models, 1 view model in 0 hours 0 minutes and 17.25 seconds (17.25s).
[0m23:38:27.554294 [debug] [MainThread]: Command end result
[0m23:38:27.564506 [info ] [MainThread]: 
[0m23:38:27.565508 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:38:27.566509 [info ] [MainThread]: 
[0m23:38:27.567505 [error] [MainThread]:   Runtime Error in model dim_customer (models\marts\customer\dim_customer.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `customer_snapshot`.`AddressID` cannot be resolved. Did you mean one of the following? [`customeraddress_snapshot`.`AddressID`, `address_snapshot`.`AddressID`, `customer_snapshot`.`FullName`, `customer_snapshot`.`CustomerId`, `customeraddress_snapshot`.`AddressType`].; line 49 pos 4
[0m23:38:27.571532 [info ] [MainThread]: 
[0m23:38:27.572506 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=1 SKIP=0 TOTAL=5
[0m23:38:27.574693 [debug] [MainThread]: Command `dbt run` failed at 23:38:27.574693 after 19.12 seconds
[0m23:38:27.575703 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001437EBB28D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001437EEA8810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001437EEA8F90>]}
[0m23:38:27.575703 [debug] [MainThread]: Flushing usage events
[0m23:41:53.139507 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FF024FE10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FF0262C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FEC7F0E90>]}


============================== 23:41:53.144506 | ad35a188-e122-4184-8cd4-9e7224b78332 ==============================
[0m23:41:53.144506 [info ] [MainThread]: Running with dbt=1.7.7
[0m23:41:53.145507 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m23:41:54.574367 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ad35a188-e122-4184-8cd4-9e7224b78332', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F82443310>]}
[0m23:41:54.649416 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ad35a188-e122-4184-8cd4-9e7224b78332', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F8253CFD0>]}
[0m23:41:54.650502 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m23:41:54.660087 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m23:41:54.746206 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:41:54.747207 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://models\marts\customer\dim_customer.sql
[0m23:41:54.971616 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ad35a188-e122-4184-8cd4-9e7224b78332', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F8241FBD0>]}
[0m23:41:54.996185 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ad35a188-e122-4184-8cd4-9e7224b78332', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F8282DD50>]}
[0m23:41:54.997185 [info ] [MainThread]: Found 5 models, 7 snapshots, 8 tests, 9 sources, 0 exposures, 0 metrics, 535 macros, 0 groups, 0 semantic models
[0m23:41:55.000192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ad35a188-e122-4184-8cd4-9e7224b78332', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F82563C90>]}
[0m23:41:55.003194 [info ] [MainThread]: 
[0m23:41:55.005189 [debug] [MainThread]: Databricks adapter: conn: 2471792156240: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (13508, 5840), cmpt: ``, lut: None
[0m23:41:55.005189 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:41:55.006852 [debug] [MainThread]: Databricks adapter: Thread (13508, 5840) using default compute resource.
[0m23:41:55.006852 [debug] [MainThread]: Databricks adapter: conn: 2471792156240: _acquire sess: None, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (13508, 5840), cmpt: ``, lut: 1707842515.0068526
[0m23:41:55.008853 [debug] [ThreadPool]: Databricks adapter: conn: 2471796350928: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (13508, 16040), cmpt: ``, lut: None
[0m23:41:55.009852 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m23:41:55.009852 [debug] [ThreadPool]: Databricks adapter: Thread (13508, 16040) using default compute resource.
[0m23:41:55.010852 [debug] [ThreadPool]: Databricks adapter: conn: 2471796350928: _acquire sess: None, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (13508, 16040), cmpt: ``, lut: 1707842515.010852
[0m23:41:55.011851 [debug] [ThreadPool]: Databricks adapter: conn: 2471796350928: get_thread_connection: sess: None, name: list_hive_metastore, idle: 0.0009992122650146484s, acqrelcnt: 1, lang: None, thrd: (13508, 16040), cmpt: ``, lut: 1707842515.010852
[0m23:41:55.011851 [debug] [ThreadPool]: Databricks adapter: conn: 2471796350928: idle check connection: sess: None, name: list_hive_metastore, idle: 0.0009992122650146484s, acqrelcnt: 1, lang: None, thrd: (13508, 16040), cmpt: ``, lut: 1707842515.010852
[0m23:41:55.012850 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m23:41:55.013850 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m23:41:55.013850 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:41:55.484847 [debug] [ThreadPool]: Databricks adapter: conn: 2471796350928: session opened sess: 4b041cd5-3cb4-4322-a795-6bf587258dea, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (13508, 16040), cmpt: ``, lut: 1707842515.4838393
[0m23:41:55.820574 [debug] [ThreadPool]: SQL status: OK in 0.8100000023841858 seconds
[0m23:41:55.827590 [debug] [ThreadPool]: Databricks adapter: conn: 2471796350928: _release sess: 4b041cd5-3cb4-4322-a795-6bf587258dea, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (13508, 16040), cmpt: ``, lut: 1707842515.8263588
[0m23:41:55.832020 [debug] [ThreadPool]: Databricks adapter: conn: 2471794034256: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_snapshots, idle: 0s, acqrelcnt: 0, lang: None, thrd: (13508, 15532), cmpt: ``, lut: None
[0m23:41:55.833482 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_snapshots'
[0m23:41:55.834491 [debug] [ThreadPool]: Databricks adapter: Thread (13508, 15532) using default compute resource.
[0m23:41:55.835987 [debug] [ThreadPool]: Databricks adapter: conn: 2471794034256: _acquire sess: None, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (13508, 15532), cmpt: ``, lut: 1707842515.8359878
[0m23:41:55.849815 [debug] [ThreadPool]: Databricks adapter: conn: 2471794034256: get_thread_connection: sess: None, name: list_hive_metastore_snapshots, idle: 0.013390302658081055s, acqrelcnt: 1, lang: None, thrd: (13508, 15532), cmpt: ``, lut: 1707842515.8359878
[0m23:41:55.851569 [debug] [ThreadPool]: Databricks adapter: conn: 2471794034256: idle check connection: sess: None, name: list_hive_metastore_snapshots, idle: 0.015581607818603516s, acqrelcnt: 1, lang: None, thrd: (13508, 15532), cmpt: ``, lut: 1707842515.8359878
[0m23:41:55.852577 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m23:41:55.853722 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m23:41:55.854749 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:41:56.122334 [debug] [ThreadPool]: Databricks adapter: conn: 2471794034256: session opened sess: a655d808-4a8e-4921-ae89-dca39ec5ad35, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (13508, 15532), cmpt: ``, lut: 1707842516.1223345
[0m23:41:56.306178 [debug] [ThreadPool]: SQL status: OK in 0.44999998807907104 seconds
[0m23:41:56.320748 [debug] [ThreadPool]: Databricks adapter: conn: 2471794034256: get_thread_connection: sess: a655d808-4a8e-4921-ae89-dca39ec5ad35, name: list_hive_metastore_snapshots, idle: 0.19841432571411133s, acqrelcnt: 1, lang: None, thrd: (13508, 15532), cmpt: ``, lut: 1707842516.1223345
[0m23:41:56.321714 [debug] [ThreadPool]: Databricks adapter: conn: 2471794034256: idle check connection: sess: a655d808-4a8e-4921-ae89-dca39ec5ad35, name: list_hive_metastore_snapshots, idle: 0.19938015937805176s, acqrelcnt: 1, lang: None, thrd: (13508, 15532), cmpt: ``, lut: 1707842516.1223345
[0m23:41:56.322730 [debug] [ThreadPool]: Databricks adapter: conn: 2471794034256: get_thread_connection: sess: a655d808-4a8e-4921-ae89-dca39ec5ad35, name: list_hive_metastore_snapshots, idle: 0.2003955841064453s, acqrelcnt: 1, lang: None, thrd: (13508, 15532), cmpt: ``, lut: 1707842516.1223345
[0m23:41:56.322730 [debug] [ThreadPool]: Databricks adapter: conn: 2471794034256: idle check connection: sess: a655d808-4a8e-4921-ae89-dca39ec5ad35, name: list_hive_metastore_snapshots, idle: 0.2003955841064453s, acqrelcnt: 1, lang: None, thrd: (13508, 15532), cmpt: ``, lut: 1707842516.1223345
[0m23:41:56.323699 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:41:56.323931 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m23:41:56.323931 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m23:41:56.490850 [debug] [ThreadPool]: SQL status: OK in 0.17000000178813934 seconds
[0m23:41:56.501429 [debug] [ThreadPool]: Databricks adapter: conn: 2471794034256: get_thread_connection: sess: a655d808-4a8e-4921-ae89-dca39ec5ad35, name: list_hive_metastore_snapshots, idle: 0.37909483909606934s, acqrelcnt: 1, lang: None, thrd: (13508, 15532), cmpt: ``, lut: 1707842516.1223345
[0m23:41:56.502430 [debug] [ThreadPool]: Databricks adapter: conn: 2471794034256: idle check connection: sess: a655d808-4a8e-4921-ae89-dca39ec5ad35, name: list_hive_metastore_snapshots, idle: 0.3800957202911377s, acqrelcnt: 1, lang: None, thrd: (13508, 15532), cmpt: ``, lut: 1707842516.1223345
[0m23:41:56.503386 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m23:41:56.504386 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m23:41:56.657190 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m23:41:56.660007 [debug] [ThreadPool]: Databricks adapter: conn: 2471794034256: _release sess: a655d808-4a8e-4921-ae89-dca39ec5ad35, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (13508, 15532), cmpt: ``, lut: 1707842516.6600077
[0m23:41:56.661933 [debug] [ThreadPool]: Databricks adapter: conn: 2471794034256: idle check connection: sess: a655d808-4a8e-4921-ae89-dca39ec5ad35, name: list_hive_metastore_snapshots, idle: 0.0009617805480957031s, acqrelcnt: 0, lang: None, thrd: (13508, 15532), cmpt: ``, lut: 1707842516.6600077
[0m23:41:56.663885 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now list_hive_metastore_saleslt)
[0m23:41:56.664886 [debug] [ThreadPool]: Databricks adapter: conn: 2471794034256: reusing connection list_hive_metastore_snapshots sess: a655d808-4a8e-4921-ae89-dca39ec5ad35, name: list_hive_metastore_saleslt, idle: 0.003877878189086914s, acqrelcnt: 0, lang: None, thrd: (13508, 15532), cmpt: ``, lut: 1707842516.6600077
[0m23:41:56.664886 [debug] [ThreadPool]: Databricks adapter: Thread (13508, 15532) using default compute resource.
[0m23:41:56.665883 [debug] [ThreadPool]: Databricks adapter: conn: 2471794034256: _acquire sess: a655d808-4a8e-4921-ae89-dca39ec5ad35, name: list_hive_metastore_saleslt, idle: 0.005876302719116211s, acqrelcnt: 1, lang: None, thrd: (13508, 15532), cmpt: ``, lut: 1707842516.6600077
[0m23:41:56.669883 [debug] [ThreadPool]: Databricks adapter: conn: 2471794034256: get_thread_connection: sess: a655d808-4a8e-4921-ae89-dca39ec5ad35, name: list_hive_metastore_saleslt, idle: 0.008874177932739258s, acqrelcnt: 1, lang: None, thrd: (13508, 15532), cmpt: ``, lut: 1707842516.6600077
[0m23:41:56.671229 [debug] [ThreadPool]: Databricks adapter: conn: 2471794034256: idle check connection: sess: a655d808-4a8e-4921-ae89-dca39ec5ad35, name: list_hive_metastore_saleslt, idle: 0.009875774383544922s, acqrelcnt: 1, lang: None, thrd: (13508, 15532), cmpt: ``, lut: 1707842516.6600077
[0m23:41:56.672392 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:41:56.673401 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m23:41:56.774382 [debug] [ThreadPool]: SQL status: OK in 0.10000000149011612 seconds
[0m23:41:56.780900 [debug] [ThreadPool]: Databricks adapter: conn: 2471794034256: get_thread_connection: sess: a655d808-4a8e-4921-ae89-dca39ec5ad35, name: list_hive_metastore_saleslt, idle: 0.12089323997497559s, acqrelcnt: 1, lang: None, thrd: (13508, 15532), cmpt: ``, lut: 1707842516.6600077
[0m23:41:56.782103 [debug] [ThreadPool]: Databricks adapter: conn: 2471794034256: idle check connection: sess: a655d808-4a8e-4921-ae89-dca39ec5ad35, name: list_hive_metastore_saleslt, idle: 0.12189388275146484s, acqrelcnt: 1, lang: None, thrd: (13508, 15532), cmpt: ``, lut: 1707842516.6600077
[0m23:41:56.782103 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:41:56.782103 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m23:41:56.928919 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m23:41:56.934426 [debug] [ThreadPool]: Databricks adapter: conn: 2471794034256: get_thread_connection: sess: a655d808-4a8e-4921-ae89-dca39ec5ad35, name: list_hive_metastore_saleslt, idle: 0.27341771125793457s, acqrelcnt: 1, lang: None, thrd: (13508, 15532), cmpt: ``, lut: 1707842516.6600077
[0m23:41:56.934426 [debug] [ThreadPool]: Databricks adapter: conn: 2471794034256: idle check connection: sess: a655d808-4a8e-4921-ae89-dca39ec5ad35, name: list_hive_metastore_saleslt, idle: 0.27441883087158203s, acqrelcnt: 1, lang: None, thrd: (13508, 15532), cmpt: ``, lut: 1707842516.6600077
[0m23:41:56.935426 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:41:56.935426 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m23:41:57.106085 [debug] [ThreadPool]: SQL status: OK in 0.17000000178813934 seconds
[0m23:41:57.110398 [debug] [ThreadPool]: Databricks adapter: conn: 2471794034256: _release sess: a655d808-4a8e-4921-ae89-dca39ec5ad35, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (13508, 15532), cmpt: ``, lut: 1707842517.1098905
[0m23:41:57.113406 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ad35a188-e122-4184-8cd4-9e7224b78332', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F8250A050>]}
[0m23:41:57.114407 [debug] [MainThread]: Databricks adapter: conn: 2471792156240: get_thread_connection: sess: None, name: master, idle: 2.1075549125671387s, acqrelcnt: 1, lang: None, thrd: (13508, 5840), cmpt: ``, lut: 1707842515.0068526
[0m23:41:57.114407 [debug] [MainThread]: Databricks adapter: conn: 2471792156240: idle check connection: sess: None, name: master, idle: 2.1075549125671387s, acqrelcnt: 1, lang: None, thrd: (13508, 5840), cmpt: ``, lut: 1707842515.0068526
[0m23:41:57.115406 [debug] [MainThread]: Databricks adapter: conn: 2471792156240: get_thread_connection: sess: None, name: master, idle: 2.108553886413574s, acqrelcnt: 1, lang: None, thrd: (13508, 5840), cmpt: ``, lut: 1707842515.0068526
[0m23:41:57.115406 [debug] [MainThread]: Databricks adapter: conn: 2471792156240: idle check connection: sess: None, name: master, idle: 2.108553886413574s, acqrelcnt: 1, lang: None, thrd: (13508, 5840), cmpt: ``, lut: 1707842515.0068526
[0m23:41:57.116406 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:41:57.116406 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:41:57.116406 [debug] [MainThread]: Databricks adapter: conn: 2471792156240: _release sess: None, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (13508, 5840), cmpt: ``, lut: 1707842517.116406
[0m23:41:57.117407 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:41:57.118734 [info ] [MainThread]: 
[0m23:41:57.123755 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.dim_customer
[0m23:41:57.124997 [info ] [Thread-1 (]: 1 of 5 START sql table model saleslt.dim_customer .............................. [RUN]
[0m23:41:57.127035 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: Creating DatabricksDBTConnection sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0s, acqrelcnt: 0, lang: None, thrd: (13508, 12312), cmpt: ``, lut: None
[0m23:41:57.127035 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.medallion_dbt_spark.dim_customer'
[0m23:41:57.128000 [debug] [Thread-1 (]: Databricks adapter: On thread (13508, 12312): `hive_metastore`.`saleslt`.`dim_customer` using default compute resource.
[0m23:41:57.128000 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: _acquire sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842517.1280007
[0m23:41:57.129000 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.dim_customer
[0m23:41:57.139608 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.dim_customer"
[0m23:41:57.141123 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_customer (compile): 23:41:57.129000 => 23:41:57.141123
[0m23:41:57.142131 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.dim_customer
[0m23:41:57.221775 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_dbt_spark.dim_customer"
[0m23:41:57.223975 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: get_thread_connection: sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0.09478116035461426s, acqrelcnt: 1, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842517.1280007
[0m23:41:57.223975 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: idle check connection: sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0.09597468376159668s, acqrelcnt: 1, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842517.1280007
[0m23:41:57.224981 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: get_thread_connection: sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0.09698057174682617s, acqrelcnt: 1, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842517.1280007
[0m23:41:57.224981 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: idle check connection: sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0.09698057174682617s, acqrelcnt: 1, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842517.1280007
[0m23:41:57.225980 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m23:41:57.225980 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.dim_customer"
[0m23:41:57.226979 [debug] [Thread-1 (]: On model.medallion_dbt_spark.dim_customer: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_customer`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/dim_customer'
      
      
      as
      

with address_snapshot as (
    select 
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
),
customeraddress_snapshot as (
    select 
        CustomerID,
        AddressID,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
),
customer_snapshot as (
    select 
        CustomerID,
        concat(ifnull(FirstName, ''), ' ', ifnull(MiddleName, ''), ' ', ifnull(LastName, '')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
),
transformed as (
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incrementing surrogate key
    customer_snapshot.CustomerID,
    customer_snapshot.FullName,
    customer_snapshot.AddressID,
    customer_snapshot.AddressType,
    customer_snapshot.AddressLine1,
    customer_snapshot.City,
    customer_snapshot.StateProvince,
    customer_snapshot.CountryRegion,
    customer_snapshot.PostalCode
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerID = customeraddress_snapshot.CustomerID
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)
select * from transformed
  
[0m23:41:57.227979 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:41:57.477362 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: session opened sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.dim_customer, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842517.4763532
[0m23:41:57.784294 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_customer`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/dim_customer'
      
      
      as
      

with address_snapshot as (
    select 
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
),
customeraddress_snapshot as (
    select 
        CustomerID,
        AddressID,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
),
customer_snapshot as (
    select 
        CustomerID,
        concat(ifnull(FirstName, ''), ' ', ifnull(MiddleName, ''), ' ', ifnull(LastName, '')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
),
transformed as (
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incrementing surrogate key
    customer_snapshot.CustomerID,
    customer_snapshot.FullName,
    customer_snapshot.AddressID,
    customer_snapshot.AddressType,
    customer_snapshot.AddressLine1,
    customer_snapshot.City,
    customer_snapshot.StateProvince,
    customer_snapshot.CountryRegion,
    customer_snapshot.PostalCode
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerID = customeraddress_snapshot.CustomerID
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)
select * from transformed
  
[0m23:41:57.786346 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `customer_snapshot`.`AddressID` cannot be resolved. Did you mean one of the following? [`customeraddress_snapshot`.`AddressID`, `address_snapshot`.`AddressID`, `customer_snapshot`.`CustomerID`, `customer_snapshot`.`FullName`, `customeraddress_snapshot`.`AddressType`].; line 49 pos 4
[0m23:41:57.786346 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `customer_snapshot`.`AddressID` cannot be resolved. Did you mean one of the following? [`customeraddress_snapshot`.`AddressID`, `address_snapshot`.`AddressID`, `customer_snapshot`.`CustomerID`, `customer_snapshot`.`FullName`, `customeraddress_snapshot`.`AddressType`].; line 49 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `customer_snapshot`.`AddressID` cannot be resolved. Did you mean one of the following? [`customeraddress_snapshot`.`AddressID`, `address_snapshot`.`AddressID`, `customer_snapshot`.`CustomerID`, `customer_snapshot`.`FullName`, `customeraddress_snapshot`.`AddressType`].; line 49 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:671)
	... 35 more

[0m23:41:57.786346 [debug] [Thread-1 (]: Databricks adapter: operation-id: 858ffeec-7ad5-4152-acff-3e14d5e8d813
[0m23:41:57.786346 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_customer (execute): 23:41:57.143131 => 23:41:57.786346
[0m23:41:57.790465 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: _release sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.dim_customer, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842517.7904658
[0m23:41:57.800973 [debug] [Thread-1 (]: Runtime Error in model dim_customer (models\marts\customer\dim_customer.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `customer_snapshot`.`AddressID` cannot be resolved. Did you mean one of the following? [`customeraddress_snapshot`.`AddressID`, `address_snapshot`.`AddressID`, `customer_snapshot`.`CustomerID`, `customer_snapshot`.`FullName`, `customeraddress_snapshot`.`AddressType`].; line 49 pos 4
[0m23:41:57.803044 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: _release sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.dim_customer, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842517.8026173
[0m23:41:57.803558 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ad35a188-e122-4184-8cd4-9e7224b78332', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F82A51790>]}
[0m23:41:57.803558 [error] [Thread-1 (]: 1 of 5 ERROR creating sql table model saleslt.dim_customer ..................... [[31mERROR[0m in 0.68s]
[0m23:41:57.803558 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.dim_customer
[0m23:41:57.803558 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.dim_product
[0m23:41:57.808066 [info ] [Thread-1 (]: 2 of 5 START sql table model saleslt.dim_product ............................... [RUN]
[0m23:41:57.809600 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: idle check connection: sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.dim_customer, idle: 0.0069828033447265625s, acqrelcnt: 0, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842517.8026173
[0m23:41:57.809600 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.dim_customer, now model.medallion_dbt_spark.dim_product)
[0m23:41:57.809600 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: reusing connection model.medallion_dbt_spark.dim_customer sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.dim_product, idle: 0.0069828033447265625s, acqrelcnt: 0, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842517.8026173
[0m23:41:57.809600 [debug] [Thread-1 (]: Databricks adapter: On thread (13508, 12312): `hive_metastore`.`saleslt`.`dim_product` using default compute resource.
[0m23:41:57.812528 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: _acquire sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.dim_product, idle: 0.009910821914672852s, acqrelcnt: 1, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842517.8026173
[0m23:41:57.813540 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.dim_product
[0m23:41:57.819678 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.dim_product"
[0m23:41:57.824898 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_product (compile): 23:41:57.813540 => 23:41:57.823899
[0m23:41:57.825898 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.dim_product
[0m23:41:57.839589 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: get_thread_connection: sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.dim_product, idle: 0.0369718074798584s, acqrelcnt: 1, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842517.8026173
[0m23:41:57.842022 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: idle check connection: sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.dim_product, idle: 0.03839302062988281s, acqrelcnt: 1, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842517.8026173
[0m23:41:57.842022 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.dim_product"
[0m23:41:57.843258 [debug] [Thread-1 (]: On model.medallion_dbt_spark.dim_product: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_product"} */

      describe extended `hive_metastore`.`saleslt`.`dim_product`
  
[0m23:41:58.335351 [debug] [Thread-1 (]: SQL status: OK in 0.49000000953674316 seconds
[0m23:41:58.349888 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_dbt_spark.dim_product"
[0m23:41:58.351898 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: get_thread_connection: sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.dim_product, idle: 0.5482807159423828s, acqrelcnt: 1, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842517.8026173
[0m23:41:58.352824 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: idle check connection: sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.dim_product, idle: 0.5502076148986816s, acqrelcnt: 1, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842517.8026173
[0m23:41:58.353280 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.dim_product"
[0m23:41:58.354861 [debug] [Thread-1 (]: On model.medallion_dbt_spark.dim_product: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/dim_product'
      
      
      as
      
with product_snapshot as (
    select 
        ProductId,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate
    from `hive_metastore`.`snapshots`.`product_snapshot` where dbt_valid_to is null
),
productmodel_snapshot as (
    select 
        ProductModelID,
        Name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot` where dbt_valid_to is null
),
transformed as (
    select
    row_number() over (order by product_snapshot.ProductId) as product_sk, -- auto-incrementing surrogate key
    product_snapshot.Name as product_name,
    product_snapshot.StandardCost,
    product_snapshot.ListPrice,
    product_snapshot.Size,
    product_snapshot.Weight,
    productmodel_snapshot.Name as model,
    productmodel_snapshot.CatalogDescription as description,
    product_snapshot.SellStartDate,
    product_snapshot.SellEndDate,
    product_snapshot.DiscontinuedDate
    from product_snapshot
    left join productmodel_snapshot on product_snapshot.ProductModelID = productmodel_snapshot.ProductModelID
)
select * from transformed
  
[0m23:42:02.167786 [debug] [Thread-1 (]: SQL status: OK in 3.809999942779541 seconds
[0m23:42:02.208612 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_product (execute): 23:41:57.826907 => 23:42:02.208612
[0m23:42:02.208612 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: _release sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.dim_product, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842522.2086124
[0m23:42:02.211129 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: _release sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.dim_product, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842522.21113
[0m23:42:02.211129 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ad35a188-e122-4184-8cd4-9e7224b78332', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F828B0D10>]}
[0m23:42:02.212132 [info ] [Thread-1 (]: 2 of 5 OK created sql table model saleslt.dim_product .......................... [[32mOK[0m in 4.40s]
[0m23:42:02.213639 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.dim_product
[0m23:42:02.215646 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.dim_sales
[0m23:42:02.216737 [info ] [Thread-1 (]: 3 of 5 START sql table model saleslt.dim_sales ................................. [RUN]
[0m23:42:02.218654 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: idle check connection: sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.dim_product, idle: 0.007524251937866211s, acqrelcnt: 0, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842522.21113
[0m23:42:02.218654 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.dim_product, now model.medallion_dbt_spark.dim_sales)
[0m23:42:02.220255 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: reusing connection model.medallion_dbt_spark.dim_product sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.dim_sales, idle: 0.009125232696533203s, acqrelcnt: 0, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842522.21113
[0m23:42:02.221167 [debug] [Thread-1 (]: Databricks adapter: On thread (13508, 12312): `hive_metastore`.`saleslt`.`dim_sales` using default compute resource.
[0m23:42:02.221167 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: _acquire sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.dim_sales, idle: 0.01003718376159668s, acqrelcnt: 1, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842522.21113
[0m23:42:02.222173 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.dim_sales
[0m23:42:02.234425 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.dim_sales"
[0m23:42:02.236422 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_sales (compile): 23:42:02.222173 => 23:42:02.235434
[0m23:42:02.237414 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.dim_sales
[0m23:42:02.331211 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: get_thread_connection: sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.dim_sales, idle: 0.11907625198364258s, acqrelcnt: 1, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842522.21113
[0m23:42:02.331211 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: idle check connection: sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.dim_sales, idle: 0.12008166313171387s, acqrelcnt: 1, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842522.21113
[0m23:42:02.332433 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.dim_sales"
[0m23:42:02.332700 [debug] [Thread-1 (]: On model.medallion_dbt_spark.dim_sales: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_sales"} */

      describe extended `hive_metastore`.`saleslt`.`dim_sales`
  
[0m23:42:02.668525 [debug] [Thread-1 (]: SQL status: OK in 0.3400000035762787 seconds
[0m23:42:02.678130 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_dbt_spark.dim_sales"
[0m23:42:02.680723 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: get_thread_connection: sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.dim_sales, idle: 0.46959352493286133s, acqrelcnt: 1, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842522.21113
[0m23:42:02.681718 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: idle check connection: sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.dim_sales, idle: 0.47058844566345215s, acqrelcnt: 1, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842522.21113
[0m23:42:02.681718 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.dim_sales"
[0m23:42:02.682716 [debug] [Thread-1 (]: On model.medallion_dbt_spark.dim_sales: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_sales`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/dim_sales'
      
      
      as
      
with product_snapshot as (
    select 
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    from `hive_metastore`.`saleslt`.`product` 
),
salesorderdetail_snapshot as (
    select 
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    from `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` 
),
salesorderheader_snapshot as (
    select 
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    from `hive_metastore`.`saleslt`.`salesorderheader` 
),
transformed as (
    select
    product_snapshot.Name,
    product_snapshot.ProductNumber,
    product_snapshot.Color,
    product_snapshot.StandardCost,
    product_snapshot.ListPrice,
    product_snapshot.Size,
    product_snapshot.Weight,
    product_snapshot.SellStartDate,
    product_snapshot.SellEndDate,
    product_snapshot.DiscontinuedDate,
    product_snapshot.ThumbNailPhoto,
    product_snapshot.ThumbnailPhotoFileName,
    salesorderdetail_snapshot.SalesOrderID,
    salesorderdetail_snapshot.SalesOrderDetailID,
    salesorderdetail_snapshot.OrderQty,
    salesorderdetail_snapshot.ProductID,
    salesorderdetail_snapshot.UnitPrice,
    salesorderdetail_snapshot.UnitPriceDiscount,
    salesorderdetail_snapshot.LineTotal,
    salesorderheader_snapshot.RevisionNumber,
    salesorderheader_snapshot.OrderDate,
    salesorderheader_snapshot.DueDate,
    salesorderheader_snapshot.ShipDate,
    salesorderheader_snapshot.Status,
    salesorderheader_snapshot.OnlineOrderFlag,
    salesorderheader_snapshot.SalesOrderNumber,
    salesorderheader_snapshot.PurchaseOrderNumber,
    salesorderheader_snapshot.AccountNumber,
    salesorderheader_snapshot.CustomerID,
    salesorderheader_snapshot.BillToAddressID,
    salesorderheader_snapshot.ShipToAddressID,
    salesorderheader_snapshot.ShipMethod,
    salesorderheader_snapshot.CreditCardApprovalCode,
    salesorderheader_snapshot.SubTotal,
    salesorderheader_snapshot.TaxAmt,
    salesorderheader_snapshot.Freight,
    salesorderheader_snapshot.TotalDue,
    salesorderheader_snapshot.Comment
    from product_snapshot
    left join salesorderdetail_snapshot on product_snapshot.ProductID = salesorderdetail_snapshot.ProductID
    left join salesorderheader_snapshot on salesorderdetail_snapshot.SalesOrderID = salesorderheader_snapshot.SalesOrderID
)

select * from transformed
  
[0m23:42:06.573727 [debug] [Thread-1 (]: SQL status: OK in 3.890000104904175 seconds
[0m23:42:06.580033 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_sales (execute): 23:42:02.238357 => 23:42:06.580033
[0m23:42:06.581534 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: _release sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.dim_sales, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842526.5800335
[0m23:42:06.582656 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: _release sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.dim_sales, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842526.582534
[0m23:42:06.582771 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ad35a188-e122-4184-8cd4-9e7224b78332', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F82B0FB90>]}
[0m23:42:06.583776 [info ] [Thread-1 (]: 3 of 5 OK created sql table model saleslt.dim_sales ............................ [[32mOK[0m in 4.36s]
[0m23:42:06.584786 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.dim_sales
[0m23:42:06.585860 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.my_first_dbt_model
[0m23:42:06.587771 [info ] [Thread-1 (]: 4 of 5 START sql table model saleslt.my_first_dbt_model ........................ [RUN]
[0m23:42:06.589773 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: idle check connection: sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.dim_sales, idle: 0.007239341735839844s, acqrelcnt: 0, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842526.582534
[0m23:42:06.591489 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.dim_sales, now model.medallion_dbt_spark.my_first_dbt_model)
[0m23:42:06.593474 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: reusing connection model.medallion_dbt_spark.dim_sales sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.009964704513549805s, acqrelcnt: 0, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842526.582534
[0m23:42:06.594427 [debug] [Thread-1 (]: Databricks adapter: On thread (13508, 12312): `hive_metastore`.`saleslt`.`my_first_dbt_model` using default compute resource.
[0m23:42:06.596388 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: _acquire sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.012889385223388672s, acqrelcnt: 1, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842526.582534
[0m23:42:06.597475 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.my_first_dbt_model
[0m23:42:06.604080 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.my_first_dbt_model"
[0m23:42:06.606166 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.my_first_dbt_model (compile): 23:42:06.598479 => 23:42:06.605082
[0m23:42:06.607079 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.my_first_dbt_model
[0m23:42:06.614050 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: get_thread_connection: sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.030437469482421875s, acqrelcnt: 1, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842526.582534
[0m23:42:06.615078 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: idle check connection: sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.032544851303100586s, acqrelcnt: 1, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842526.582534
[0m23:42:06.616056 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.my_first_dbt_model"
[0m23:42:06.616056 [debug] [Thread-1 (]: On model.medallion_dbt_spark.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.my_first_dbt_model"} */

      describe extended `hive_metastore`.`saleslt`.`my_first_dbt_model`
  
[0m23:42:06.850039 [debug] [Thread-1 (]: SQL status: OK in 0.23000000417232513 seconds
[0m23:42:06.857356 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_dbt_spark.my_first_dbt_model"
[0m23:42:06.858357 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: get_thread_connection: sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.27582335472106934s, acqrelcnt: 1, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842526.582534
[0m23:42:06.858357 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: idle check connection: sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.27582335472106934s, acqrelcnt: 1, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842526.582534
[0m23:42:06.859870 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.my_first_dbt_model"
[0m23:42:06.859870 [debug] [Thread-1 (]: On model.medallion_dbt_spark.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.my_first_dbt_model"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`my_first_dbt_model`
      
      
    using delta
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m23:42:08.742305 [debug] [Thread-1 (]: SQL status: OK in 1.8799999952316284 seconds
[0m23:42:08.747303 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.my_first_dbt_model (execute): 23:42:06.608115 => 23:42:08.747303
[0m23:42:08.748304 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: _release sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842528.7483046
[0m23:42:08.750058 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: _release sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842528.7500584
[0m23:42:08.751061 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ad35a188-e122-4184-8cd4-9e7224b78332', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F824642D0>]}
[0m23:42:08.753146 [info ] [Thread-1 (]: 4 of 5 OK created sql table model saleslt.my_first_dbt_model ................... [[32mOK[0m in 2.16s]
[0m23:42:08.754126 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.my_first_dbt_model
[0m23:42:08.755057 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.my_second_dbt_model
[0m23:42:08.756146 [info ] [Thread-1 (]: 5 of 5 START sql view model saleslt.my_second_dbt_model ........................ [RUN]
[0m23:42:08.758572 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: idle check connection: sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.007506608963012695s, acqrelcnt: 0, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842528.7500584
[0m23:42:08.758572 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.my_first_dbt_model, now model.medallion_dbt_spark.my_second_dbt_model)
[0m23:42:08.758572 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: reusing connection model.medallion_dbt_spark.my_first_dbt_model sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.008513689041137695s, acqrelcnt: 0, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842528.7500584
[0m23:42:08.760203 [debug] [Thread-1 (]: Databricks adapter: On thread (13508, 12312): `hive_metastore`.`saleslt`.`my_second_dbt_model` using default compute resource.
[0m23:42:08.760203 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: _acquire sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.010144948959350586s, acqrelcnt: 1, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842528.7500584
[0m23:42:08.761092 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.my_second_dbt_model
[0m23:42:08.765091 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.my_second_dbt_model"
[0m23:42:08.766090 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.my_second_dbt_model (compile): 23:42:08.761092 => 23:42:08.766090
[0m23:42:08.767088 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.my_second_dbt_model
[0m23:42:08.792930 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_dbt_spark.my_second_dbt_model"
[0m23:42:08.794883 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: get_thread_connection: sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.04390525817871094s, acqrelcnt: 1, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842528.7500584
[0m23:42:08.795990 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: idle check connection: sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.04482531547546387s, acqrelcnt: 1, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842528.7500584
[0m23:42:08.795990 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.my_second_dbt_model"
[0m23:42:08.796979 [debug] [Thread-1 (]: On model.medallion_dbt_spark.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.my_second_dbt_model"} */
create or replace view `hive_metastore`.`saleslt`.`my_second_dbt_model`
  
  
  
  as
    -- Use the `ref` function to select from other models

select *
from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id = 1

[0m23:42:09.271832 [debug] [Thread-1 (]: SQL status: OK in 0.4699999988079071 seconds
[0m23:42:09.277230 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.my_second_dbt_model (execute): 23:42:08.767088 => 23:42:09.277230
[0m23:42:09.279140 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: _release sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842529.2782178
[0m23:42:09.281222 [debug] [Thread-1 (]: Databricks adapter: conn: 2471795991440: _release sess: bc8e23f6-2bf0-4c0d-8bc9-dc1d40516db7, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (13508, 12312), cmpt: ``, lut: 1707842529.2812228
[0m23:42:09.283339 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ad35a188-e122-4184-8cd4-9e7224b78332', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F82EA3390>]}
[0m23:42:09.284406 [info ] [Thread-1 (]: 5 of 5 OK created sql view model saleslt.my_second_dbt_model ................... [[32mOK[0m in 0.52s]
[0m23:42:09.287300 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.my_second_dbt_model
[0m23:42:09.291298 [debug] [MainThread]: Databricks adapter: conn: 2471792156240: idle check connection: sess: None, name: master, idle: 12.17489218711853s, acqrelcnt: 0, lang: None, thrd: (13508, 5840), cmpt: ``, lut: 1707842517.116406
[0m23:42:09.293297 [debug] [MainThread]: Databricks adapter: conn: 2471792156240: reusing connection master sess: None, name: master, idle: 12.175892353057861s, acqrelcnt: 0, lang: None, thrd: (13508, 5840), cmpt: ``, lut: 1707842517.116406
[0m23:42:09.294296 [debug] [MainThread]: Databricks adapter: Thread (13508, 5840) using default compute resource.
[0m23:42:09.296298 [debug] [MainThread]: Databricks adapter: conn: 2471792156240: _acquire sess: None, name: master, idle: 12.178889513015747s, acqrelcnt: 1, lang: None, thrd: (13508, 5840), cmpt: ``, lut: 1707842517.116406
[0m23:42:09.297297 [debug] [MainThread]: Databricks adapter: conn: 2471792156240: get_thread_connection: sess: None, name: master, idle: 12.180891990661621s, acqrelcnt: 1, lang: None, thrd: (13508, 5840), cmpt: ``, lut: 1707842517.116406
[0m23:42:09.298294 [debug] [MainThread]: Databricks adapter: conn: 2471792156240: idle check connection: sess: None, name: master, idle: 12.181888103485107s, acqrelcnt: 1, lang: None, thrd: (13508, 5840), cmpt: ``, lut: 1707842517.116406
[0m23:42:09.299522 [debug] [MainThread]: On master: ROLLBACK
[0m23:42:09.299522 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:42:09.611947 [debug] [MainThread]: Databricks adapter: conn: 2471792156240: session opened sess: 9a861f85-d816-4706-86b2-3cd7ed236acb, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (13508, 5840), cmpt: ``, lut: 1707842529.6119478
[0m23:42:09.613022 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:42:09.614045 [debug] [MainThread]: Databricks adapter: conn: 2471792156240: get_thread_connection: sess: 9a861f85-d816-4706-86b2-3cd7ed236acb, name: master, idle: 0.002097606658935547s, acqrelcnt: 1, lang: None, thrd: (13508, 5840), cmpt: ``, lut: 1707842529.6119478
[0m23:42:09.614954 [debug] [MainThread]: Databricks adapter: conn: 2471792156240: idle check connection: sess: 9a861f85-d816-4706-86b2-3cd7ed236acb, name: master, idle: 0.0030066967010498047s, acqrelcnt: 1, lang: None, thrd: (13508, 5840), cmpt: ``, lut: 1707842529.6119478
[0m23:42:09.614954 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:42:09.615952 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:42:09.615952 [debug] [MainThread]: Databricks adapter: conn: 2471792156240: _release sess: 9a861f85-d816-4706-86b2-3cd7ed236acb, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (13508, 5840), cmpt: ``, lut: 1707842529.615952
[0m23:42:09.616951 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:42:09.616951 [debug] [MainThread]: On master: ROLLBACK
[0m23:42:09.617950 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:42:09.617950 [debug] [MainThread]: On master: Close
[0m23:42:09.722438 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m23:42:09.723444 [debug] [MainThread]: On list_hive_metastore: Close
[0m23:42:09.818805 [debug] [MainThread]: Connection 'list_hive_metastore_saleslt' was properly closed.
[0m23:42:09.819687 [debug] [MainThread]: On list_hive_metastore_saleslt: ROLLBACK
[0m23:42:09.820278 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:42:09.821300 [debug] [MainThread]: On list_hive_metastore_saleslt: Close
[0m23:42:09.899425 [debug] [MainThread]: Connection 'model.medallion_dbt_spark.my_second_dbt_model' was properly closed.
[0m23:42:09.900520 [debug] [MainThread]: On model.medallion_dbt_spark.my_second_dbt_model: ROLLBACK
[0m23:42:09.902428 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:42:09.903431 [debug] [MainThread]: On model.medallion_dbt_spark.my_second_dbt_model: Close
[0m23:42:10.011644 [info ] [MainThread]: 
[0m23:42:10.012643 [info ] [MainThread]: Finished running 4 table models, 1 view model in 0 hours 0 minutes and 15.01 seconds (15.01s).
[0m23:42:10.015947 [debug] [MainThread]: Command end result
[0m23:42:10.038498 [info ] [MainThread]: 
[0m23:42:10.039499 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:42:10.040498 [info ] [MainThread]: 
[0m23:42:10.041498 [error] [MainThread]:   Runtime Error in model dim_customer (models\marts\customer\dim_customer.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `customer_snapshot`.`AddressID` cannot be resolved. Did you mean one of the following? [`customeraddress_snapshot`.`AddressID`, `address_snapshot`.`AddressID`, `customer_snapshot`.`CustomerID`, `customer_snapshot`.`FullName`, `customeraddress_snapshot`.`AddressType`].; line 49 pos 4
[0m23:42:10.042497 [info ] [MainThread]: 
[0m23:42:10.043497 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=1 SKIP=0 TOTAL=5
[0m23:42:10.046499 [debug] [MainThread]: Command `dbt run` failed at 23:42:10.045498 after 16.95 seconds
[0m23:42:10.046499 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FEFF23F10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FF0216A10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F82434090>]}
[0m23:42:10.047951 [debug] [MainThread]: Flushing usage events
[0m23:46:18.851735 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195FCCA4BD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195FCCD4390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195FC394090>]}


============================== 23:46:18.857739 | f4ca8aec-6e82-4f5d-986d-39c666b7d877 ==============================
[0m23:46:18.857739 [info ] [MainThread]: Running with dbt=1.7.7
[0m23:46:18.859736 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m23:46:20.489135 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f4ca8aec-6e82-4f5d-986d-39c666b7d877', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195FCD1DF10>]}
[0m23:46:20.556238 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f4ca8aec-6e82-4f5d-986d-39c666b7d877', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001958FBB3190>]}
[0m23:46:20.557341 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m23:46:20.568103 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m23:46:20.716070 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 4 files changed.
[0m23:46:20.717070 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://models\marts\sales\dim_sales.yml
[0m23:46:20.718070 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://models\marts\product\dim_product.yml
[0m23:46:20.719070 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://models\marts\customer\dim_customer.yml
[0m23:46:20.719070 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://models\marts\customer\dim_customer.sql
[0m23:46:20.891013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f4ca8aec-6e82-4f5d-986d-39c666b7d877', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001958FF2AAD0>]}
[0m23:46:20.910498 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f4ca8aec-6e82-4f5d-986d-39c666b7d877', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001958FDA4050>]}
[0m23:46:20.911458 [info ] [MainThread]: Found 5 models, 7 snapshots, 8 tests, 9 sources, 0 exposures, 0 metrics, 535 macros, 0 groups, 0 semantic models
[0m23:46:20.912411 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f4ca8aec-6e82-4f5d-986d-39c666b7d877', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001958FB6D390>]}
[0m23:46:20.914493 [info ] [MainThread]: 
[0m23:46:20.916407 [debug] [MainThread]: Databricks adapter: conn: 1741874230032: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (2452, 24972), cmpt: ``, lut: None
[0m23:46:20.917407 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:46:20.918407 [debug] [MainThread]: Databricks adapter: Thread (2452, 24972) using default compute resource.
[0m23:46:20.918407 [debug] [MainThread]: Databricks adapter: conn: 1741874230032: _acquire sess: None, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (2452, 24972), cmpt: ``, lut: 1707842780.9184074
[0m23:46:20.925408 [debug] [ThreadPool]: Databricks adapter: conn: 1741876958032: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (2452, 22884), cmpt: ``, lut: None
[0m23:46:20.927409 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m23:46:20.931409 [debug] [ThreadPool]: Databricks adapter: Thread (2452, 22884) using default compute resource.
[0m23:46:20.934007 [debug] [ThreadPool]: Databricks adapter: conn: 1741876958032: _acquire sess: None, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (2452, 22884), cmpt: ``, lut: 1707842780.934007
[0m23:46:20.935237 [debug] [ThreadPool]: Databricks adapter: conn: 1741876958032: get_thread_connection: sess: None, name: list_hive_metastore, idle: 0.0012307167053222656s, acqrelcnt: 1, lang: None, thrd: (2452, 22884), cmpt: ``, lut: 1707842780.934007
[0m23:46:20.936246 [debug] [ThreadPool]: Databricks adapter: conn: 1741876958032: idle check connection: sess: None, name: list_hive_metastore, idle: 0.0022389888763427734s, acqrelcnt: 1, lang: None, thrd: (2452, 22884), cmpt: ``, lut: 1707842780.934007
[0m23:46:20.937303 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m23:46:20.938311 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m23:46:20.938311 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:46:21.675013 [debug] [ThreadPool]: Databricks adapter: conn: 1741876958032: session opened sess: abea81c3-07e6-4fff-8032-25520f8e7ebb, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (2452, 22884), cmpt: ``, lut: 1707842781.675013
[0m23:46:22.188134 [debug] [ThreadPool]: SQL status: OK in 1.25 seconds
[0m23:46:22.208901 [debug] [ThreadPool]: Databricks adapter: conn: 1741876958032: _release sess: abea81c3-07e6-4fff-8032-25520f8e7ebb, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (2452, 22884), cmpt: ``, lut: 1707842782.2089012
[0m23:46:22.211903 [debug] [ThreadPool]: Databricks adapter: conn: 1741874986064: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_saleslt, idle: 0s, acqrelcnt: 0, lang: None, thrd: (2452, 6632), cmpt: ``, lut: None
[0m23:46:22.212905 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m23:46:22.213899 [debug] [ThreadPool]: Databricks adapter: Thread (2452, 6632) using default compute resource.
[0m23:46:22.213899 [debug] [ThreadPool]: Databricks adapter: conn: 1741874986064: _acquire sess: None, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (2452, 6632), cmpt: ``, lut: 1707842782.2138999
[0m23:46:22.218920 [debug] [ThreadPool]: Databricks adapter: conn: 1741874986064: get_thread_connection: sess: None, name: list_hive_metastore_saleslt, idle: 0.005020618438720703s, acqrelcnt: 1, lang: None, thrd: (2452, 6632), cmpt: ``, lut: 1707842782.2138999
[0m23:46:22.219919 [debug] [ThreadPool]: Databricks adapter: conn: 1741874986064: idle check connection: sess: None, name: list_hive_metastore_saleslt, idle: 0.00601959228515625s, acqrelcnt: 1, lang: None, thrd: (2452, 6632), cmpt: ``, lut: 1707842782.2138999
[0m23:46:22.219919 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:46:22.220900 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m23:46:22.220900 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:46:22.610184 [debug] [ThreadPool]: Databricks adapter: conn: 1741874986064: session opened sess: 76493b4a-154e-4d58-b533-f0f919c41a8c, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (2452, 6632), cmpt: ``, lut: 1707842782.6091888
[0m23:46:22.725470 [debug] [ThreadPool]: SQL status: OK in 0.5 seconds
[0m23:46:22.739187 [debug] [ThreadPool]: Databricks adapter: conn: 1741874986064: get_thread_connection: sess: 76493b4a-154e-4d58-b533-f0f919c41a8c, name: list_hive_metastore_saleslt, idle: 0.12999820709228516s, acqrelcnt: 1, lang: None, thrd: (2452, 6632), cmpt: ``, lut: 1707842782.6091888
[0m23:46:22.740091 [debug] [ThreadPool]: Databricks adapter: conn: 1741874986064: idle check connection: sess: 76493b4a-154e-4d58-b533-f0f919c41a8c, name: list_hive_metastore_saleslt, idle: 0.12999820709228516s, acqrelcnt: 1, lang: None, thrd: (2452, 6632), cmpt: ``, lut: 1707842782.6091888
[0m23:46:22.740091 [debug] [ThreadPool]: Databricks adapter: conn: 1741874986064: get_thread_connection: sess: 76493b4a-154e-4d58-b533-f0f919c41a8c, name: list_hive_metastore_saleslt, idle: 0.13090229034423828s, acqrelcnt: 1, lang: None, thrd: (2452, 6632), cmpt: ``, lut: 1707842782.6091888
[0m23:46:22.740091 [debug] [ThreadPool]: Databricks adapter: conn: 1741874986064: idle check connection: sess: 76493b4a-154e-4d58-b533-f0f919c41a8c, name: list_hive_metastore_saleslt, idle: 0.13090229034423828s, acqrelcnt: 1, lang: None, thrd: (2452, 6632), cmpt: ``, lut: 1707842782.6091888
[0m23:46:22.741091 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:46:22.741091 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:46:22.741091 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m23:46:22.933355 [debug] [ThreadPool]: SQL status: OK in 0.1899999976158142 seconds
[0m23:46:22.941357 [debug] [ThreadPool]: Databricks adapter: conn: 1741874986064: get_thread_connection: sess: 76493b4a-154e-4d58-b533-f0f919c41a8c, name: list_hive_metastore_saleslt, idle: 0.3321688175201416s, acqrelcnt: 1, lang: None, thrd: (2452, 6632), cmpt: ``, lut: 1707842782.6091888
[0m23:46:22.942358 [debug] [ThreadPool]: Databricks adapter: conn: 1741874986064: idle check connection: sess: 76493b4a-154e-4d58-b533-f0f919c41a8c, name: list_hive_metastore_saleslt, idle: 0.33316969871520996s, acqrelcnt: 1, lang: None, thrd: (2452, 6632), cmpt: ``, lut: 1707842782.6091888
[0m23:46:22.942358 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:46:22.943359 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m23:46:23.142141 [debug] [ThreadPool]: SQL status: OK in 0.20000000298023224 seconds
[0m23:46:23.147065 [debug] [ThreadPool]: Databricks adapter: conn: 1741874986064: _release sess: 76493b4a-154e-4d58-b533-f0f919c41a8c, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (2452, 6632), cmpt: ``, lut: 1707842783.1470656
[0m23:46:23.149285 [debug] [ThreadPool]: Databricks adapter: conn: 1741874986064: idle check connection: sess: 76493b4a-154e-4d58-b533-f0f919c41a8c, name: list_hive_metastore_saleslt, idle: 0.00222015380859375s, acqrelcnt: 0, lang: None, thrd: (2452, 6632), cmpt: ``, lut: 1707842783.1470656
[0m23:46:23.152286 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m23:46:23.152286 [debug] [ThreadPool]: Databricks adapter: conn: 1741874986064: reusing connection list_hive_metastore_saleslt sess: 76493b4a-154e-4d58-b533-f0f919c41a8c, name: list_hive_metastore_snapshots, idle: 0.005220890045166016s, acqrelcnt: 0, lang: None, thrd: (2452, 6632), cmpt: ``, lut: 1707842783.1470656
[0m23:46:23.153285 [debug] [ThreadPool]: Databricks adapter: Thread (2452, 6632) using default compute resource.
[0m23:46:23.153285 [debug] [ThreadPool]: Databricks adapter: conn: 1741874986064: _acquire sess: 76493b4a-154e-4d58-b533-f0f919c41a8c, name: list_hive_metastore_snapshots, idle: 0.006220102310180664s, acqrelcnt: 1, lang: None, thrd: (2452, 6632), cmpt: ``, lut: 1707842783.1470656
[0m23:46:23.155284 [debug] [ThreadPool]: Databricks adapter: conn: 1741874986064: get_thread_connection: sess: 76493b4a-154e-4d58-b533-f0f919c41a8c, name: list_hive_metastore_snapshots, idle: 0.008218526840209961s, acqrelcnt: 1, lang: None, thrd: (2452, 6632), cmpt: ``, lut: 1707842783.1470656
[0m23:46:23.156284 [debug] [ThreadPool]: Databricks adapter: conn: 1741874986064: idle check connection: sess: 76493b4a-154e-4d58-b533-f0f919c41a8c, name: list_hive_metastore_snapshots, idle: 0.009218692779541016s, acqrelcnt: 1, lang: None, thrd: (2452, 6632), cmpt: ``, lut: 1707842783.1470656
[0m23:46:23.156284 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m23:46:23.156284 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m23:46:23.263157 [debug] [ThreadPool]: SQL status: OK in 0.10999999940395355 seconds
[0m23:46:23.270617 [debug] [ThreadPool]: Databricks adapter: conn: 1741874986064: get_thread_connection: sess: 76493b4a-154e-4d58-b533-f0f919c41a8c, name: list_hive_metastore_snapshots, idle: 0.1235513687133789s, acqrelcnt: 1, lang: None, thrd: (2452, 6632), cmpt: ``, lut: 1707842783.1470656
[0m23:46:23.271747 [debug] [ThreadPool]: Databricks adapter: conn: 1741874986064: idle check connection: sess: 76493b4a-154e-4d58-b533-f0f919c41a8c, name: list_hive_metastore_snapshots, idle: 0.12468194961547852s, acqrelcnt: 1, lang: None, thrd: (2452, 6632), cmpt: ``, lut: 1707842783.1470656
[0m23:46:23.271747 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m23:46:23.272625 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m23:46:23.515555 [debug] [ThreadPool]: SQL status: OK in 0.23999999463558197 seconds
[0m23:46:23.519554 [debug] [ThreadPool]: Databricks adapter: conn: 1741874986064: get_thread_connection: sess: 76493b4a-154e-4d58-b533-f0f919c41a8c, name: list_hive_metastore_snapshots, idle: 0.37248849868774414s, acqrelcnt: 1, lang: None, thrd: (2452, 6632), cmpt: ``, lut: 1707842783.1470656
[0m23:46:23.520555 [debug] [ThreadPool]: Databricks adapter: conn: 1741874986064: idle check connection: sess: 76493b4a-154e-4d58-b533-f0f919c41a8c, name: list_hive_metastore_snapshots, idle: 0.37248849868774414s, acqrelcnt: 1, lang: None, thrd: (2452, 6632), cmpt: ``, lut: 1707842783.1470656
[0m23:46:23.520555 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m23:46:23.520555 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m23:46:23.690136 [debug] [ThreadPool]: SQL status: OK in 0.17000000178813934 seconds
[0m23:46:23.693137 [debug] [ThreadPool]: Databricks adapter: conn: 1741874986064: _release sess: 76493b4a-154e-4d58-b533-f0f919c41a8c, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (2452, 6632), cmpt: ``, lut: 1707842783.693137
[0m23:46:23.696138 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f4ca8aec-6e82-4f5d-986d-39c666b7d877', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001958FEF2050>]}
[0m23:46:23.697139 [debug] [MainThread]: Databricks adapter: conn: 1741874230032: get_thread_connection: sess: None, name: master, idle: 2.777730941772461s, acqrelcnt: 1, lang: None, thrd: (2452, 24972), cmpt: ``, lut: 1707842780.9184074
[0m23:46:23.697139 [debug] [MainThread]: Databricks adapter: conn: 1741874230032: idle check connection: sess: None, name: master, idle: 2.7787320613861084s, acqrelcnt: 1, lang: None, thrd: (2452, 24972), cmpt: ``, lut: 1707842780.9184074
[0m23:46:23.698140 [debug] [MainThread]: Databricks adapter: conn: 1741874230032: get_thread_connection: sess: None, name: master, idle: 2.7787320613861084s, acqrelcnt: 1, lang: None, thrd: (2452, 24972), cmpt: ``, lut: 1707842780.9184074
[0m23:46:23.698140 [debug] [MainThread]: Databricks adapter: conn: 1741874230032: idle check connection: sess: None, name: master, idle: 2.779733180999756s, acqrelcnt: 1, lang: None, thrd: (2452, 24972), cmpt: ``, lut: 1707842780.9184074
[0m23:46:23.698140 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:46:23.699274 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:46:23.699274 [debug] [MainThread]: Databricks adapter: conn: 1741874230032: _release sess: None, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (2452, 24972), cmpt: ``, lut: 1707842783.699275
[0m23:46:23.700280 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:46:23.701275 [info ] [MainThread]: 
[0m23:46:23.708447 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.dim_customer
[0m23:46:23.709447 [info ] [Thread-1 (]: 1 of 5 START sql table model saleslt.dim_customer .............................. [RUN]
[0m23:46:23.710447 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: Creating DatabricksDBTConnection sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0s, acqrelcnt: 0, lang: None, thrd: (2452, 13724), cmpt: ``, lut: None
[0m23:46:23.711446 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.medallion_dbt_spark.dim_customer'
[0m23:46:23.711446 [debug] [Thread-1 (]: Databricks adapter: On thread (2452, 13724): `hive_metastore`.`saleslt`.`dim_customer` using default compute resource.
[0m23:46:23.712447 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: _acquire sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842783.7124472
[0m23:46:23.712447 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.dim_customer
[0m23:46:23.721529 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.dim_customer"
[0m23:46:23.723446 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_customer (compile): 23:46:23.713445 => 23:46:23.722447
[0m23:46:23.723446 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.dim_customer
[0m23:46:23.794352 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_dbt_spark.dim_customer"
[0m23:46:23.796317 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: get_thread_connection: sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0.08386993408203125s, acqrelcnt: 1, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842783.7124472
[0m23:46:23.797262 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: idle check connection: sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0.08481526374816895s, acqrelcnt: 1, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842783.7124472
[0m23:46:23.798260 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: get_thread_connection: sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0.08581304550170898s, acqrelcnt: 1, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842783.7124472
[0m23:46:23.799490 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: idle check connection: sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0.08704280853271484s, acqrelcnt: 1, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842783.7124472
[0m23:46:23.799490 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m23:46:23.800493 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.dim_customer"
[0m23:46:23.800493 [debug] [Thread-1 (]: On model.medallion_dbt_spark.dim_customer: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_customer`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/dim_customer'
      
      
      as
      

with address_snapshot as (
    select 
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
),
customeraddress_snapshot as (
    select 
        CustomerID,
        AddressID,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
),
customer_snapshot as (
    select 
        CustomerID,
        concat(ifnull(FirstName, ''), ' ', ifnull(MiddleName, ''), ' ', ifnull(LastName, '')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
),
transformed as (
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incrementing surrogate key
    customer_snapshot.CustomerID,
    customer_snapshot.FullName,
    customer_snapshot.AddressID,
    customer_snapshot.AddressType,
    customer_snapshot.AddressLine1,
    customer_snapshot.City,
    customer_snapshot.StateProvince,
    customer_snapshot.CountryRegion,
    customer_snapshot.PostalCode
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerID = customeraddress_snapshot.CustomerID
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)
select * from transformed
  
[0m23:46:23.801496 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:46:24.042978 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: session opened sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.dim_customer, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842784.042978
[0m23:46:24.590827 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_customer`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/dim_customer'
      
      
      as
      

with address_snapshot as (
    select 
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
),
customeraddress_snapshot as (
    select 
        CustomerID,
        AddressID,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
),
customer_snapshot as (
    select 
        CustomerID,
        concat(ifnull(FirstName, ''), ' ', ifnull(MiddleName, ''), ' ', ifnull(LastName, '')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
),
transformed as (
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incrementing surrogate key
    customer_snapshot.CustomerID,
    customer_snapshot.FullName,
    customer_snapshot.AddressID,
    customer_snapshot.AddressType,
    customer_snapshot.AddressLine1,
    customer_snapshot.City,
    customer_snapshot.StateProvince,
    customer_snapshot.CountryRegion,
    customer_snapshot.PostalCode
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerID = customeraddress_snapshot.CustomerID
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)
select * from transformed
  
[0m23:46:24.591824 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `customer_snapshot`.`AddressID` cannot be resolved. Did you mean one of the following? [`customeraddress_snapshot`.`AddressID`, `address_snapshot`.`AddressID`, `customer_snapshot`.`CustomerID`, `customer_snapshot`.`FullName`, `customeraddress_snapshot`.`AddressType`].; line 49 pos 4
[0m23:46:24.591824 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `customer_snapshot`.`AddressID` cannot be resolved. Did you mean one of the following? [`customeraddress_snapshot`.`AddressID`, `address_snapshot`.`AddressID`, `customer_snapshot`.`CustomerID`, `customer_snapshot`.`FullName`, `customeraddress_snapshot`.`AddressType`].; line 49 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `customer_snapshot`.`AddressID` cannot be resolved. Did you mean one of the following? [`customeraddress_snapshot`.`AddressID`, `address_snapshot`.`AddressID`, `customer_snapshot`.`CustomerID`, `customer_snapshot`.`FullName`, `customeraddress_snapshot`.`AddressType`].; line 49 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:671)
	... 35 more

[0m23:46:24.592826 [debug] [Thread-1 (]: Databricks adapter: operation-id: 6da037a7-aa76-44cb-af5e-6a1aa36aca0c
[0m23:46:24.593826 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_customer (execute): 23:46:23.723446 => 23:46:24.593826
[0m23:46:24.594826 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: _release sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.dim_customer, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842784.5938263
[0m23:46:24.610535 [debug] [Thread-1 (]: Runtime Error in model dim_customer (models\marts\customer\dim_customer.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `customer_snapshot`.`AddressID` cannot be resolved. Did you mean one of the following? [`customeraddress_snapshot`.`AddressID`, `address_snapshot`.`AddressID`, `customer_snapshot`.`CustomerID`, `customer_snapshot`.`FullName`, `customeraddress_snapshot`.`AddressType`].; line 49 pos 4
[0m23:46:24.610535 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: _release sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.dim_customer, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842784.6105356
[0m23:46:24.611517 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f4ca8aec-6e82-4f5d-986d-39c666b7d877', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019590171B50>]}
[0m23:46:24.612517 [error] [Thread-1 (]: 1 of 5 ERROR creating sql table model saleslt.dim_customer ..................... [[31mERROR[0m in 0.90s]
[0m23:46:24.613520 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.dim_customer
[0m23:46:24.614516 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.dim_product
[0m23:46:24.615516 [info ] [Thread-1 (]: 2 of 5 START sql table model saleslt.dim_product ............................... [RUN]
[0m23:46:24.616173 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: idle check connection: sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.dim_customer, idle: 0.005637407302856445s, acqrelcnt: 0, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842784.6105356
[0m23:46:24.617267 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.dim_customer, now model.medallion_dbt_spark.dim_product)
[0m23:46:24.617267 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: reusing connection model.medallion_dbt_spark.dim_customer sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.dim_product, idle: 0.006732463836669922s, acqrelcnt: 0, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842784.6105356
[0m23:46:24.618258 [debug] [Thread-1 (]: Databricks adapter: On thread (2452, 13724): `hive_metastore`.`saleslt`.`dim_product` using default compute resource.
[0m23:46:24.618258 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: _acquire sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.dim_product, idle: 0.007722616195678711s, acqrelcnt: 1, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842784.6105356
[0m23:46:24.618258 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.dim_product
[0m23:46:24.622198 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.dim_product"
[0m23:46:24.624180 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_product (compile): 23:46:24.619182 => 23:46:24.623271
[0m23:46:24.625181 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.dim_product
[0m23:46:24.632312 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: get_thread_connection: sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.dim_product, idle: 0.021776676177978516s, acqrelcnt: 1, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842784.6105356
[0m23:46:24.633310 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: idle check connection: sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.dim_product, idle: 0.02277517318725586s, acqrelcnt: 1, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842784.6105356
[0m23:46:24.633310 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.dim_product"
[0m23:46:24.634312 [debug] [Thread-1 (]: On model.medallion_dbt_spark.dim_product: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_product"} */

      describe extended `hive_metastore`.`saleslt`.`dim_product`
  
[0m23:46:24.952384 [debug] [Thread-1 (]: SQL status: OK in 0.3199999928474426 seconds
[0m23:46:24.957644 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_dbt_spark.dim_product"
[0m23:46:24.958734 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: get_thread_connection: sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.dim_product, idle: 0.34819889068603516s, acqrelcnt: 1, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842784.6105356
[0m23:46:24.959646 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: idle check connection: sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.dim_product, idle: 0.34911131858825684s, acqrelcnt: 1, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842784.6105356
[0m23:46:24.959646 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.dim_product"
[0m23:46:24.960648 [debug] [Thread-1 (]: On model.medallion_dbt_spark.dim_product: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/dim_product'
      
      
      as
      
with product_snapshot as (
    select 
        ProductId,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate
    from `hive_metastore`.`snapshots`.`product_snapshot` where dbt_valid_to is null
),
productmodel_snapshot as (
    select 
        ProductModelID,
        Name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot` where dbt_valid_to is null
),
transformed as (
    select
    row_number() over (order by product_snapshot.ProductId) as product_sk, -- auto-incrementing surrogate key
    product_snapshot.Name as product_name,
    product_snapshot.StandardCost,
    product_snapshot.ListPrice,
    product_snapshot.Size,
    product_snapshot.Weight,
    productmodel_snapshot.Name as model,
    productmodel_snapshot.CatalogDescription as description,
    product_snapshot.SellStartDate,
    product_snapshot.SellEndDate,
    product_snapshot.DiscontinuedDate
    from product_snapshot
    left join productmodel_snapshot on product_snapshot.ProductModelID = productmodel_snapshot.ProductModelID
)
select * from transformed
  
[0m23:46:28.898771 [debug] [Thread-1 (]: SQL status: OK in 3.940000057220459 seconds
[0m23:46:28.927034 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_product (execute): 23:46:24.625308 => 23:46:28.927034
[0m23:46:28.928011 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: _release sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.dim_product, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842788.9270341
[0m23:46:28.928994 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: _release sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.dim_product, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842788.928012
[0m23:46:28.928994 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f4ca8aec-6e82-4f5d-986d-39c666b7d877', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001958FF9B450>]}
[0m23:46:28.929964 [info ] [Thread-1 (]: 2 of 5 OK created sql table model saleslt.dim_product .......................... [[32mOK[0m in 4.31s]
[0m23:46:28.930961 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.dim_product
[0m23:46:28.931963 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.dim_sales
[0m23:46:28.931963 [info ] [Thread-1 (]: 3 of 5 START sql table model saleslt.dim_sales ................................. [RUN]
[0m23:46:28.934236 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: idle check connection: sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.dim_product, idle: 0.006224632263183594s, acqrelcnt: 0, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842788.928012
[0m23:46:28.934236 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.dim_product, now model.medallion_dbt_spark.dim_sales)
[0m23:46:28.935238 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: reusing connection model.medallion_dbt_spark.dim_product sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.dim_sales, idle: 0.007225990295410156s, acqrelcnt: 0, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842788.928012
[0m23:46:28.936242 [debug] [Thread-1 (]: Databricks adapter: On thread (2452, 13724): `hive_metastore`.`saleslt`.`dim_sales` using default compute resource.
[0m23:46:28.936242 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: _acquire sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.dim_sales, idle: 0.00823068618774414s, acqrelcnt: 1, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842788.928012
[0m23:46:28.937246 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.dim_sales
[0m23:46:28.941319 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.dim_sales"
[0m23:46:28.942239 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_sales (compile): 23:46:28.937246 => 23:46:28.942239
[0m23:46:28.942239 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.dim_sales
[0m23:46:28.947319 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: get_thread_connection: sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.dim_sales, idle: 0.01826763153076172s, acqrelcnt: 1, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842788.928012
[0m23:46:28.947319 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: idle check connection: sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.dim_sales, idle: 0.019307851791381836s, acqrelcnt: 1, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842788.928012
[0m23:46:28.948282 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.dim_sales"
[0m23:46:28.948282 [debug] [Thread-1 (]: On model.medallion_dbt_spark.dim_sales: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_sales"} */

      describe extended `hive_metastore`.`saleslt`.`dim_sales`
  
[0m23:46:29.231896 [debug] [Thread-1 (]: SQL status: OK in 0.2800000011920929 seconds
[0m23:46:29.238990 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_dbt_spark.dim_sales"
[0m23:46:29.240901 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: get_thread_connection: sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.dim_sales, idle: 0.31197142601013184s, acqrelcnt: 1, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842788.928012
[0m23:46:29.240901 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: idle check connection: sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.dim_sales, idle: 0.31288957595825195s, acqrelcnt: 1, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842788.928012
[0m23:46:29.241897 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.dim_sales"
[0m23:46:29.241897 [debug] [Thread-1 (]: On model.medallion_dbt_spark.dim_sales: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_sales`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/dim_sales'
      
      
      as
      
with product_snapshot as (
    select 
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    from `hive_metastore`.`saleslt`.`product` 
),
salesorderdetail_snapshot as (
    select 
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    from `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` 
),
salesorderheader_snapshot as (
    select 
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    from `hive_metastore`.`saleslt`.`salesorderheader` 
),
transformed as (
    select
    product_snapshot.Name,
    product_snapshot.ProductNumber,
    product_snapshot.Color,
    product_snapshot.StandardCost,
    product_snapshot.ListPrice,
    product_snapshot.Size,
    product_snapshot.Weight,
    product_snapshot.SellStartDate,
    product_snapshot.SellEndDate,
    product_snapshot.DiscontinuedDate,
    product_snapshot.ThumbNailPhoto,
    product_snapshot.ThumbnailPhotoFileName,
    salesorderdetail_snapshot.SalesOrderID,
    salesorderdetail_snapshot.SalesOrderDetailID,
    salesorderdetail_snapshot.OrderQty,
    salesorderdetail_snapshot.ProductID,
    salesorderdetail_snapshot.UnitPrice,
    salesorderdetail_snapshot.UnitPriceDiscount,
    salesorderdetail_snapshot.LineTotal,
    salesorderheader_snapshot.RevisionNumber,
    salesorderheader_snapshot.OrderDate,
    salesorderheader_snapshot.DueDate,
    salesorderheader_snapshot.ShipDate,
    salesorderheader_snapshot.Status,
    salesorderheader_snapshot.OnlineOrderFlag,
    salesorderheader_snapshot.SalesOrderNumber,
    salesorderheader_snapshot.PurchaseOrderNumber,
    salesorderheader_snapshot.AccountNumber,
    salesorderheader_snapshot.CustomerID,
    salesorderheader_snapshot.BillToAddressID,
    salesorderheader_snapshot.ShipToAddressID,
    salesorderheader_snapshot.ShipMethod,
    salesorderheader_snapshot.CreditCardApprovalCode,
    salesorderheader_snapshot.SubTotal,
    salesorderheader_snapshot.TaxAmt,
    salesorderheader_snapshot.Freight,
    salesorderheader_snapshot.TotalDue,
    salesorderheader_snapshot.Comment
    from product_snapshot
    left join salesorderdetail_snapshot on product_snapshot.ProductID = salesorderdetail_snapshot.ProductID
    left join salesorderheader_snapshot on salesorderdetail_snapshot.SalesOrderID = salesorderheader_snapshot.SalesOrderID
)

select * from transformed
  
[0m23:46:32.713797 [debug] [Thread-1 (]: SQL status: OK in 3.4700000286102295 seconds
[0m23:46:32.717776 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_sales (execute): 23:46:28.943320 => 23:46:32.716802
[0m23:46:32.717776 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: _release sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.dim_sales, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842792.7177763
[0m23:46:32.718706 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: _release sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.dim_sales, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842792.7187064
[0m23:46:32.719702 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f4ca8aec-6e82-4f5d-986d-39c666b7d877', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001958FFE9F10>]}
[0m23:46:32.719702 [info ] [Thread-1 (]: 3 of 5 OK created sql table model saleslt.dim_sales ............................ [[32mOK[0m in 3.79s]
[0m23:46:32.720705 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.dim_sales
[0m23:46:32.721703 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.my_first_dbt_model
[0m23:46:32.722754 [info ] [Thread-1 (]: 4 of 5 START sql table model saleslt.my_first_dbt_model ........................ [RUN]
[0m23:46:32.725059 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: idle check connection: sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.dim_sales, idle: 0.005273103713989258s, acqrelcnt: 0, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842792.7187064
[0m23:46:32.725059 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.dim_sales, now model.medallion_dbt_spark.my_first_dbt_model)
[0m23:46:32.725985 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: reusing connection model.medallion_dbt_spark.dim_sales sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.006352663040161133s, acqrelcnt: 0, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842792.7187064
[0m23:46:32.725985 [debug] [Thread-1 (]: Databricks adapter: On thread (2452, 13724): `hive_metastore`.`saleslt`.`my_first_dbt_model` using default compute resource.
[0m23:46:32.726982 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: _acquire sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.007279396057128906s, acqrelcnt: 1, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842792.7187064
[0m23:46:32.726982 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.my_first_dbt_model
[0m23:46:32.730068 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.my_first_dbt_model"
[0m23:46:32.730983 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.my_first_dbt_model (compile): 23:46:32.726982 => 23:46:32.730983
[0m23:46:32.730983 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.my_first_dbt_model
[0m23:46:32.831008 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: get_thread_connection: sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.01748800277709961s, acqrelcnt: 1, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842792.7187064
[0m23:46:32.832256 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: idle check connection: sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.11355042457580566s, acqrelcnt: 1, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842792.7187064
[0m23:46:32.832256 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.my_first_dbt_model"
[0m23:46:32.833259 [debug] [Thread-1 (]: On model.medallion_dbt_spark.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.my_first_dbt_model"} */

      describe extended `hive_metastore`.`saleslt`.`my_first_dbt_model`
  
[0m23:46:33.118577 [debug] [Thread-1 (]: SQL status: OK in 0.28999999165534973 seconds
[0m23:46:33.125657 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_dbt_spark.my_first_dbt_model"
[0m23:46:33.127654 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: get_thread_connection: sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.40795063972473145s, acqrelcnt: 1, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842792.7187064
[0m23:46:33.127654 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: idle check connection: sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.4089477062225342s, acqrelcnt: 1, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842792.7187064
[0m23:46:33.128658 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.my_first_dbt_model"
[0m23:46:33.128658 [debug] [Thread-1 (]: On model.medallion_dbt_spark.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.my_first_dbt_model"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`my_first_dbt_model`
      
      
    using delta
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m23:46:35.425729 [debug] [Thread-1 (]: SQL status: OK in 2.299999952316284 seconds
[0m23:46:35.431829 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.my_first_dbt_model (execute): 23:46:32.732102 => 23:46:35.431829
[0m23:46:35.433046 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: _release sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842795.432942
[0m23:46:35.434096 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: _release sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842795.4340968
[0m23:46:35.434096 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f4ca8aec-6e82-4f5d-986d-39c666b7d877', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001958FA13C90>]}
[0m23:46:35.435078 [info ] [Thread-1 (]: 4 of 5 OK created sql table model saleslt.my_first_dbt_model ................... [[32mOK[0m in 2.71s]
[0m23:46:35.436069 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.my_first_dbt_model
[0m23:46:35.438048 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.my_second_dbt_model
[0m23:46:35.438048 [info ] [Thread-1 (]: 5 of 5 START sql view model saleslt.my_second_dbt_model ........................ [RUN]
[0m23:46:35.440049 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: idle check connection: sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.0059528350830078125s, acqrelcnt: 0, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842795.4340968
[0m23:46:35.441052 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.my_first_dbt_model, now model.medallion_dbt_spark.my_second_dbt_model)
[0m23:46:35.441052 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: reusing connection model.medallion_dbt_spark.my_first_dbt_model sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.006955385208129883s, acqrelcnt: 0, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842795.4340968
[0m23:46:35.442050 [debug] [Thread-1 (]: Databricks adapter: On thread (2452, 13724): `hive_metastore`.`saleslt`.`my_second_dbt_model` using default compute resource.
[0m23:46:35.442050 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: _acquire sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.007953166961669922s, acqrelcnt: 1, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842795.4340968
[0m23:46:35.443051 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.my_second_dbt_model
[0m23:46:35.447052 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.my_second_dbt_model"
[0m23:46:35.448050 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.my_second_dbt_model (compile): 23:46:35.443051 => 23:46:35.448050
[0m23:46:35.448050 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.my_second_dbt_model
[0m23:46:35.466223 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_dbt_spark.my_second_dbt_model"
[0m23:46:35.467222 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: get_thread_connection: sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.03312516212463379s, acqrelcnt: 1, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842795.4340968
[0m23:46:35.468224 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: idle check connection: sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.03412771224975586s, acqrelcnt: 1, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842795.4340968
[0m23:46:35.468224 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.my_second_dbt_model"
[0m23:46:35.469226 [debug] [Thread-1 (]: On model.medallion_dbt_spark.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.my_second_dbt_model"} */
create or replace view `hive_metastore`.`saleslt`.`my_second_dbt_model`
  
  
  
  as
    -- Use the `ref` function to select from other models

select *
from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id = 1

[0m23:46:35.918550 [debug] [Thread-1 (]: SQL status: OK in 0.44999998807907104 seconds
[0m23:46:35.921543 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.my_second_dbt_model (execute): 23:46:35.449219 => 23:46:35.921543
[0m23:46:35.922542 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: _release sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842795.922542
[0m23:46:35.923540 [debug] [Thread-1 (]: Databricks adapter: conn: 1741874063376: _release sess: d1c12bc4-13f8-4993-a5e1-d7e8d8ee48ec, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2452, 13724), cmpt: ``, lut: 1707842795.9235406
[0m23:46:35.923540 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f4ca8aec-6e82-4f5d-986d-39c666b7d877', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195905EF810>]}
[0m23:46:35.924724 [info ] [Thread-1 (]: 5 of 5 OK created sql view model saleslt.my_second_dbt_model ................... [[32mOK[0m in 0.48s]
[0m23:46:35.927124 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.my_second_dbt_model
[0m23:46:35.929194 [debug] [MainThread]: Databricks adapter: conn: 1741874230032: idle check connection: sess: None, name: master, idle: 12.228849411010742s, acqrelcnt: 0, lang: None, thrd: (2452, 24972), cmpt: ``, lut: 1707842783.699275
[0m23:46:35.929194 [debug] [MainThread]: Databricks adapter: conn: 1741874230032: reusing connection master sess: None, name: master, idle: 12.229919195175171s, acqrelcnt: 0, lang: None, thrd: (2452, 24972), cmpt: ``, lut: 1707842783.699275
[0m23:46:35.930125 [debug] [MainThread]: Databricks adapter: Thread (2452, 24972) using default compute resource.
[0m23:46:35.930125 [debug] [MainThread]: Databricks adapter: conn: 1741874230032: _acquire sess: None, name: master, idle: 12.230850458145142s, acqrelcnt: 1, lang: None, thrd: (2452, 24972), cmpt: ``, lut: 1707842783.699275
[0m23:46:35.930125 [debug] [MainThread]: Databricks adapter: conn: 1741874230032: get_thread_connection: sess: None, name: master, idle: 12.230850458145142s, acqrelcnt: 1, lang: None, thrd: (2452, 24972), cmpt: ``, lut: 1707842783.699275
[0m23:46:35.931121 [debug] [MainThread]: Databricks adapter: conn: 1741874230032: idle check connection: sess: None, name: master, idle: 12.231846332550049s, acqrelcnt: 1, lang: None, thrd: (2452, 24972), cmpt: ``, lut: 1707842783.699275
[0m23:46:35.931121 [debug] [MainThread]: On master: ROLLBACK
[0m23:46:35.932125 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:46:36.207095 [debug] [MainThread]: Databricks adapter: conn: 1741874230032: session opened sess: cabf8ae5-f439-4a0f-9e81-d48e4e7389c0, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (2452, 24972), cmpt: ``, lut: 1707842796.2070954
[0m23:46:36.208338 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:46:36.208338 [debug] [MainThread]: Databricks adapter: conn: 1741874230032: get_thread_connection: sess: cabf8ae5-f439-4a0f-9e81-d48e4e7389c0, name: master, idle: 0.0012428760528564453s, acqrelcnt: 1, lang: None, thrd: (2452, 24972), cmpt: ``, lut: 1707842796.2070954
[0m23:46:36.209340 [debug] [MainThread]: Databricks adapter: conn: 1741874230032: idle check connection: sess: cabf8ae5-f439-4a0f-9e81-d48e4e7389c0, name: master, idle: 0.002245187759399414s, acqrelcnt: 1, lang: None, thrd: (2452, 24972), cmpt: ``, lut: 1707842796.2070954
[0m23:46:36.209340 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:46:36.210340 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:46:36.210340 [debug] [MainThread]: Databricks adapter: conn: 1741874230032: _release sess: cabf8ae5-f439-4a0f-9e81-d48e4e7389c0, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (2452, 24972), cmpt: ``, lut: 1707842796.2103405
[0m23:46:36.211382 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:46:36.211382 [debug] [MainThread]: On master: ROLLBACK
[0m23:46:36.212344 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:46:36.212344 [debug] [MainThread]: On master: Close
[0m23:46:36.289305 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m23:46:36.290233 [debug] [MainThread]: On list_hive_metastore: Close
[0m23:46:36.373570 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m23:46:36.373570 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m23:46:36.374834 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:46:36.374834 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m23:46:36.461097 [debug] [MainThread]: Connection 'model.medallion_dbt_spark.my_second_dbt_model' was properly closed.
[0m23:46:36.462098 [debug] [MainThread]: On model.medallion_dbt_spark.my_second_dbt_model: ROLLBACK
[0m23:46:36.462098 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:46:36.463097 [debug] [MainThread]: On model.medallion_dbt_spark.my_second_dbt_model: Close
[0m23:46:36.549790 [info ] [MainThread]: 
[0m23:46:36.551836 [info ] [MainThread]: Finished running 4 table models, 1 view model in 0 hours 0 minutes and 15.63 seconds (15.63s).
[0m23:46:36.553866 [debug] [MainThread]: Command end result
[0m23:46:36.568926 [info ] [MainThread]: 
[0m23:46:36.569927 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:46:36.570926 [info ] [MainThread]: 
[0m23:46:36.571926 [error] [MainThread]:   Runtime Error in model dim_customer (models\marts\customer\dim_customer.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `customer_snapshot`.`AddressID` cannot be resolved. Did you mean one of the following? [`customeraddress_snapshot`.`AddressID`, `address_snapshot`.`AddressID`, `customer_snapshot`.`CustomerID`, `customer_snapshot`.`FullName`, `customeraddress_snapshot`.`AddressType`].; line 49 pos 4
[0m23:46:36.572926 [info ] [MainThread]: 
[0m23:46:36.572926 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=1 SKIP=0 TOTAL=5
[0m23:46:36.575213 [debug] [MainThread]: Command `dbt run` failed at 23:46:36.575213 after 17.77 seconds
[0m23:46:36.576124 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195FCCF62D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195FC93A290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195FC93A650>]}
[0m23:46:36.577126 [debug] [MainThread]: Flushing usage events
[0m23:49:59.781425 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BE3041ACD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BE3069CB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BE30A06750>]}


============================== 23:49:59.781425 | 7341cc4d-b3f5-4c76-96fc-f1e18406ce59 ==============================
[0m23:49:59.781425 [info ] [MainThread]: Running with dbt=1.7.7
[0m23:49:59.781425 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m23:50:01.136255 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7341cc4d-b3f5-4c76-96fc-f1e18406ce59', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BE42B47390>]}
[0m23:50:01.252240 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7341cc4d-b3f5-4c76-96fc-f1e18406ce59', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BE429E3590>]}
[0m23:50:01.252240 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m23:50:01.268761 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m23:50:01.368743 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:50:01.368743 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://models\marts\customer\dim_customer.sql
[0m23:50:01.502034 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7341cc4d-b3f5-4c76-96fc-f1e18406ce59', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BE42D41290>]}
[0m23:50:01.518808 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7341cc4d-b3f5-4c76-96fc-f1e18406ce59', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BE42F979D0>]}
[0m23:50:01.518808 [info ] [MainThread]: Found 5 models, 7 snapshots, 8 tests, 9 sources, 0 exposures, 0 metrics, 535 macros, 0 groups, 0 semantic models
[0m23:50:01.518808 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7341cc4d-b3f5-4c76-96fc-f1e18406ce59', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BE42BE5710>]}
[0m23:50:01.518808 [info ] [MainThread]: 
[0m23:50:01.535396 [debug] [MainThread]: Databricks adapter: conn: 3016188336208: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (19340, 780), cmpt: ``, lut: None
[0m23:50:01.535396 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:50:01.535396 [debug] [MainThread]: Databricks adapter: Thread (19340, 780) using default compute resource.
[0m23:50:01.535396 [debug] [MainThread]: Databricks adapter: conn: 3016188336208: _acquire sess: None, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (19340, 780), cmpt: ``, lut: 1707843001.5353963
[0m23:50:01.535396 [debug] [ThreadPool]: Databricks adapter: conn: 3016190683344: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (19340, 22848), cmpt: ``, lut: None
[0m23:50:01.535396 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m23:50:01.535396 [debug] [ThreadPool]: Databricks adapter: Thread (19340, 22848) using default compute resource.
[0m23:50:01.535396 [debug] [ThreadPool]: Databricks adapter: conn: 3016190683344: _acquire sess: None, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (19340, 22848), cmpt: ``, lut: 1707843001.5353963
[0m23:50:01.535396 [debug] [ThreadPool]: Databricks adapter: conn: 3016190683344: get_thread_connection: sess: None, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (19340, 22848), cmpt: ``, lut: 1707843001.5353963
[0m23:50:01.535396 [debug] [ThreadPool]: Databricks adapter: conn: 3016190683344: idle check connection: sess: None, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (19340, 22848), cmpt: ``, lut: 1707843001.5353963
[0m23:50:01.535396 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m23:50:01.535396 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m23:50:01.535396 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:50:01.970518 [debug] [ThreadPool]: Databricks adapter: conn: 3016190683344: session opened sess: ff560770-8085-4881-a53f-f934e0c3cee0, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (19340, 22848), cmpt: ``, lut: 1707843001.9705184
[0m23:50:02.202863 [debug] [ThreadPool]: SQL status: OK in 0.6700000166893005 seconds
[0m23:50:02.202863 [debug] [ThreadPool]: Databricks adapter: conn: 3016190683344: _release sess: ff560770-8085-4881-a53f-f934e0c3cee0, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (19340, 22848), cmpt: ``, lut: 1707843002.2028637
[0m23:50:02.202863 [debug] [ThreadPool]: Databricks adapter: conn: 3016185173648: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_saleslt, idle: 0s, acqrelcnt: 0, lang: None, thrd: (19340, 5772), cmpt: ``, lut: None
[0m23:50:02.202863 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m23:50:02.202863 [debug] [ThreadPool]: Databricks adapter: Thread (19340, 5772) using default compute resource.
[0m23:50:02.202863 [debug] [ThreadPool]: Databricks adapter: conn: 3016185173648: _acquire sess: None, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (19340, 5772), cmpt: ``, lut: 1707843002.2028637
[0m23:50:02.219706 [debug] [ThreadPool]: Databricks adapter: conn: 3016185173648: get_thread_connection: sess: None, name: list_hive_metastore_saleslt, idle: 0.016843080520629883s, acqrelcnt: 1, lang: None, thrd: (19340, 5772), cmpt: ``, lut: 1707843002.2028637
[0m23:50:02.219706 [debug] [ThreadPool]: Databricks adapter: conn: 3016185173648: idle check connection: sess: None, name: list_hive_metastore_saleslt, idle: 0.016843080520629883s, acqrelcnt: 1, lang: None, thrd: (19340, 5772), cmpt: ``, lut: 1707843002.2028637
[0m23:50:02.219706 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:50:02.219706 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m23:50:02.219706 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:50:02.419906 [debug] [ThreadPool]: Databricks adapter: conn: 3016185173648: session opened sess: 34d25f32-22f7-4f37-921c-192ddc4e2fc4, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (19340, 5772), cmpt: ``, lut: 1707843002.4199064
[0m23:50:02.553419 [debug] [ThreadPool]: SQL status: OK in 0.33000001311302185 seconds
[0m23:50:02.569968 [debug] [ThreadPool]: Databricks adapter: conn: 3016185173648: get_thread_connection: sess: 34d25f32-22f7-4f37-921c-192ddc4e2fc4, name: list_hive_metastore_saleslt, idle: 0.15006208419799805s, acqrelcnt: 1, lang: None, thrd: (19340, 5772), cmpt: ``, lut: 1707843002.4199064
[0m23:50:02.569968 [debug] [ThreadPool]: Databricks adapter: conn: 3016185173648: idle check connection: sess: 34d25f32-22f7-4f37-921c-192ddc4e2fc4, name: list_hive_metastore_saleslt, idle: 0.15006208419799805s, acqrelcnt: 1, lang: None, thrd: (19340, 5772), cmpt: ``, lut: 1707843002.4199064
[0m23:50:02.569968 [debug] [ThreadPool]: Databricks adapter: conn: 3016185173648: get_thread_connection: sess: 34d25f32-22f7-4f37-921c-192ddc4e2fc4, name: list_hive_metastore_saleslt, idle: 0.15006208419799805s, acqrelcnt: 1, lang: None, thrd: (19340, 5772), cmpt: ``, lut: 1707843002.4199064
[0m23:50:02.569968 [debug] [ThreadPool]: Databricks adapter: conn: 3016185173648: idle check connection: sess: 34d25f32-22f7-4f37-921c-192ddc4e2fc4, name: list_hive_metastore_saleslt, idle: 0.15006208419799805s, acqrelcnt: 1, lang: None, thrd: (19340, 5772), cmpt: ``, lut: 1707843002.4199064
[0m23:50:02.569968 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:50:02.569968 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:50:02.569968 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m23:50:02.736881 [debug] [ThreadPool]: SQL status: OK in 0.17000000178813934 seconds
[0m23:50:02.736881 [debug] [ThreadPool]: Databricks adapter: conn: 3016185173648: get_thread_connection: sess: 34d25f32-22f7-4f37-921c-192ddc4e2fc4, name: list_hive_metastore_saleslt, idle: 0.31697559356689453s, acqrelcnt: 1, lang: None, thrd: (19340, 5772), cmpt: ``, lut: 1707843002.4199064
[0m23:50:02.753085 [debug] [ThreadPool]: Databricks adapter: conn: 3016185173648: idle check connection: sess: 34d25f32-22f7-4f37-921c-192ddc4e2fc4, name: list_hive_metastore_saleslt, idle: 0.3331789970397949s, acqrelcnt: 1, lang: None, thrd: (19340, 5772), cmpt: ``, lut: 1707843002.4199064
[0m23:50:02.753522 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:50:02.753522 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m23:50:02.920176 [debug] [ThreadPool]: SQL status: OK in 0.17000000178813934 seconds
[0m23:50:02.935261 [debug] [ThreadPool]: Databricks adapter: conn: 3016185173648: _release sess: 34d25f32-22f7-4f37-921c-192ddc4e2fc4, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (19340, 5772), cmpt: ``, lut: 1707843002.9352615
[0m23:50:02.937145 [debug] [ThreadPool]: Databricks adapter: conn: 3016185173648: idle check connection: sess: 34d25f32-22f7-4f37-921c-192ddc4e2fc4, name: list_hive_metastore_saleslt, idle: 0.00188446044921875s, acqrelcnt: 0, lang: None, thrd: (19340, 5772), cmpt: ``, lut: 1707843002.9352615
[0m23:50:02.937145 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m23:50:02.937145 [debug] [ThreadPool]: Databricks adapter: conn: 3016185173648: reusing connection list_hive_metastore_saleslt sess: 34d25f32-22f7-4f37-921c-192ddc4e2fc4, name: list_hive_metastore_snapshots, idle: 0.00188446044921875s, acqrelcnt: 0, lang: None, thrd: (19340, 5772), cmpt: ``, lut: 1707843002.9352615
[0m23:50:02.937145 [debug] [ThreadPool]: Databricks adapter: Thread (19340, 5772) using default compute resource.
[0m23:50:02.937145 [debug] [ThreadPool]: Databricks adapter: conn: 3016185173648: _acquire sess: 34d25f32-22f7-4f37-921c-192ddc4e2fc4, name: list_hive_metastore_snapshots, idle: 0.00188446044921875s, acqrelcnt: 1, lang: None, thrd: (19340, 5772), cmpt: ``, lut: 1707843002.9352615
[0m23:50:02.937145 [debug] [ThreadPool]: Databricks adapter: conn: 3016185173648: get_thread_connection: sess: 34d25f32-22f7-4f37-921c-192ddc4e2fc4, name: list_hive_metastore_snapshots, idle: 0.00188446044921875s, acqrelcnt: 1, lang: None, thrd: (19340, 5772), cmpt: ``, lut: 1707843002.9352615
[0m23:50:02.937145 [debug] [ThreadPool]: Databricks adapter: conn: 3016185173648: idle check connection: sess: 34d25f32-22f7-4f37-921c-192ddc4e2fc4, name: list_hive_metastore_snapshots, idle: 0.00188446044921875s, acqrelcnt: 1, lang: None, thrd: (19340, 5772), cmpt: ``, lut: 1707843002.9352615
[0m23:50:02.937145 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m23:50:02.937145 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m23:50:03.103787 [debug] [ThreadPool]: SQL status: OK in 0.17000000178813934 seconds
[0m23:50:03.120600 [debug] [ThreadPool]: Databricks adapter: conn: 3016185173648: get_thread_connection: sess: 34d25f32-22f7-4f37-921c-192ddc4e2fc4, name: list_hive_metastore_snapshots, idle: 0.18533873558044434s, acqrelcnt: 1, lang: None, thrd: (19340, 5772), cmpt: ``, lut: 1707843002.9352615
[0m23:50:03.120600 [debug] [ThreadPool]: Databricks adapter: conn: 3016185173648: idle check connection: sess: 34d25f32-22f7-4f37-921c-192ddc4e2fc4, name: list_hive_metastore_snapshots, idle: 0.18533873558044434s, acqrelcnt: 1, lang: None, thrd: (19340, 5772), cmpt: ``, lut: 1707843002.9352615
[0m23:50:03.120600 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m23:50:03.120600 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m23:50:03.237372 [debug] [ThreadPool]: SQL status: OK in 0.11999999731779099 seconds
[0m23:50:03.237372 [debug] [ThreadPool]: Databricks adapter: conn: 3016185173648: get_thread_connection: sess: 34d25f32-22f7-4f37-921c-192ddc4e2fc4, name: list_hive_metastore_snapshots, idle: 0.3021106719970703s, acqrelcnt: 1, lang: None, thrd: (19340, 5772), cmpt: ``, lut: 1707843002.9352615
[0m23:50:03.237372 [debug] [ThreadPool]: Databricks adapter: conn: 3016185173648: idle check connection: sess: 34d25f32-22f7-4f37-921c-192ddc4e2fc4, name: list_hive_metastore_snapshots, idle: 0.3021106719970703s, acqrelcnt: 1, lang: None, thrd: (19340, 5772), cmpt: ``, lut: 1707843002.9352615
[0m23:50:03.237372 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m23:50:03.237372 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m23:50:03.387517 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m23:50:03.387517 [debug] [ThreadPool]: Databricks adapter: conn: 3016185173648: _release sess: 34d25f32-22f7-4f37-921c-192ddc4e2fc4, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (19340, 5772), cmpt: ``, lut: 1707843003.387518
[0m23:50:03.387517 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7341cc4d-b3f5-4c76-96fc-f1e18406ce59', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BE42F51B50>]}
[0m23:50:03.387517 [debug] [MainThread]: Databricks adapter: conn: 3016188336208: get_thread_connection: sess: None, name: master, idle: 1.8521215915679932s, acqrelcnt: 1, lang: None, thrd: (19340, 780), cmpt: ``, lut: 1707843001.5353963
[0m23:50:03.387517 [debug] [MainThread]: Databricks adapter: conn: 3016188336208: idle check connection: sess: None, name: master, idle: 1.8521215915679932s, acqrelcnt: 1, lang: None, thrd: (19340, 780), cmpt: ``, lut: 1707843001.5353963
[0m23:50:03.387517 [debug] [MainThread]: Databricks adapter: conn: 3016188336208: get_thread_connection: sess: None, name: master, idle: 1.8521215915679932s, acqrelcnt: 1, lang: None, thrd: (19340, 780), cmpt: ``, lut: 1707843001.5353963
[0m23:50:03.387517 [debug] [MainThread]: Databricks adapter: conn: 3016188336208: idle check connection: sess: None, name: master, idle: 1.8521215915679932s, acqrelcnt: 1, lang: None, thrd: (19340, 780), cmpt: ``, lut: 1707843001.5353963
[0m23:50:03.387517 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:50:03.387517 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:50:03.387517 [debug] [MainThread]: Databricks adapter: conn: 3016188336208: _release sess: None, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (19340, 780), cmpt: ``, lut: 1707843003.387518
[0m23:50:03.387517 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:50:03.387517 [info ] [MainThread]: 
[0m23:50:03.404123 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.dim_customer
[0m23:50:03.404123 [info ] [Thread-1 (]: 1 of 5 START sql table model saleslt.dim_customer .............................. [RUN]
[0m23:50:03.407996 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: Creating DatabricksDBTConnection sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0s, acqrelcnt: 0, lang: None, thrd: (19340, 15028), cmpt: ``, lut: None
[0m23:50:03.407996 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.medallion_dbt_spark.dim_customer'
[0m23:50:03.409049 [debug] [Thread-1 (]: Databricks adapter: On thread (19340, 15028): `hive_metastore`.`saleslt`.`dim_customer` using default compute resource.
[0m23:50:03.409049 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: _acquire sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843003.409049
[0m23:50:03.410002 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.dim_customer
[0m23:50:03.413095 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.dim_customer"
[0m23:50:03.413095 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_customer (compile): 23:50:03.410002 => 23:50:03.413095
[0m23:50:03.413095 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.dim_customer
[0m23:50:03.487564 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_dbt_spark.dim_customer"
[0m23:50:03.487564 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: get_thread_connection: sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0.07851505279541016s, acqrelcnt: 1, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843003.409049
[0m23:50:03.487564 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: idle check connection: sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0.07851505279541016s, acqrelcnt: 1, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843003.409049
[0m23:50:03.503191 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: get_thread_connection: sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0.09414267539978027s, acqrelcnt: 1, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843003.409049
[0m23:50:03.504226 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: idle check connection: sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0.09485173225402832s, acqrelcnt: 1, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843003.409049
[0m23:50:03.504226 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m23:50:03.504226 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.dim_customer"
[0m23:50:03.504226 [debug] [Thread-1 (]: On model.medallion_dbt_spark.dim_customer: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_customer"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_customer`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/dim_customer'
      
      
      as
      

with address_snapshot as (
    select 
        AddressID,
        AddressLine1,
        AddressLine2,
        City,
        StateProvince,
        CountryRegion,
        PostalCode
    from `hive_metastore`.`snapshots`.`address_snapshot` where dbt_valid_to is null
),
customeraddress_snapshot as (
    select 
        CustomerID,
        AddressID,
        AddressType
    from `hive_metastore`.`snapshots`.`customeraddress_snapshot` where dbt_valid_to is null
),
customer_snapshot as (
    select 
        CustomerID,
        concat(ifnull(FirstName, ''), ' ', ifnull(MiddleName, ''), ' ', ifnull(LastName, '')) as FullName
    from `hive_metastore`.`snapshots`.`customer_snapshot` where dbt_valid_to is null
),
transformed as (
    select
    row_number() over (order by customer_snapshot.customerid) as customer_sk, -- auto-incrementing surrogate key
    customer_snapshot.CustomerID,
    customer_snapshot.FullName,
    customeraddress_snapshot.AddressID,
    customeraddress_snapshot.AddressType,
    address_snapshot.AddressLine1,
    address_snapshot.City,
    address_snapshot.StateProvince,
    address_snapshot.CountryRegion,
    address_snapshot.PostalCode
    from customer_snapshot
    inner join customeraddress_snapshot on customer_snapshot.CustomerID = customeraddress_snapshot.CustomerID
    inner join address_snapshot on customeraddress_snapshot.AddressID = address_snapshot.AddressID
)
select * from transformed
  
[0m23:50:03.504226 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:50:03.704627 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: session opened sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.dim_customer, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843003.7046273
[0m23:50:08.192837 [debug] [Thread-1 (]: SQL status: OK in 4.690000057220459 seconds
[0m23:50:08.209503 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_customer (execute): 23:50:03.413095 => 23:50:08.209503
[0m23:50:08.209503 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: _release sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.dim_customer, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843008.209504
[0m23:50:08.226071 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: _release sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.dim_customer, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843008.2260714
[0m23:50:08.226071 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7341cc4d-b3f5-4c76-96fc-f1e18406ce59', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BE42F513D0>]}
[0m23:50:08.226071 [info ] [Thread-1 (]: 1 of 5 OK created sql table model saleslt.dim_customer ......................... [[32mOK[0m in 4.82s]
[0m23:50:08.226071 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.dim_customer
[0m23:50:08.226071 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.dim_product
[0m23:50:08.226071 [info ] [Thread-1 (]: 2 of 5 START sql table model saleslt.dim_product ............................... [RUN]
[0m23:50:08.226071 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: idle check connection: sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.dim_customer, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843008.2260714
[0m23:50:08.226071 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.dim_customer, now model.medallion_dbt_spark.dim_product)
[0m23:50:08.226071 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: reusing connection model.medallion_dbt_spark.dim_customer sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.dim_product, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843008.2260714
[0m23:50:08.226071 [debug] [Thread-1 (]: Databricks adapter: On thread (19340, 15028): `hive_metastore`.`saleslt`.`dim_product` using default compute resource.
[0m23:50:08.226071 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: _acquire sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.dim_product, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843008.2260714
[0m23:50:08.226071 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.dim_product
[0m23:50:08.226071 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.dim_product"
[0m23:50:08.226071 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_product (compile): 23:50:08.226071 => 23:50:08.226071
[0m23:50:08.226071 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.dim_product
[0m23:50:08.248066 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: get_thread_connection: sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.dim_product, idle: 0.02199554443359375s, acqrelcnt: 1, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843008.2260714
[0m23:50:08.248066 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: idle check connection: sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.dim_product, idle: 0.02199554443359375s, acqrelcnt: 1, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843008.2260714
[0m23:50:08.248066 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.dim_product"
[0m23:50:08.248066 [debug] [Thread-1 (]: On model.medallion_dbt_spark.dim_product: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_product"} */

      describe extended `hive_metastore`.`saleslt`.`dim_product`
  
[0m23:50:08.576640 [debug] [Thread-1 (]: SQL status: OK in 0.33000001311302185 seconds
[0m23:50:08.576640 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_dbt_spark.dim_product"
[0m23:50:08.576640 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: get_thread_connection: sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.dim_product, idle: 0.350569486618042s, acqrelcnt: 1, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843008.2260714
[0m23:50:08.593412 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: idle check connection: sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.dim_product, idle: 0.36702656745910645s, acqrelcnt: 1, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843008.2260714
[0m23:50:08.593412 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.dim_product"
[0m23:50:08.593412 [debug] [Thread-1 (]: On model.medallion_dbt_spark.dim_product: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_product"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_product`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/dim_product'
      
      
      as
      
with product_snapshot as (
    select 
        ProductId,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        ProductCategoryID,
        ProductModelID,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate
    from `hive_metastore`.`snapshots`.`product_snapshot` where dbt_valid_to is null
),
productmodel_snapshot as (
    select 
        ProductModelID,
        Name,
        CatalogDescription,
        row_number() over (order by name) as model_id
    from `hive_metastore`.`snapshots`.`productmodel_snapshot` where dbt_valid_to is null
),
transformed as (
    select
    row_number() over (order by product_snapshot.ProductId) as product_sk, -- auto-incrementing surrogate key
    product_snapshot.Name as product_name,
    product_snapshot.StandardCost,
    product_snapshot.ListPrice,
    product_snapshot.Size,
    product_snapshot.Weight,
    productmodel_snapshot.Name as model,
    productmodel_snapshot.CatalogDescription as description,
    product_snapshot.SellStartDate,
    product_snapshot.SellEndDate,
    product_snapshot.DiscontinuedDate
    from product_snapshot
    left join productmodel_snapshot on product_snapshot.ProductModelID = productmodel_snapshot.ProductModelID
)
select * from transformed
  
[0m23:50:12.019922 [debug] [Thread-1 (]: SQL status: OK in 3.430000066757202 seconds
[0m23:50:12.121472 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_product (execute): 23:50:08.241697 => 23:50:12.121472
[0m23:50:12.122469 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: _release sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.dim_product, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843012.1224692
[0m23:50:12.123313 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: _release sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.dim_product, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843012.123314
[0m23:50:12.123313 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7341cc4d-b3f5-4c76-96fc-f1e18406ce59', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BE42A2A690>]}
[0m23:50:12.124319 [info ] [Thread-1 (]: 2 of 5 OK created sql table model saleslt.dim_product .......................... [[32mOK[0m in 3.90s]
[0m23:50:12.125609 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.dim_product
[0m23:50:12.126654 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.dim_sales
[0m23:50:12.127611 [info ] [Thread-1 (]: 3 of 5 START sql table model saleslt.dim_sales ................................. [RUN]
[0m23:50:12.128613 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: idle check connection: sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.dim_product, idle: 0.005300045013427734s, acqrelcnt: 0, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843012.123314
[0m23:50:12.128613 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.dim_product, now model.medallion_dbt_spark.dim_sales)
[0m23:50:12.129617 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: reusing connection model.medallion_dbt_spark.dim_product sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.dim_sales, idle: 0.0063037872314453125s, acqrelcnt: 0, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843012.123314
[0m23:50:12.129617 [debug] [Thread-1 (]: Databricks adapter: On thread (19340, 15028): `hive_metastore`.`saleslt`.`dim_sales` using default compute resource.
[0m23:50:12.130536 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: _acquire sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.dim_sales, idle: 0.007222175598144531s, acqrelcnt: 1, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843012.123314
[0m23:50:12.131608 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.dim_sales
[0m23:50:12.134708 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.dim_sales"
[0m23:50:12.136719 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_sales (compile): 23:50:12.131863 => 23:50:12.135758
[0m23:50:12.136719 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.dim_sales
[0m23:50:12.140766 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: get_thread_connection: sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.dim_sales, idle: 0.01745295524597168s, acqrelcnt: 1, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843012.123314
[0m23:50:12.141749 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: idle check connection: sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.dim_sales, idle: 0.01745295524597168s, acqrelcnt: 1, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843012.123314
[0m23:50:12.141749 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.dim_sales"
[0m23:50:12.141749 [debug] [Thread-1 (]: On model.medallion_dbt_spark.dim_sales: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_sales"} */

      describe extended `hive_metastore`.`saleslt`.`dim_sales`
  
[0m23:50:12.449869 [debug] [Thread-1 (]: SQL status: OK in 0.3100000023841858 seconds
[0m23:50:12.458643 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_dbt_spark.dim_sales"
[0m23:50:12.459643 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: get_thread_connection: sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.dim_sales, idle: 0.33632946014404297s, acqrelcnt: 1, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843012.123314
[0m23:50:12.460643 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: idle check connection: sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.dim_sales, idle: 0.3373293876647949s, acqrelcnt: 1, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843012.123314
[0m23:50:12.461642 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.dim_sales"
[0m23:50:12.461642 [debug] [Thread-1 (]: On model.medallion_dbt_spark.dim_sales: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.dim_sales"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`dim_sales`
      
      
    using delta
      
      
      
      
      
    location '/mnt/silver/customer/dim_sales'
      
      
      as
      
with product_snapshot as (
    select 
        ProductID,
        Name,
        ProductNumber,
        Color,
        StandardCost,
        ListPrice,
        Size,
        Weight,
        SellStartDate,
        SellEndDate,
        DiscontinuedDate,
        ThumbNailPhoto,
        ThumbnailPhotoFileName
    from `hive_metastore`.`saleslt`.`product` 
),
salesorderdetail_snapshot as (
    select 
        SalesOrderID,
        SalesOrderDetailID,
        OrderQty,
        ProductID,
        UnitPrice,
        UnitPriceDiscount,
        LineTotal
    from `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` 
),
salesorderheader_snapshot as (
    select 
        SalesOrderID,
        RevisionNumber,
        OrderDate,
        DueDate,
        ShipDate,
        Status,
        OnlineOrderFlag,
        SalesOrderNumber,
        PurchaseOrderNumber,
        AccountNumber,
        CustomerID,
        ShipToAddressID,
        BillToAddressID,
        ShipMethod,
        CreditCardApprovalCode,
        SubTotal,
        TaxAmt,
        Freight,
        TotalDue,
        Comment,
        row_number() over (partition by SalesOrderID order by SalesOrderID) as row_num
    from `hive_metastore`.`saleslt`.`salesorderheader` 
),
transformed as (
    select
    product_snapshot.Name,
    product_snapshot.ProductNumber,
    product_snapshot.Color,
    product_snapshot.StandardCost,
    product_snapshot.ListPrice,
    product_snapshot.Size,
    product_snapshot.Weight,
    product_snapshot.SellStartDate,
    product_snapshot.SellEndDate,
    product_snapshot.DiscontinuedDate,
    product_snapshot.ThumbNailPhoto,
    product_snapshot.ThumbnailPhotoFileName,
    salesorderdetail_snapshot.SalesOrderID,
    salesorderdetail_snapshot.SalesOrderDetailID,
    salesorderdetail_snapshot.OrderQty,
    salesorderdetail_snapshot.ProductID,
    salesorderdetail_snapshot.UnitPrice,
    salesorderdetail_snapshot.UnitPriceDiscount,
    salesorderdetail_snapshot.LineTotal,
    salesorderheader_snapshot.RevisionNumber,
    salesorderheader_snapshot.OrderDate,
    salesorderheader_snapshot.DueDate,
    salesorderheader_snapshot.ShipDate,
    salesorderheader_snapshot.Status,
    salesorderheader_snapshot.OnlineOrderFlag,
    salesorderheader_snapshot.SalesOrderNumber,
    salesorderheader_snapshot.PurchaseOrderNumber,
    salesorderheader_snapshot.AccountNumber,
    salesorderheader_snapshot.CustomerID,
    salesorderheader_snapshot.BillToAddressID,
    salesorderheader_snapshot.ShipToAddressID,
    salesorderheader_snapshot.ShipMethod,
    salesorderheader_snapshot.CreditCardApprovalCode,
    salesorderheader_snapshot.SubTotal,
    salesorderheader_snapshot.TaxAmt,
    salesorderheader_snapshot.Freight,
    salesorderheader_snapshot.TotalDue,
    salesorderheader_snapshot.Comment
    from product_snapshot
    left join salesorderdetail_snapshot on product_snapshot.ProductID = salesorderdetail_snapshot.ProductID
    left join salesorderheader_snapshot on salesorderdetail_snapshot.SalesOrderID = salesorderheader_snapshot.SalesOrderID
)

select * from transformed
  
[0m23:50:15.847565 [debug] [Thread-1 (]: SQL status: OK in 3.380000114440918 seconds
[0m23:50:15.851938 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_sales (execute): 23:50:12.137751 => 23:50:15.851938
[0m23:50:15.853446 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: _release sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.dim_sales, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843015.8519382
[0m23:50:15.854457 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: _release sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.dim_sales, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843015.8544571
[0m23:50:15.855501 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7341cc4d-b3f5-4c76-96fc-f1e18406ce59', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BE42B46A50>]}
[0m23:50:15.855501 [info ] [Thread-1 (]: 3 of 5 OK created sql table model saleslt.dim_sales ............................ [[32mOK[0m in 3.73s]
[0m23:50:15.857768 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.dim_sales
[0m23:50:15.857768 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.my_first_dbt_model
[0m23:50:15.858826 [info ] [Thread-1 (]: 4 of 5 START sql table model saleslt.my_first_dbt_model ........................ [RUN]
[0m23:50:15.859771 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: idle check connection: sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.dim_sales, idle: 0.005314826965332031s, acqrelcnt: 0, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843015.8544571
[0m23:50:15.860850 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.dim_sales, now model.medallion_dbt_spark.my_first_dbt_model)
[0m23:50:15.861813 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: reusing connection model.medallion_dbt_spark.dim_sales sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.006392955780029297s, acqrelcnt: 0, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843015.8544571
[0m23:50:15.861813 [debug] [Thread-1 (]: Databricks adapter: On thread (19340, 15028): `hive_metastore`.`saleslt`.`my_first_dbt_model` using default compute resource.
[0m23:50:15.861813 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: _acquire sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.007356405258178711s, acqrelcnt: 1, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843015.8544571
[0m23:50:15.863319 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.my_first_dbt_model
[0m23:50:15.867323 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.my_first_dbt_model"
[0m23:50:15.868327 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.my_first_dbt_model (compile): 23:50:15.863319 => 23:50:15.868327
[0m23:50:15.869326 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.my_first_dbt_model
[0m23:50:15.875319 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: get_thread_connection: sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.020862579345703125s, acqrelcnt: 1, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843015.8544571
[0m23:50:15.876312 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: idle check connection: sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.020862579345703125s, acqrelcnt: 1, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843015.8544571
[0m23:50:15.876312 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.my_first_dbt_model"
[0m23:50:15.877349 [debug] [Thread-1 (]: On model.medallion_dbt_spark.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.my_first_dbt_model"} */

      describe extended `hive_metastore`.`saleslt`.`my_first_dbt_model`
  
[0m23:50:16.079846 [debug] [Thread-1 (]: SQL status: OK in 0.20000000298023224 seconds
[0m23:50:16.089282 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_dbt_spark.my_first_dbt_model"
[0m23:50:16.090204 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: get_thread_connection: sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.2357478141784668s, acqrelcnt: 1, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843015.8544571
[0m23:50:16.091510 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: idle check connection: sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.23705291748046875s, acqrelcnt: 1, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843015.8544571
[0m23:50:16.091510 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.my_first_dbt_model"
[0m23:50:16.092525 [debug] [Thread-1 (]: On model.medallion_dbt_spark.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.my_first_dbt_model"} */

  
    
        create or replace table `hive_metastore`.`saleslt`.`my_first_dbt_model`
      
      
    using delta
      
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m23:50:18.406626 [debug] [Thread-1 (]: SQL status: OK in 2.309999942779541 seconds
[0m23:50:18.410936 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.my_first_dbt_model (execute): 23:50:15.870355 => 23:50:18.410936
[0m23:50:18.412329 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: _release sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843018.4123292
[0m23:50:18.413436 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: _release sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843018.4134362
[0m23:50:18.414369 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7341cc4d-b3f5-4c76-96fc-f1e18406ce59', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BE430F4A10>]}
[0m23:50:18.415353 [info ] [Thread-1 (]: 4 of 5 OK created sql table model saleslt.my_first_dbt_model ................... [[32mOK[0m in 2.55s]
[0m23:50:18.415855 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.my_first_dbt_model
[0m23:50:18.416909 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.my_second_dbt_model
[0m23:50:18.417938 [info ] [Thread-1 (]: 5 of 5 START sql view model saleslt.my_second_dbt_model ........................ [RUN]
[0m23:50:18.423196 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: idle check connection: sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.009760141372680664s, acqrelcnt: 0, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843018.4134362
[0m23:50:18.424140 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.my_first_dbt_model, now model.medallion_dbt_spark.my_second_dbt_model)
[0m23:50:18.425142 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: reusing connection model.medallion_dbt_spark.my_first_dbt_model sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.011705875396728516s, acqrelcnt: 0, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843018.4134362
[0m23:50:18.426157 [debug] [Thread-1 (]: Databricks adapter: On thread (19340, 15028): `hive_metastore`.`saleslt`.`my_second_dbt_model` using default compute resource.
[0m23:50:18.427175 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: _acquire sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.012721061706542969s, acqrelcnt: 1, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843018.4134362
[0m23:50:18.427175 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.my_second_dbt_model
[0m23:50:18.431774 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.my_second_dbt_model"
[0m23:50:18.433557 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.my_second_dbt_model (compile): 23:50:18.427175 => 23:50:18.433557
[0m23:50:18.434576 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.my_second_dbt_model
[0m23:50:18.459807 [debug] [Thread-1 (]: Writing runtime sql for node "model.medallion_dbt_spark.my_second_dbt_model"
[0m23:50:18.461783 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: get_thread_connection: sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.04737257957458496s, acqrelcnt: 1, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843018.4134362
[0m23:50:18.461783 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: idle check connection: sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.04834771156311035s, acqrelcnt: 1, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843018.4134362
[0m23:50:18.461783 [debug] [Thread-1 (]: Using databricks connection "model.medallion_dbt_spark.my_second_dbt_model"
[0m23:50:18.463298 [debug] [Thread-1 (]: On model.medallion_dbt_spark.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "model.medallion_dbt_spark.my_second_dbt_model"} */
create or replace view `hive_metastore`.`saleslt`.`my_second_dbt_model`
  
  
  
  as
    -- Use the `ref` function to select from other models

select *
from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id = 1

[0m23:50:18.908368 [debug] [Thread-1 (]: SQL status: OK in 0.4399999976158142 seconds
[0m23:50:18.913468 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.my_second_dbt_model (execute): 23:50:18.434576 => 23:50:18.912372
[0m23:50:18.913468 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: _release sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843018.9134684
[0m23:50:18.915670 [debug] [Thread-1 (]: Databricks adapter: conn: 3016190428112: _release sess: c6830406-bc09-419c-812c-f5962986fe2a, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19340, 15028), cmpt: ``, lut: 1707843018.9156709
[0m23:50:18.916176 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7341cc4d-b3f5-4c76-96fc-f1e18406ce59', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BE303A5190>]}
[0m23:50:18.917183 [info ] [Thread-1 (]: 5 of 5 OK created sql view model saleslt.my_second_dbt_model ................... [[32mOK[0m in 0.49s]
[0m23:50:18.922419 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.my_second_dbt_model
[0m23:50:18.924792 [debug] [MainThread]: Databricks adapter: conn: 3016188336208: idle check connection: sess: None, name: master, idle: 15.537158489227295s, acqrelcnt: 0, lang: None, thrd: (19340, 780), cmpt: ``, lut: 1707843003.387518
[0m23:50:18.924792 [debug] [MainThread]: Databricks adapter: conn: 3016188336208: reusing connection master sess: None, name: master, idle: 15.537274837493896s, acqrelcnt: 0, lang: None, thrd: (19340, 780), cmpt: ``, lut: 1707843003.387518
[0m23:50:18.925841 [debug] [MainThread]: Databricks adapter: Thread (19340, 780) using default compute resource.
[0m23:50:18.926794 [debug] [MainThread]: Databricks adapter: conn: 3016188336208: _acquire sess: None, name: master, idle: 15.538323402404785s, acqrelcnt: 1, lang: None, thrd: (19340, 780), cmpt: ``, lut: 1707843003.387518
[0m23:50:18.926794 [debug] [MainThread]: Databricks adapter: conn: 3016188336208: get_thread_connection: sess: None, name: master, idle: 15.539276123046875s, acqrelcnt: 1, lang: None, thrd: (19340, 780), cmpt: ``, lut: 1707843003.387518
[0m23:50:18.927859 [debug] [MainThread]: Databricks adapter: conn: 3016188336208: idle check connection: sess: None, name: master, idle: 15.539276123046875s, acqrelcnt: 1, lang: None, thrd: (19340, 780), cmpt: ``, lut: 1707843003.387518
[0m23:50:18.927859 [debug] [MainThread]: On master: ROLLBACK
[0m23:50:18.928796 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:50:19.154054 [debug] [MainThread]: Databricks adapter: conn: 3016188336208: session opened sess: 9964dff9-73ed-40b0-8e9c-71e037dee719, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (19340, 780), cmpt: ``, lut: 1707843019.153155
[0m23:50:19.155067 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:50:19.157173 [debug] [MainThread]: Databricks adapter: conn: 3016188336208: get_thread_connection: sess: 9964dff9-73ed-40b0-8e9c-71e037dee719, name: master, idle: 0.0029044151306152344s, acqrelcnt: 1, lang: None, thrd: (19340, 780), cmpt: ``, lut: 1707843019.153155
[0m23:50:19.158470 [debug] [MainThread]: Databricks adapter: conn: 3016188336208: idle check connection: sess: 9964dff9-73ed-40b0-8e9c-71e037dee719, name: master, idle: 0.004230022430419922s, acqrelcnt: 1, lang: None, thrd: (19340, 780), cmpt: ``, lut: 1707843019.153155
[0m23:50:19.159432 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:50:19.160425 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:50:19.162439 [debug] [MainThread]: Databricks adapter: conn: 3016188336208: _release sess: 9964dff9-73ed-40b0-8e9c-71e037dee719, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (19340, 780), cmpt: ``, lut: 1707843019.1613858
[0m23:50:19.165073 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:50:19.166158 [debug] [MainThread]: On master: ROLLBACK
[0m23:50:19.167165 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:50:19.168160 [debug] [MainThread]: On master: Close
[0m23:50:19.268369 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m23:50:19.269288 [debug] [MainThread]: On list_hive_metastore: Close
[0m23:50:19.341472 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m23:50:19.342486 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m23:50:19.343482 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:50:19.343991 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m23:50:19.431556 [debug] [MainThread]: Connection 'model.medallion_dbt_spark.my_second_dbt_model' was properly closed.
[0m23:50:19.432958 [debug] [MainThread]: On model.medallion_dbt_spark.my_second_dbt_model: ROLLBACK
[0m23:50:19.433620 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:50:19.434579 [debug] [MainThread]: On model.medallion_dbt_spark.my_second_dbt_model: Close
[0m23:50:19.517569 [info ] [MainThread]: 
[0m23:50:19.518583 [info ] [MainThread]: Finished running 4 table models, 1 view model in 0 hours 0 minutes and 17.98 seconds (17.98s).
[0m23:50:19.521670 [debug] [MainThread]: Command end result
[0m23:50:19.536035 [info ] [MainThread]: 
[0m23:50:19.537052 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:50:19.538054 [info ] [MainThread]: 
[0m23:50:19.538964 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m23:50:19.541267 [debug] [MainThread]: Command `dbt run` succeeded at 23:50:19.541267 after 19.81 seconds
[0m23:50:19.542359 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BE303B8910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BE2957AF10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BE294F0D90>]}
[0m23:50:19.543323 [debug] [MainThread]: Flushing usage events
[0m23:50:37.080330 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8BE297750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8BE2E3CD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8BDC3FF90>]}


============================== 23:50:37.083532 | 9771b7bd-830d-4126-b52a-ccd5b2209ce8 ==============================
[0m23:50:37.083532 [info ] [MainThread]: Running with dbt=1.7.7
[0m23:50:37.086056 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt test', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m23:50:38.585762 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9771b7bd-830d-4126-b52a-ccd5b2209ce8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8D040D950>]}
[0m23:50:38.664406 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9771b7bd-830d-4126-b52a-ccd5b2209ce8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8BDC3D490>]}
[0m23:50:38.665411 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m23:50:38.675632 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m23:50:38.763356 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m23:50:38.764357 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m23:50:38.770999 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9771b7bd-830d-4126-b52a-ccd5b2209ce8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8D05DF090>]}
[0m23:50:38.789391 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9771b7bd-830d-4126-b52a-ccd5b2209ce8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8D057BCD0>]}
[0m23:50:38.789391 [info ] [MainThread]: Found 5 models, 7 snapshots, 8 tests, 9 sources, 0 exposures, 0 metrics, 535 macros, 0 groups, 0 semantic models
[0m23:50:38.790900 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9771b7bd-830d-4126-b52a-ccd5b2209ce8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8D04EC610>]}
[0m23:50:38.792907 [info ] [MainThread]: 
[0m23:50:38.795421 [debug] [MainThread]: Databricks adapter: conn: 1962000497296: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (11768, 13732), cmpt: ``, lut: None
[0m23:50:38.796429 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:50:38.796429 [debug] [MainThread]: Databricks adapter: Thread (11768, 13732) using default compute resource.
[0m23:50:38.796429 [debug] [MainThread]: Databricks adapter: conn: 1962000497296: _acquire sess: None, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (11768, 13732), cmpt: ``, lut: 1707843038.7964296
[0m23:50:38.798438 [debug] [ThreadPool]: Databricks adapter: conn: 1962000485008: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_saleslt, idle: 0s, acqrelcnt: 0, lang: None, thrd: (11768, 1096), cmpt: ``, lut: None
[0m23:50:38.799679 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m23:50:38.799679 [debug] [ThreadPool]: Databricks adapter: Thread (11768, 1096) using default compute resource.
[0m23:50:38.800712 [debug] [ThreadPool]: Databricks adapter: conn: 1962000485008: _acquire sess: None, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (11768, 1096), cmpt: ``, lut: 1707843038.79968
[0m23:50:38.803769 [debug] [ThreadPool]: Databricks adapter: conn: 1962000485008: get_thread_connection: sess: None, name: list_hive_metastore_saleslt, idle: 0.0040891170501708984s, acqrelcnt: 1, lang: None, thrd: (11768, 1096), cmpt: ``, lut: 1707843038.79968
[0m23:50:38.805349 [debug] [ThreadPool]: Databricks adapter: conn: 1962000485008: idle check connection: sess: None, name: list_hive_metastore_saleslt, idle: 0.0040891170501708984s, acqrelcnt: 1, lang: None, thrd: (11768, 1096), cmpt: ``, lut: 1707843038.79968
[0m23:50:38.805349 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:50:38.805349 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m23:50:38.806357 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:50:39.070875 [debug] [ThreadPool]: Databricks adapter: conn: 1962000485008: session opened sess: 049cddce-4147-4e3b-bfee-e5c94d2ba8ae, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (11768, 1096), cmpt: ``, lut: 1707843039.070876
[0m23:50:39.221284 [debug] [ThreadPool]: SQL status: OK in 0.4099999964237213 seconds
[0m23:50:39.238382 [debug] [ThreadPool]: Databricks adapter: conn: 1962000485008: get_thread_connection: sess: 049cddce-4147-4e3b-bfee-e5c94d2ba8ae, name: list_hive_metastore_saleslt, idle: 0.16750645637512207s, acqrelcnt: 1, lang: None, thrd: (11768, 1096), cmpt: ``, lut: 1707843039.070876
[0m23:50:39.239335 [debug] [ThreadPool]: Databricks adapter: conn: 1962000485008: idle check connection: sess: 049cddce-4147-4e3b-bfee-e5c94d2ba8ae, name: list_hive_metastore_saleslt, idle: 0.16845917701721191s, acqrelcnt: 1, lang: None, thrd: (11768, 1096), cmpt: ``, lut: 1707843039.070876
[0m23:50:39.239335 [debug] [ThreadPool]: Databricks adapter: conn: 1962000485008: get_thread_connection: sess: 049cddce-4147-4e3b-bfee-e5c94d2ba8ae, name: list_hive_metastore_saleslt, idle: 0.16845917701721191s, acqrelcnt: 1, lang: None, thrd: (11768, 1096), cmpt: ``, lut: 1707843039.070876
[0m23:50:39.240298 [debug] [ThreadPool]: Databricks adapter: conn: 1962000485008: idle check connection: sess: 049cddce-4147-4e3b-bfee-e5c94d2ba8ae, name: list_hive_metastore_saleslt, idle: 0.16942262649536133s, acqrelcnt: 1, lang: None, thrd: (11768, 1096), cmpt: ``, lut: 1707843039.070876
[0m23:50:39.240298 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:50:39.241330 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:50:39.241330 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m23:50:39.409500 [debug] [ThreadPool]: SQL status: OK in 0.17000000178813934 seconds
[0m23:50:39.417612 [debug] [ThreadPool]: Databricks adapter: conn: 1962000485008: get_thread_connection: sess: 049cddce-4147-4e3b-bfee-e5c94d2ba8ae, name: list_hive_metastore_saleslt, idle: 0.34673643112182617s, acqrelcnt: 1, lang: None, thrd: (11768, 1096), cmpt: ``, lut: 1707843039.070876
[0m23:50:39.418610 [debug] [ThreadPool]: Databricks adapter: conn: 1962000485008: idle check connection: sess: 049cddce-4147-4e3b-bfee-e5c94d2ba8ae, name: list_hive_metastore_saleslt, idle: 0.3477339744567871s, acqrelcnt: 1, lang: None, thrd: (11768, 1096), cmpt: ``, lut: 1707843039.070876
[0m23:50:39.419531 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:50:39.419531 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m23:50:39.588826 [debug] [ThreadPool]: SQL status: OK in 0.17000000178813934 seconds
[0m23:50:39.591813 [debug] [ThreadPool]: Databricks adapter: conn: 1962000485008: _release sess: 049cddce-4147-4e3b-bfee-e5c94d2ba8ae, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (11768, 1096), cmpt: ``, lut: 1707843039.5918136
[0m23:50:39.592740 [debug] [ThreadPool]: Databricks adapter: conn: 1962000485008: idle check connection: sess: 049cddce-4147-4e3b-bfee-e5c94d2ba8ae, name: list_hive_metastore_saleslt, idle: 0.000926971435546875s, acqrelcnt: 0, lang: None, thrd: (11768, 1096), cmpt: ``, lut: 1707843039.5918136
[0m23:50:39.597360 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m23:50:39.597360 [debug] [ThreadPool]: Databricks adapter: conn: 1962000485008: reusing connection list_hive_metastore_saleslt sess: 049cddce-4147-4e3b-bfee-e5c94d2ba8ae, name: list_hive_metastore_snapshots, idle: 0.00554656982421875s, acqrelcnt: 0, lang: None, thrd: (11768, 1096), cmpt: ``, lut: 1707843039.5918136
[0m23:50:39.598348 [debug] [ThreadPool]: Databricks adapter: Thread (11768, 1096) using default compute resource.
[0m23:50:39.598348 [debug] [ThreadPool]: Databricks adapter: conn: 1962000485008: _acquire sess: 049cddce-4147-4e3b-bfee-e5c94d2ba8ae, name: list_hive_metastore_snapshots, idle: 0.006535053253173828s, acqrelcnt: 1, lang: None, thrd: (11768, 1096), cmpt: ``, lut: 1707843039.5918136
[0m23:50:39.600701 [debug] [ThreadPool]: Databricks adapter: conn: 1962000485008: get_thread_connection: sess: 049cddce-4147-4e3b-bfee-e5c94d2ba8ae, name: list_hive_metastore_snapshots, idle: 0.008887529373168945s, acqrelcnt: 1, lang: None, thrd: (11768, 1096), cmpt: ``, lut: 1707843039.5918136
[0m23:50:39.601706 [debug] [ThreadPool]: Databricks adapter: conn: 1962000485008: idle check connection: sess: 049cddce-4147-4e3b-bfee-e5c94d2ba8ae, name: list_hive_metastore_snapshots, idle: 0.009892940521240234s, acqrelcnt: 1, lang: None, thrd: (11768, 1096), cmpt: ``, lut: 1707843039.5918136
[0m23:50:39.601706 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m23:50:39.602707 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m23:50:39.711161 [debug] [ThreadPool]: SQL status: OK in 0.10999999940395355 seconds
[0m23:50:39.716961 [debug] [ThreadPool]: Databricks adapter: conn: 1962000485008: get_thread_connection: sess: 049cddce-4147-4e3b-bfee-e5c94d2ba8ae, name: list_hive_metastore_snapshots, idle: 0.12514829635620117s, acqrelcnt: 1, lang: None, thrd: (11768, 1096), cmpt: ``, lut: 1707843039.5918136
[0m23:50:39.717877 [debug] [ThreadPool]: Databricks adapter: conn: 1962000485008: idle check connection: sess: 049cddce-4147-4e3b-bfee-e5c94d2ba8ae, name: list_hive_metastore_snapshots, idle: 0.12606358528137207s, acqrelcnt: 1, lang: None, thrd: (11768, 1096), cmpt: ``, lut: 1707843039.5918136
[0m23:50:39.717877 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m23:50:39.718712 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m23:50:39.858809 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m23:50:39.865468 [debug] [ThreadPool]: Databricks adapter: conn: 1962000485008: get_thread_connection: sess: 049cddce-4147-4e3b-bfee-e5c94d2ba8ae, name: list_hive_metastore_snapshots, idle: 0.2736546993255615s, acqrelcnt: 1, lang: None, thrd: (11768, 1096), cmpt: ``, lut: 1707843039.5918136
[0m23:50:39.866705 [debug] [ThreadPool]: Databricks adapter: conn: 1962000485008: idle check connection: sess: 049cddce-4147-4e3b-bfee-e5c94d2ba8ae, name: list_hive_metastore_snapshots, idle: 0.27489185333251953s, acqrelcnt: 1, lang: None, thrd: (11768, 1096), cmpt: ``, lut: 1707843039.5918136
[0m23:50:39.866705 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m23:50:39.867712 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m23:50:40.026297 [debug] [ThreadPool]: SQL status: OK in 0.1599999964237213 seconds
[0m23:50:40.030301 [debug] [ThreadPool]: Databricks adapter: conn: 1962000485008: _release sess: 049cddce-4147-4e3b-bfee-e5c94d2ba8ae, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (11768, 1096), cmpt: ``, lut: 1707843040.029296
[0m23:50:40.032425 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9771b7bd-830d-4126-b52a-ccd5b2209ce8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8D05A37D0>]}
[0m23:50:40.033411 [debug] [MainThread]: Databricks adapter: conn: 1962000497296: get_thread_connection: sess: None, name: master, idle: 1.2369813919067383s, acqrelcnt: 1, lang: None, thrd: (11768, 13732), cmpt: ``, lut: 1707843038.7964296
[0m23:50:40.034547 [debug] [MainThread]: Databricks adapter: conn: 1962000497296: idle check connection: sess: None, name: master, idle: 1.2379841804504395s, acqrelcnt: 1, lang: None, thrd: (11768, 13732), cmpt: ``, lut: 1707843038.7964296
[0m23:50:40.034547 [debug] [MainThread]: Databricks adapter: conn: 1962000497296: get_thread_connection: sess: None, name: master, idle: 1.2381181716918945s, acqrelcnt: 1, lang: None, thrd: (11768, 13732), cmpt: ``, lut: 1707843038.7964296
[0m23:50:40.035556 [debug] [MainThread]: Databricks adapter: conn: 1962000497296: idle check connection: sess: None, name: master, idle: 1.2381181716918945s, acqrelcnt: 1, lang: None, thrd: (11768, 13732), cmpt: ``, lut: 1707843038.7964296
[0m23:50:40.036177 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:50:40.036177 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:50:40.036177 [debug] [MainThread]: Databricks adapter: conn: 1962000497296: _release sess: None, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (11768, 13732), cmpt: ``, lut: 1707843040.0361776
[0m23:50:40.037271 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:50:40.038234 [info ] [MainThread]: 
[0m23:50:40.041840 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m23:50:40.042912 [info ] [Thread-1 (]: 1 of 8 START test not_null_dim_product_product_name ............................ [RUN]
[0m23:50:40.045178 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: Creating DatabricksDBTConnection sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0s, acqrelcnt: 0, lang: None, thrd: (11768, 2792), cmpt: ``, lut: None
[0m23:50:40.046188 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5'
[0m23:50:40.047203 [debug] [Thread-1 (]: Databricks adapter: On thread (11768, 2792): None using default compute resource.
[0m23:50:40.048693 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: _acquire sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843040.048186
[0m23:50:40.048693 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m23:50:40.070237 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m23:50:40.071708 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (compile): 23:50:40.049706 => 23:50:40.071708
[0m23:50:40.072716 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m23:50:40.089753 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m23:50:40.091672 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.042574167251586914s, acqrelcnt: 1, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843040.048186
[0m23:50:40.091672 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.04348635673522949s, acqrelcnt: 1, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843040.048186
[0m23:50:40.092699 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.04451394081115723s, acqrelcnt: 1, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843040.048186
[0m23:50:40.092699 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.04451394081115723s, acqrelcnt: 1, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843040.048186
[0m23:50:40.093762 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m23:50:40.094455 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m23:50:40.096210 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_name
from `hive_metastore`.`saleslt`.`dim_product`
where product_name is null



      
    ) dbt_internal_test
[0m23:50:40.097207 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:50:40.331156 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: session opened sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843040.3311567
[0m23:50:40.817011 [debug] [Thread-1 (]: SQL status: OK in 0.7200000286102295 seconds
[0m23:50:40.827419 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (execute): 23:50:40.072716 => 23:50:40.826420
[0m23:50:40.827419 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: _release sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843040.82742
[0m23:50:40.828425 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: _release sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843040.8284252
[0m23:50:40.829425 [info ] [Thread-1 (]: 1 of 8 PASS not_null_dim_product_product_name .................................. [[32mPASS[0m in 0.79s]
[0m23:50:40.831432 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m23:50:40.831698 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m23:50:40.831698 [info ] [Thread-1 (]: 2 of 8 START test not_null_dim_product_product_sk .............................. [RUN]
[0m23:50:40.833705 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: idle check connection: sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.004275321960449219s, acqrelcnt: 0, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843040.8284252
[0m23:50:40.833705 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, now test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417)
[0m23:50:40.833705 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.005280733108520508s, acqrelcnt: 0, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843040.8284252
[0m23:50:40.833705 [debug] [Thread-1 (]: Databricks adapter: On thread (11768, 2792): None using default compute resource.
[0m23:50:40.835280 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: _acquire sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.006855487823486328s, acqrelcnt: 1, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843040.8284252
[0m23:50:40.835280 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m23:50:40.840228 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m23:50:40.842028 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (compile): 23:50:40.836230 => 23:50:40.842028
[0m23:50:40.842028 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m23:50:40.845622 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m23:50:40.846555 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: get_thread_connection: sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.01813030242919922s, acqrelcnt: 1, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843040.8284252
[0m23:50:40.847553 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: idle check connection: sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.01813030242919922s, acqrelcnt: 1, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843040.8284252
[0m23:50:40.847553 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m23:50:40.847553 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_sk
from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is null



      
    ) dbt_internal_test
[0m23:50:41.112118 [debug] [Thread-1 (]: SQL status: OK in 0.25999999046325684 seconds
[0m23:50:41.117880 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (execute): 23:50:40.843034 => 23:50:41.117880
[0m23:50:41.118881 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: _release sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843041.1188815
[0m23:50:41.119869 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: _release sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843041.1198697
[0m23:50:41.119869 [info ] [Thread-1 (]: 2 of 8 PASS not_null_dim_product_product_sk .................................... [[32mPASS[0m in 0.29s]
[0m23:50:41.121818 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m23:50:41.121818 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m23:50:41.122843 [info ] [Thread-1 (]: 3 of 8 START test not_null_dim_product_sellstartdate ........................... [RUN]
[0m23:50:41.123873 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: idle check connection: sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.004004001617431641s, acqrelcnt: 0, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843041.1198697
[0m23:50:41.123873 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, now test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546)
[0m23:50:41.125501 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.004004001617431641s, acqrelcnt: 0, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843041.1198697
[0m23:50:41.125714 [debug] [Thread-1 (]: Databricks adapter: On thread (11768, 2792): None using default compute resource.
[0m23:50:41.125714 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: _acquire sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0058443546295166016s, acqrelcnt: 1, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843041.1198697
[0m23:50:41.126713 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m23:50:41.130784 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m23:50:41.132175 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (compile): 23:50:41.126713 => 23:50:41.132175
[0m23:50:41.133172 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m23:50:41.135257 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m23:50:41.135858 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: get_thread_connection: sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.01598834991455078s, acqrelcnt: 1, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843041.1198697
[0m23:50:41.136911 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: idle check connection: sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.017041444778442383s, acqrelcnt: 1, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843041.1198697
[0m23:50:41.136911 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m23:50:41.137866 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select sellstartdate
from `hive_metastore`.`saleslt`.`dim_product`
where sellstartdate is null



      
    ) dbt_internal_test
[0m23:50:41.510980 [debug] [Thread-1 (]: SQL status: OK in 0.3700000047683716 seconds
[0m23:50:41.515723 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (execute): 23:50:41.133172 => 23:50:41.515578
[0m23:50:41.516625 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: _release sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843041.5166252
[0m23:50:41.517687 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: _release sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843041.5176876
[0m23:50:41.518686 [info ] [Thread-1 (]: 3 of 8 PASS not_null_dim_product_sellstartdate ................................. [[32mPASS[0m in 0.39s]
[0m23:50:41.519593 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m23:50:41.520677 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m23:50:41.521585 [info ] [Thread-1 (]: 4 of 8 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m23:50:41.522622 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: idle check connection: sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.004935264587402344s, acqrelcnt: 0, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843041.5176876
[0m23:50:41.522622 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, now test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710)
[0m23:50:41.523676 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: reusing connection test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.005988359451293945s, acqrelcnt: 0, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843041.5176876
[0m23:50:41.523676 [debug] [Thread-1 (]: Databricks adapter: On thread (11768, 2792): None using default compute resource.
[0m23:50:41.523676 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: _acquire sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.005988359451293945s, acqrelcnt: 1, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843041.5176876
[0m23:50:41.525335 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m23:50:41.529481 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m23:50:41.530487 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 23:50:41.525482 => 23:50:41.530487
[0m23:50:41.531744 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m23:50:41.533830 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m23:50:41.537431 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: get_thread_connection: sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.018736600875854492s, acqrelcnt: 1, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843041.5176876
[0m23:50:41.537431 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: idle check connection: sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.019743680953979492s, acqrelcnt: 1, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843041.5176876
[0m23:50:41.538444 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m23:50:41.539444 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m23:50:41.877979 [debug] [Thread-1 (]: SQL status: OK in 0.3400000035762787 seconds
[0m23:50:41.883338 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 23:50:41.531744 => 23:50:41.883338
[0m23:50:41.884344 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: _release sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843041.8843446
[0m23:50:41.885864 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: _release sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843041.885347
[0m23:50:41.885864 [error] [Thread-1 (]: 4 of 8 FAIL 1 not_null_my_first_dbt_model_id ................................... [[31mFAIL 1[0m in 0.36s]
[0m23:50:41.887875 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m23:50:41.887875 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m23:50:41.888875 [info ] [Thread-1 (]: 5 of 8 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m23:50:41.889936 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: idle check connection: sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.00458979606628418s, acqrelcnt: 0, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843041.885347
[0m23:50:41.890950 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, now test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778)
[0m23:50:41.890950 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: reusing connection test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0056035518646240234s, acqrelcnt: 0, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843041.885347
[0m23:50:41.891887 [debug] [Thread-1 (]: Databricks adapter: On thread (11768, 2792): None using default compute resource.
[0m23:50:41.891887 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: _acquire sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.006540536880493164s, acqrelcnt: 1, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843041.885347
[0m23:50:41.891887 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m23:50:41.896553 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m23:50:41.898566 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (compile): 23:50:41.892922 => 23:50:41.897557
[0m23:50:41.898566 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m23:50:41.900983 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m23:50:41.901984 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: get_thread_connection: sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.016637325286865234s, acqrelcnt: 1, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843041.885347
[0m23:50:41.902895 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: idle check connection: sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.01754903793334961s, acqrelcnt: 1, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843041.885347
[0m23:50:41.902895 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m23:50:41.903935 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m23:50:42.270005 [debug] [Thread-1 (]: SQL status: OK in 0.3700000047683716 seconds
[0m23:50:42.276961 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (execute): 23:50:41.898566 => 23:50:42.275951
[0m23:50:42.278959 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: _release sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843042.2780497
[0m23:50:42.282336 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: _release sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843042.2810488
[0m23:50:42.283425 [info ] [Thread-1 (]: 5 of 8 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.39s]
[0m23:50:42.285339 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m23:50:42.286802 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m23:50:42.287802 [info ] [Thread-1 (]: 6 of 8 START test unique_dim_product_product_sk ................................ [RUN]
[0m23:50:42.288803 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: idle check connection: sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.007754325866699219s, acqrelcnt: 0, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843042.2810488
[0m23:50:42.289899 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, now test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c)
[0m23:50:42.289899 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: reusing connection test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.008850336074829102s, acqrelcnt: 0, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843042.2810488
[0m23:50:42.290834 [debug] [Thread-1 (]: Databricks adapter: On thread (11768, 2792): None using default compute resource.
[0m23:50:42.290834 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: _acquire sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.00978541374206543s, acqrelcnt: 1, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843042.2810488
[0m23:50:42.291844 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m23:50:42.300488 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m23:50:42.307135 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (compile): 23:50:42.291844 => 23:50:42.306133
[0m23:50:42.309148 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m23:50:42.318481 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m23:50:42.320520 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: get_thread_connection: sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.03947162628173828s, acqrelcnt: 1, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843042.2810488
[0m23:50:42.321516 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: idle check connection: sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.04046750068664551s, acqrelcnt: 1, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843042.2810488
[0m23:50:42.322493 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m23:50:42.323491 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    product_sk as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is not null
group by product_sk
having count(*) > 1



      
    ) dbt_internal_test
[0m23:50:42.781399 [debug] [Thread-1 (]: SQL status: OK in 0.46000000834465027 seconds
[0m23:50:42.787933 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (execute): 23:50:42.310146 => 23:50:42.787933
[0m23:50:42.789938 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: _release sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843042.7889345
[0m23:50:42.791937 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: _release sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843042.7919374
[0m23:50:42.792936 [info ] [Thread-1 (]: 6 of 8 PASS unique_dim_product_product_sk ...................................... [[32mPASS[0m in 0.50s]
[0m23:50:42.795606 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m23:50:42.796550 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m23:50:42.797545 [info ] [Thread-1 (]: 7 of 8 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m23:50:42.800871 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: idle check connection: sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.007833242416381836s, acqrelcnt: 0, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843042.7919374
[0m23:50:42.801859 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, now test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321)
[0m23:50:42.802785 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: reusing connection test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.010848045349121094s, acqrelcnt: 0, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843042.7919374
[0m23:50:42.803807 [debug] [Thread-1 (]: Databricks adapter: On thread (11768, 2792): None using default compute resource.
[0m23:50:42.805413 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: _acquire sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.013476371765136719s, acqrelcnt: 1, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843042.7919374
[0m23:50:42.806423 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m23:50:42.816412 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m23:50:42.818499 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (compile): 23:50:42.807795 => 23:50:42.817521
[0m23:50:42.820446 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m23:50:42.825466 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m23:50:42.826473 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: get_thread_connection: sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.034535884857177734s, acqrelcnt: 1, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843042.7919374
[0m23:50:42.827576 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: idle check connection: sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.034535884857177734s, acqrelcnt: 1, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843042.7919374
[0m23:50:42.827576 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m23:50:42.828504 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m23:50:43.170651 [debug] [Thread-1 (]: SQL status: OK in 0.3400000035762787 seconds
[0m23:50:43.175467 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (execute): 23:50:42.820446 => 23:50:43.175467
[0m23:50:43.177550 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: _release sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843043.1775503
[0m23:50:43.180551 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: _release sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843043.1794722
[0m23:50:43.181472 [info ] [Thread-1 (]: 7 of 8 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.38s]
[0m23:50:43.184516 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m23:50:43.185555 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m23:50:43.186951 [info ] [Thread-1 (]: 8 of 8 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m23:50:43.189876 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: idle check connection: sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.009455204010009766s, acqrelcnt: 0, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843043.1794722
[0m23:50:43.190957 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, now test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493)
[0m23:50:43.192955 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: reusing connection test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.012488842010498047s, acqrelcnt: 0, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843043.1794722
[0m23:50:43.193969 [debug] [Thread-1 (]: Databricks adapter: On thread (11768, 2792): None using default compute resource.
[0m23:50:43.195668 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: _acquire sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.01610589027404785s, acqrelcnt: 1, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843043.1794722
[0m23:50:43.197632 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m23:50:43.206106 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m23:50:43.208113 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (compile): 23:50:43.198595 => 23:50:43.207116
[0m23:50:43.208113 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m23:50:43.212116 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m23:50:43.213117 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: get_thread_connection: sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.033644676208496094s, acqrelcnt: 1, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843043.1794722
[0m23:50:43.213117 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: idle check connection: sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.033644676208496094s, acqrelcnt: 1, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843043.1794722
[0m23:50:43.214312 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m23:50:43.215380 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m23:50:43.617498 [debug] [Thread-1 (]: SQL status: OK in 0.4000000059604645 seconds
[0m23:50:43.624789 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (execute): 23:50:43.209115 => 23:50:43.623580
[0m23:50:43.625383 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: _release sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843043.6253831
[0m23:50:43.626305 [debug] [Thread-1 (]: Databricks adapter: conn: 1962002858768: _release sess: 56f1e5fc-1a5e-466e-876c-25c29fc448b5, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11768, 2792), cmpt: ``, lut: 1707843043.6263058
[0m23:50:43.627308 [info ] [Thread-1 (]: 8 of 8 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.44s]
[0m23:50:43.628776 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m23:50:43.632156 [debug] [MainThread]: Databricks adapter: conn: 1962000497296: idle check connection: sess: None, name: master, idle: 3.5959789752960205s, acqrelcnt: 0, lang: None, thrd: (11768, 13732), cmpt: ``, lut: 1707843040.0361776
[0m23:50:43.633170 [debug] [MainThread]: Databricks adapter: conn: 1962000497296: reusing connection master sess: None, name: master, idle: 3.596992254257202s, acqrelcnt: 0, lang: None, thrd: (11768, 13732), cmpt: ``, lut: 1707843040.0361776
[0m23:50:43.634165 [debug] [MainThread]: Databricks adapter: Thread (11768, 13732) using default compute resource.
[0m23:50:43.635166 [debug] [MainThread]: Databricks adapter: conn: 1962000497296: _acquire sess: None, name: master, idle: 3.5979878902435303s, acqrelcnt: 1, lang: None, thrd: (11768, 13732), cmpt: ``, lut: 1707843040.0361776
[0m23:50:43.635675 [debug] [MainThread]: Databricks adapter: conn: 1962000497296: get_thread_connection: sess: None, name: master, idle: 3.5994982719421387s, acqrelcnt: 1, lang: None, thrd: (11768, 13732), cmpt: ``, lut: 1707843040.0361776
[0m23:50:43.635675 [debug] [MainThread]: Databricks adapter: conn: 1962000497296: idle check connection: sess: None, name: master, idle: 3.5994982719421387s, acqrelcnt: 1, lang: None, thrd: (11768, 13732), cmpt: ``, lut: 1707843040.0361776
[0m23:50:43.636686 [debug] [MainThread]: On master: ROLLBACK
[0m23:50:43.636686 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:50:43.948244 [debug] [MainThread]: Databricks adapter: conn: 1962000497296: session opened sess: 2e0b1fba-049e-48e6-9f89-2a822fb618c1, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (11768, 13732), cmpt: ``, lut: 1707843043.9482448
[0m23:50:43.950442 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:50:43.951816 [debug] [MainThread]: Databricks adapter: conn: 1962000497296: get_thread_connection: sess: 2e0b1fba-049e-48e6-9f89-2a822fb618c1, name: master, idle: 0.0025479793548583984s, acqrelcnt: 1, lang: None, thrd: (11768, 13732), cmpt: ``, lut: 1707843043.9482448
[0m23:50:43.952890 [debug] [MainThread]: Databricks adapter: conn: 1962000497296: idle check connection: sess: 2e0b1fba-049e-48e6-9f89-2a822fb618c1, name: master, idle: 0.004645586013793945s, acqrelcnt: 1, lang: None, thrd: (11768, 13732), cmpt: ``, lut: 1707843043.9482448
[0m23:50:43.954185 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:50:43.955794 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:50:43.956781 [debug] [MainThread]: Databricks adapter: conn: 1962000497296: _release sess: 2e0b1fba-049e-48e6-9f89-2a822fb618c1, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (11768, 13732), cmpt: ``, lut: 1707843043.9567819
[0m23:50:43.959292 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:50:43.960303 [debug] [MainThread]: On master: ROLLBACK
[0m23:50:43.961288 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:50:43.962292 [debug] [MainThread]: On master: Close
[0m23:50:44.050732 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m23:50:44.052828 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m23:50:44.054517 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:50:44.055539 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m23:50:44.153102 [debug] [MainThread]: Connection 'test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493' was properly closed.
[0m23:50:44.154010 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: ROLLBACK
[0m23:50:44.155529 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:50:44.156660 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m23:50:44.238027 [info ] [MainThread]: 
[0m23:50:44.239945 [info ] [MainThread]: Finished running 8 tests in 0 hours 0 minutes and 5.44 seconds (5.44s).
[0m23:50:44.243998 [debug] [MainThread]: Command end result
[0m23:50:44.260002 [info ] [MainThread]: 
[0m23:50:44.261991 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:50:44.262992 [info ] [MainThread]: 
[0m23:50:44.264071 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models\example\schema.yml)[0m
[0m23:50:44.265589 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m23:50:44.266613 [info ] [MainThread]: 
[0m23:50:44.267604 [info ] [MainThread]:   compiled Code at target\compiled\medallion_dbt_spark\models\example\schema.yml\not_null_my_first_dbt_model_id.sql
[0m23:50:44.269695 [info ] [MainThread]: 
[0m23:50:44.270687 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=1 SKIP=0 TOTAL=8
[0m23:50:44.273689 [debug] [MainThread]: Command `dbt test` failed at 23:50:44.273689 after 7.24 seconds
[0m23:50:44.275503 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8BDE6D590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8BD4DD610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8BDC50E90>]}
[0m23:50:44.276503 [debug] [MainThread]: Flushing usage events
[0m23:56:54.753528 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BAF44E650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BAF29E9D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BAF44DE50>]}


============================== 23:56:54.774220 | 6df1bb24-e2ce-4c04-80dd-9714a6ef73a3 ==============================
[0m23:56:54.774220 [info ] [MainThread]: Running with dbt=1.7.7
[0m23:56:54.774220 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt test', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m23:56:56.709508 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6df1bb24-e2ce-4c04-80dd-9714a6ef73a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BC1567110>]}
[0m23:56:56.776391 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6df1bb24-e2ce-4c04-80dd-9714a6ef73a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BC14720D0>]}
[0m23:56:56.776391 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m23:56:56.795472 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m23:56:56.960122 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:56:56.960122 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://models\example\my_first_dbt_model.sql
[0m23:56:57.210529 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6df1bb24-e2ce-4c04-80dd-9714a6ef73a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BC19F9010>]}
[0m23:56:57.242104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6df1bb24-e2ce-4c04-80dd-9714a6ef73a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BC1956010>]}
[0m23:56:57.243868 [info ] [MainThread]: Found 5 models, 7 snapshots, 8 tests, 9 sources, 0 exposures, 0 metrics, 535 macros, 0 groups, 0 semantic models
[0m23:56:57.243868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6df1bb24-e2ce-4c04-80dd-9714a6ef73a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BC1810310>]}
[0m23:56:57.243868 [info ] [MainThread]: 
[0m23:56:57.243868 [debug] [MainThread]: Databricks adapter: conn: 2386956388816: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (16744, 16264), cmpt: ``, lut: None
[0m23:56:57.243868 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:56:57.243868 [debug] [MainThread]: Databricks adapter: Thread (16744, 16264) using default compute resource.
[0m23:56:57.243868 [debug] [MainThread]: Databricks adapter: conn: 2386956388816: _acquire sess: None, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (16744, 16264), cmpt: ``, lut: 1707843417.2438684
[0m23:56:57.243868 [debug] [ThreadPool]: Databricks adapter: conn: 2386955365584: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_snapshots, idle: 0s, acqrelcnt: 0, lang: None, thrd: (16744, 13732), cmpt: ``, lut: None
[0m23:56:57.243868 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_snapshots'
[0m23:56:57.243868 [debug] [ThreadPool]: Databricks adapter: Thread (16744, 13732) using default compute resource.
[0m23:56:57.243868 [debug] [ThreadPool]: Databricks adapter: conn: 2386955365584: _acquire sess: None, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (16744, 13732), cmpt: ``, lut: 1707843417.2438684
[0m23:56:57.243868 [debug] [ThreadPool]: Databricks adapter: conn: 2386955365584: get_thread_connection: sess: None, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (16744, 13732), cmpt: ``, lut: 1707843417.2438684
[0m23:56:57.243868 [debug] [ThreadPool]: Databricks adapter: conn: 2386955365584: idle check connection: sess: None, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (16744, 13732), cmpt: ``, lut: 1707843417.2438684
[0m23:56:57.243868 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m23:56:57.259402 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m23:56:57.259938 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:56:57.794061 [debug] [ThreadPool]: Databricks adapter: conn: 2386955365584: session opened sess: 9adb7f16-6122-409d-a076-28335d0017f6, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (16744, 13732), cmpt: ``, lut: 1707843417.794062
[0m23:56:58.244355 [debug] [ThreadPool]: SQL status: OK in 0.9800000190734863 seconds
[0m23:56:58.344799 [debug] [ThreadPool]: Databricks adapter: conn: 2386955365584: get_thread_connection: sess: 9adb7f16-6122-409d-a076-28335d0017f6, name: list_hive_metastore_snapshots, idle: 0.5507376194000244s, acqrelcnt: 1, lang: None, thrd: (16744, 13732), cmpt: ``, lut: 1707843417.794062
[0m23:56:58.344799 [debug] [ThreadPool]: Databricks adapter: conn: 2386955365584: idle check connection: sess: 9adb7f16-6122-409d-a076-28335d0017f6, name: list_hive_metastore_snapshots, idle: 0.5507376194000244s, acqrelcnt: 1, lang: None, thrd: (16744, 13732), cmpt: ``, lut: 1707843417.794062
[0m23:56:58.344799 [debug] [ThreadPool]: Databricks adapter: conn: 2386955365584: get_thread_connection: sess: 9adb7f16-6122-409d-a076-28335d0017f6, name: list_hive_metastore_snapshots, idle: 0.5507376194000244s, acqrelcnt: 1, lang: None, thrd: (16744, 13732), cmpt: ``, lut: 1707843417.794062
[0m23:56:58.344799 [debug] [ThreadPool]: Databricks adapter: conn: 2386955365584: idle check connection: sess: 9adb7f16-6122-409d-a076-28335d0017f6, name: list_hive_metastore_snapshots, idle: 0.5507376194000244s, acqrelcnt: 1, lang: None, thrd: (16744, 13732), cmpt: ``, lut: 1707843417.794062
[0m23:56:58.344799 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m23:56:58.344799 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m23:56:58.344799 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m23:56:58.629091 [debug] [ThreadPool]: SQL status: OK in 0.2800000011920929 seconds
[0m23:56:58.645139 [debug] [ThreadPool]: Databricks adapter: conn: 2386955365584: get_thread_connection: sess: 9adb7f16-6122-409d-a076-28335d0017f6, name: list_hive_metastore_snapshots, idle: 0.8510773181915283s, acqrelcnt: 1, lang: None, thrd: (16744, 13732), cmpt: ``, lut: 1707843417.794062
[0m23:56:58.645139 [debug] [ThreadPool]: Databricks adapter: conn: 2386955365584: idle check connection: sess: 9adb7f16-6122-409d-a076-28335d0017f6, name: list_hive_metastore_snapshots, idle: 0.8510773181915283s, acqrelcnt: 1, lang: None, thrd: (16744, 13732), cmpt: ``, lut: 1707843417.794062
[0m23:56:58.645139 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m23:56:58.645139 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m23:56:58.896012 [debug] [ThreadPool]: SQL status: OK in 0.25 seconds
[0m23:56:58.896012 [debug] [ThreadPool]: Databricks adapter: conn: 2386955365584: _release sess: 9adb7f16-6122-409d-a076-28335d0017f6, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (16744, 13732), cmpt: ``, lut: 1707843418.8960128
[0m23:56:58.896012 [debug] [ThreadPool]: Databricks adapter: conn: 2386955365584: idle check connection: sess: 9adb7f16-6122-409d-a076-28335d0017f6, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (16744, 13732), cmpt: ``, lut: 1707843418.8960128
[0m23:56:58.896012 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now list_hive_metastore_saleslt)
[0m23:56:58.896012 [debug] [ThreadPool]: Databricks adapter: conn: 2386955365584: reusing connection list_hive_metastore_snapshots sess: 9adb7f16-6122-409d-a076-28335d0017f6, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (16744, 13732), cmpt: ``, lut: 1707843418.8960128
[0m23:56:58.896012 [debug] [ThreadPool]: Databricks adapter: Thread (16744, 13732) using default compute resource.
[0m23:56:58.896012 [debug] [ThreadPool]: Databricks adapter: conn: 2386955365584: _acquire sess: 9adb7f16-6122-409d-a076-28335d0017f6, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (16744, 13732), cmpt: ``, lut: 1707843418.8960128
[0m23:56:58.896012 [debug] [ThreadPool]: Databricks adapter: conn: 2386955365584: get_thread_connection: sess: 9adb7f16-6122-409d-a076-28335d0017f6, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (16744, 13732), cmpt: ``, lut: 1707843418.8960128
[0m23:56:58.896012 [debug] [ThreadPool]: Databricks adapter: conn: 2386955365584: idle check connection: sess: 9adb7f16-6122-409d-a076-28335d0017f6, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (16744, 13732), cmpt: ``, lut: 1707843418.8960128
[0m23:56:58.896012 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:56:58.896012 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m23:56:59.029450 [debug] [ThreadPool]: SQL status: OK in 0.11999999731779099 seconds
[0m23:56:59.044459 [debug] [ThreadPool]: Databricks adapter: conn: 2386955365584: get_thread_connection: sess: 9adb7f16-6122-409d-a076-28335d0017f6, name: list_hive_metastore_saleslt, idle: 0.1334381103515625s, acqrelcnt: 1, lang: None, thrd: (16744, 13732), cmpt: ``, lut: 1707843418.8960128
[0m23:56:59.045209 [debug] [ThreadPool]: Databricks adapter: conn: 2386955365584: idle check connection: sess: 9adb7f16-6122-409d-a076-28335d0017f6, name: list_hive_metastore_saleslt, idle: 0.14919710159301758s, acqrelcnt: 1, lang: None, thrd: (16744, 13732), cmpt: ``, lut: 1707843418.8960128
[0m23:56:59.045654 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:56:59.045654 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m23:56:59.179609 [debug] [ThreadPool]: SQL status: OK in 0.12999999523162842 seconds
[0m23:56:59.195460 [debug] [ThreadPool]: Databricks adapter: conn: 2386955365584: get_thread_connection: sess: 9adb7f16-6122-409d-a076-28335d0017f6, name: list_hive_metastore_saleslt, idle: 0.29944753646850586s, acqrelcnt: 1, lang: None, thrd: (16744, 13732), cmpt: ``, lut: 1707843418.8960128
[0m23:56:59.195944 [debug] [ThreadPool]: Databricks adapter: conn: 2386955365584: idle check connection: sess: 9adb7f16-6122-409d-a076-28335d0017f6, name: list_hive_metastore_saleslt, idle: 0.29993224143981934s, acqrelcnt: 1, lang: None, thrd: (16744, 13732), cmpt: ``, lut: 1707843418.8960128
[0m23:56:59.195944 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m23:56:59.195944 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m23:56:59.378288 [debug] [ThreadPool]: SQL status: OK in 0.18000000715255737 seconds
[0m23:56:59.379598 [debug] [ThreadPool]: Databricks adapter: conn: 2386955365584: _release sess: 9adb7f16-6122-409d-a076-28335d0017f6, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (16744, 13732), cmpt: ``, lut: 1707843419.3795986
[0m23:56:59.379598 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6df1bb24-e2ce-4c04-80dd-9714a6ef73a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BC169FB90>]}
[0m23:56:59.379598 [debug] [MainThread]: Databricks adapter: conn: 2386956388816: get_thread_connection: sess: None, name: master, idle: 2.135730266571045s, acqrelcnt: 1, lang: None, thrd: (16744, 16264), cmpt: ``, lut: 1707843417.2438684
[0m23:56:59.379598 [debug] [MainThread]: Databricks adapter: conn: 2386956388816: idle check connection: sess: None, name: master, idle: 2.135730266571045s, acqrelcnt: 1, lang: None, thrd: (16744, 16264), cmpt: ``, lut: 1707843417.2438684
[0m23:56:59.379598 [debug] [MainThread]: Databricks adapter: conn: 2386956388816: get_thread_connection: sess: None, name: master, idle: 2.135730266571045s, acqrelcnt: 1, lang: None, thrd: (16744, 16264), cmpt: ``, lut: 1707843417.2438684
[0m23:56:59.379598 [debug] [MainThread]: Databricks adapter: conn: 2386956388816: idle check connection: sess: None, name: master, idle: 2.135730266571045s, acqrelcnt: 1, lang: None, thrd: (16744, 16264), cmpt: ``, lut: 1707843417.2438684
[0m23:56:59.379598 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:56:59.379598 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:56:59.393295 [debug] [MainThread]: Databricks adapter: conn: 2386956388816: _release sess: None, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (16744, 16264), cmpt: ``, lut: 1707843419.3795986
[0m23:56:59.394140 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:56:59.394140 [info ] [MainThread]: 
[0m23:56:59.395926 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m23:56:59.395926 [info ] [Thread-1 (]: 1 of 8 START test not_null_dim_product_product_name ............................ [RUN]
[0m23:56:59.404945 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: Creating DatabricksDBTConnection sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0s, acqrelcnt: 0, lang: None, thrd: (16744, 8224), cmpt: ``, lut: None
[0m23:56:59.404945 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5'
[0m23:56:59.405953 [debug] [Thread-1 (]: Databricks adapter: On thread (16744, 8224): None using default compute resource.
[0m23:56:59.405953 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: _acquire sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843419.405954
[0m23:56:59.405953 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m23:56:59.425923 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m23:56:59.425923 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (compile): 23:56:59.406951 => 23:56:59.425923
[0m23:56:59.425923 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m23:56:59.436517 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m23:56:59.436517 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0305633544921875s, acqrelcnt: 1, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843419.405954
[0m23:56:59.445645 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.039691925048828125s, acqrelcnt: 1, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843419.405954
[0m23:56:59.446200 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.04024672508239746s, acqrelcnt: 1, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843419.405954
[0m23:56:59.447240 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.04128623008728027s, acqrelcnt: 1, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843419.405954
[0m23:56:59.447240 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m23:56:59.448402 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m23:56:59.448402 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_name
from `hive_metastore`.`saleslt`.`dim_product`
where product_name is null



      
    ) dbt_internal_test
[0m23:56:59.449519 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:56:59.846740 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: session opened sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843419.846741
[0m23:57:00.163599 [debug] [Thread-1 (]: SQL status: OK in 0.7099999785423279 seconds
[0m23:57:00.163599 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (execute): 23:56:59.429297 => 23:57:00.163599
[0m23:57:00.163599 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: _release sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843420.1636
[0m23:57:00.163599 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: _release sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843420.1636
[0m23:57:00.163599 [info ] [Thread-1 (]: 1 of 8 PASS not_null_dim_product_product_name .................................. [[32mPASS[0m in 0.77s]
[0m23:57:00.180128 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m23:57:00.180398 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m23:57:00.180398 [info ] [Thread-1 (]: 2 of 8 START test not_null_dim_product_product_sk .............................. [RUN]
[0m23:57:00.180398 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: idle check connection: sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.016798019409179688s, acqrelcnt: 0, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843420.1636
[0m23:57:00.180398 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, now test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417)
[0m23:57:00.180398 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.016798019409179688s, acqrelcnt: 0, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843420.1636
[0m23:57:00.180398 [debug] [Thread-1 (]: Databricks adapter: On thread (16744, 8224): None using default compute resource.
[0m23:57:00.180398 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: _acquire sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.016798019409179688s, acqrelcnt: 1, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843420.1636
[0m23:57:00.180398 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m23:57:00.180398 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m23:57:00.180398 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (compile): 23:57:00.180398 => 23:57:00.180398
[0m23:57:00.180398 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m23:57:00.180398 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m23:57:00.196029 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: get_thread_connection: sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.016798019409179688s, acqrelcnt: 1, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843420.1636
[0m23:57:00.196805 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: idle check connection: sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.03289937973022461s, acqrelcnt: 1, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843420.1636
[0m23:57:00.196805 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m23:57:00.198806 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_sk
from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is null



      
    ) dbt_internal_test
[0m23:57:00.497860 [debug] [Thread-1 (]: SQL status: OK in 0.30000001192092896 seconds
[0m23:57:00.497860 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (execute): 23:57:00.180398 => 23:57:00.497860
[0m23:57:00.497860 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: _release sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843420.497861
[0m23:57:00.497860 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: _release sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843420.497861
[0m23:57:00.497860 [info ] [Thread-1 (]: 2 of 8 PASS not_null_dim_product_product_sk .................................... [[32mPASS[0m in 0.32s]
[0m23:57:00.497860 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m23:57:00.512971 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m23:57:00.513766 [info ] [Thread-1 (]: 3 of 8 START test not_null_dim_product_sellstartdate ........................... [RUN]
[0m23:57:00.513923 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: idle check connection: sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.016062498092651367s, acqrelcnt: 0, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843420.497861
[0m23:57:00.513923 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, now test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546)
[0m23:57:00.513923 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.016062498092651367s, acqrelcnt: 0, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843420.497861
[0m23:57:00.513923 [debug] [Thread-1 (]: Databricks adapter: On thread (16744, 8224): None using default compute resource.
[0m23:57:00.513923 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: _acquire sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.016062498092651367s, acqrelcnt: 1, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843420.497861
[0m23:57:00.513923 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m23:57:00.513923 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m23:57:00.513923 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (compile): 23:57:00.513923 => 23:57:00.513923
[0m23:57:00.513923 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m23:57:00.513923 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m23:57:00.513923 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: get_thread_connection: sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.016062498092651367s, acqrelcnt: 1, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843420.497861
[0m23:57:00.513923 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: idle check connection: sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.016062498092651367s, acqrelcnt: 1, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843420.497861
[0m23:57:00.513923 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m23:57:00.513923 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select sellstartdate
from `hive_metastore`.`saleslt`.`dim_product`
where sellstartdate is null



      
    ) dbt_internal_test
[0m23:57:00.832572 [debug] [Thread-1 (]: SQL status: OK in 0.3199999928474426 seconds
[0m23:57:00.832572 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (execute): 23:57:00.513923 => 23:57:00.832572
[0m23:57:00.832572 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: _release sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843420.8325727
[0m23:57:00.832572 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: _release sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843420.8325727
[0m23:57:00.832572 [info ] [Thread-1 (]: 3 of 8 PASS not_null_dim_product_sellstartdate ................................. [[32mPASS[0m in 0.32s]
[0m23:57:00.832572 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m23:57:00.847279 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m23:57:00.847902 [info ] [Thread-1 (]: 4 of 8 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m23:57:00.851234 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: idle check connection: sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.015330314636230469s, acqrelcnt: 0, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843420.8325727
[0m23:57:00.851234 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, now test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710)
[0m23:57:00.852284 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: reusing connection test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0186612606048584s, acqrelcnt: 0, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843420.8325727
[0m23:57:00.852284 [debug] [Thread-1 (]: Databricks adapter: On thread (16744, 8224): None using default compute resource.
[0m23:57:00.853287 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: _acquire sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.01971149444580078s, acqrelcnt: 1, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843420.8325727
[0m23:57:00.853287 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m23:57:00.856506 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m23:57:00.856506 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 23:57:00.853287 => 23:57:00.856506
[0m23:57:00.856506 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m23:57:00.856506 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m23:57:00.865033 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: get_thread_connection: sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.031870126724243164s, acqrelcnt: 1, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843420.8325727
[0m23:57:00.866434 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: idle check connection: sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.03332209587097168s, acqrelcnt: 1, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843420.8325727
[0m23:57:00.866812 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m23:57:00.867481 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m23:57:01.180884 [debug] [Thread-1 (]: SQL status: OK in 0.30000001192092896 seconds
[0m23:57:01.182094 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 23:57:00.856506 => 23:57:01.182094
[0m23:57:01.182094 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: _release sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843421.182094
[0m23:57:01.182094 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: _release sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843421.182094
[0m23:57:01.182094 [error] [Thread-1 (]: 4 of 8 FAIL 1 not_null_my_first_dbt_model_id ................................... [[31mFAIL 1[0m in 0.33s]
[0m23:57:01.182094 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m23:57:01.182094 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m23:57:01.182094 [info ] [Thread-1 (]: 5 of 8 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m23:57:01.182094 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: idle check connection: sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843421.182094
[0m23:57:01.182094 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, now test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778)
[0m23:57:01.182094 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: reusing connection test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843421.182094
[0m23:57:01.182094 [debug] [Thread-1 (]: Databricks adapter: On thread (16744, 8224): None using default compute resource.
[0m23:57:01.182094 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: _acquire sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843421.182094
[0m23:57:01.182094 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m23:57:01.202106 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m23:57:01.204156 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (compile): 23:57:01.182094 => 23:57:01.203105
[0m23:57:01.204156 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m23:57:01.204156 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m23:57:01.204156 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: get_thread_connection: sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.02206254005432129s, acqrelcnt: 1, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843421.182094
[0m23:57:01.204156 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: idle check connection: sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.02206254005432129s, acqrelcnt: 1, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843421.182094
[0m23:57:01.204156 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m23:57:01.204156 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m23:57:01.465036 [debug] [Thread-1 (]: SQL status: OK in 0.25999999046325684 seconds
[0m23:57:01.465036 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (execute): 23:57:01.204156 => 23:57:01.465036
[0m23:57:01.465036 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: _release sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843421.4650366
[0m23:57:01.465036 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: _release sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843421.4650366
[0m23:57:01.465036 [info ] [Thread-1 (]: 5 of 8 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.28s]
[0m23:57:01.465036 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m23:57:01.465036 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m23:57:01.465036 [info ] [Thread-1 (]: 6 of 8 START test unique_dim_product_product_sk ................................ [RUN]
[0m23:57:01.465036 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: idle check connection: sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843421.4650366
[0m23:57:01.465036 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, now test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c)
[0m23:57:01.465036 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: reusing connection test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843421.4650366
[0m23:57:01.465036 [debug] [Thread-1 (]: Databricks adapter: On thread (16744, 8224): None using default compute resource.
[0m23:57:01.465036 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: _acquire sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843421.4650366
[0m23:57:01.465036 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m23:57:01.485204 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m23:57:01.487189 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (compile): 23:57:01.465036 => 23:57:01.487189
[0m23:57:01.487189 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m23:57:01.487189 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m23:57:01.487189 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: get_thread_connection: sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.022153139114379883s, acqrelcnt: 1, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843421.4650366
[0m23:57:01.487189 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: idle check connection: sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.022153139114379883s, acqrelcnt: 1, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843421.4650366
[0m23:57:01.487189 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m23:57:01.487189 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    product_sk as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is not null
group by product_sk
having count(*) > 1



      
    ) dbt_internal_test
[0m23:57:01.916078 [debug] [Thread-1 (]: SQL status: OK in 0.4300000071525574 seconds
[0m23:57:01.932220 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (execute): 23:57:01.487189 => 23:57:01.932220
[0m23:57:01.932220 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: _release sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843421.9322205
[0m23:57:01.932220 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: _release sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843421.9322205
[0m23:57:01.932220 [info ] [Thread-1 (]: 6 of 8 PASS unique_dim_product_product_sk ...................................... [[32mPASS[0m in 0.47s]
[0m23:57:01.932220 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m23:57:01.932220 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m23:57:01.932220 [info ] [Thread-1 (]: 7 of 8 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m23:57:01.932220 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: idle check connection: sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843421.9322205
[0m23:57:01.932220 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, now test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321)
[0m23:57:01.932220 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: reusing connection test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843421.9322205
[0m23:57:01.932220 [debug] [Thread-1 (]: Databricks adapter: On thread (16744, 8224): None using default compute resource.
[0m23:57:01.932220 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: _acquire sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843421.9322205
[0m23:57:01.932220 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m23:57:01.932220 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m23:57:01.947849 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (compile): 23:57:01.932220 => 23:57:01.932220
[0m23:57:01.948373 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m23:57:01.951470 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m23:57:01.952463 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: get_thread_connection: sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.020242691040039062s, acqrelcnt: 1, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843421.9322205
[0m23:57:01.953381 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: idle check connection: sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.02116084098815918s, acqrelcnt: 1, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843421.9322205
[0m23:57:01.953381 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m23:57:01.954381 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m23:57:02.300030 [debug] [Thread-1 (]: SQL status: OK in 0.3499999940395355 seconds
[0m23:57:02.315971 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (execute): 23:57:01.948373 => 23:57:02.315971
[0m23:57:02.315971 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: _release sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843422.3159716
[0m23:57:02.315971 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: _release sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843422.3159716
[0m23:57:02.315971 [info ] [Thread-1 (]: 7 of 8 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.38s]
[0m23:57:02.315971 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m23:57:02.315971 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m23:57:02.315971 [info ] [Thread-1 (]: 8 of 8 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m23:57:02.315971 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: idle check connection: sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843422.3159716
[0m23:57:02.315971 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, now test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493)
[0m23:57:02.315971 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: reusing connection test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843422.3159716
[0m23:57:02.315971 [debug] [Thread-1 (]: Databricks adapter: On thread (16744, 8224): None using default compute resource.
[0m23:57:02.315971 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: _acquire sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843422.3159716
[0m23:57:02.315971 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m23:57:02.315971 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m23:57:02.315971 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (compile): 23:57:02.315971 => 23:57:02.315971
[0m23:57:02.315971 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m23:57:02.332492 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m23:57:02.332492 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: get_thread_connection: sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.016520977020263672s, acqrelcnt: 1, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843422.3159716
[0m23:57:02.332492 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: idle check connection: sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.016520977020263672s, acqrelcnt: 1, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843422.3159716
[0m23:57:02.332492 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m23:57:02.332492 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m23:57:02.749876 [debug] [Thread-1 (]: SQL status: OK in 0.41999998688697815 seconds
[0m23:57:02.750181 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (execute): 23:57:02.315971 => 23:57:02.750181
[0m23:57:02.750181 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: _release sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843422.7501817
[0m23:57:02.750181 [debug] [Thread-1 (]: Databricks adapter: conn: 2386955048656: _release sess: 9ceaeba1-9bde-446e-9f6e-3231ffd1f700, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (16744, 8224), cmpt: ``, lut: 1707843422.7501817
[0m23:57:02.750181 [info ] [Thread-1 (]: 8 of 8 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.43s]
[0m23:57:02.750181 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m23:57:02.750181 [debug] [MainThread]: Databricks adapter: conn: 2386956388816: idle check connection: sess: None, name: master, idle: 3.3705830574035645s, acqrelcnt: 0, lang: None, thrd: (16744, 16264), cmpt: ``, lut: 1707843419.3795986
[0m23:57:02.750181 [debug] [MainThread]: Databricks adapter: conn: 2386956388816: reusing connection master sess: None, name: master, idle: 3.3705830574035645s, acqrelcnt: 0, lang: None, thrd: (16744, 16264), cmpt: ``, lut: 1707843419.3795986
[0m23:57:02.750181 [debug] [MainThread]: Databricks adapter: Thread (16744, 16264) using default compute resource.
[0m23:57:02.750181 [debug] [MainThread]: Databricks adapter: conn: 2386956388816: _acquire sess: None, name: master, idle: 3.3705830574035645s, acqrelcnt: 1, lang: None, thrd: (16744, 16264), cmpt: ``, lut: 1707843419.3795986
[0m23:57:02.750181 [debug] [MainThread]: Databricks adapter: conn: 2386956388816: get_thread_connection: sess: None, name: master, idle: 3.3705830574035645s, acqrelcnt: 1, lang: None, thrd: (16744, 16264), cmpt: ``, lut: 1707843419.3795986
[0m23:57:02.750181 [debug] [MainThread]: Databricks adapter: conn: 2386956388816: idle check connection: sess: None, name: master, idle: 3.3705830574035645s, acqrelcnt: 1, lang: None, thrd: (16744, 16264), cmpt: ``, lut: 1707843419.3795986
[0m23:57:02.750181 [debug] [MainThread]: On master: ROLLBACK
[0m23:57:02.750181 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:57:03.000625 [debug] [MainThread]: Databricks adapter: conn: 2386956388816: session opened sess: 4116b7ac-1b85-4cb7-9801-9ab501825e28, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (16744, 16264), cmpt: ``, lut: 1707843423.000625
[0m23:57:03.000625 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:57:03.000625 [debug] [MainThread]: Databricks adapter: conn: 2386956388816: get_thread_connection: sess: 4116b7ac-1b85-4cb7-9801-9ab501825e28, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (16744, 16264), cmpt: ``, lut: 1707843423.000625
[0m23:57:03.000625 [debug] [MainThread]: Databricks adapter: conn: 2386956388816: idle check connection: sess: 4116b7ac-1b85-4cb7-9801-9ab501825e28, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (16744, 16264), cmpt: ``, lut: 1707843423.000625
[0m23:57:03.000625 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m23:57:03.000625 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m23:57:03.000625 [debug] [MainThread]: Databricks adapter: conn: 2386956388816: _release sess: 4116b7ac-1b85-4cb7-9801-9ab501825e28, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (16744, 16264), cmpt: ``, lut: 1707843423.000625
[0m23:57:03.000625 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:57:03.000625 [debug] [MainThread]: On master: ROLLBACK
[0m23:57:03.000625 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:57:03.000625 [debug] [MainThread]: On master: Close
[0m23:57:03.100303 [debug] [MainThread]: Connection 'list_hive_metastore_saleslt' was properly closed.
[0m23:57:03.100303 [debug] [MainThread]: On list_hive_metastore_saleslt: ROLLBACK
[0m23:57:03.100303 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:57:03.100303 [debug] [MainThread]: On list_hive_metastore_saleslt: Close
[0m23:57:03.188029 [debug] [MainThread]: Connection 'test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493' was properly closed.
[0m23:57:03.188029 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: ROLLBACK
[0m23:57:03.188029 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m23:57:03.199107 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m23:57:03.284358 [info ] [MainThread]: 
[0m23:57:03.284358 [info ] [MainThread]: Finished running 8 tests in 0 hours 0 minutes and 6.04 seconds (6.04s).
[0m23:57:03.284358 [debug] [MainThread]: Command end result
[0m23:57:03.308820 [info ] [MainThread]: 
[0m23:57:03.310212 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:57:03.310212 [info ] [MainThread]: 
[0m23:57:03.311769 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models\example\schema.yml)[0m
[0m23:57:03.312775 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m23:57:03.312775 [info ] [MainThread]: 
[0m23:57:03.313862 [info ] [MainThread]:   compiled Code at target\compiled\medallion_dbt_spark\models\example\schema.yml\not_null_my_first_dbt_model_id.sql
[0m23:57:03.313862 [info ] [MainThread]: 
[0m23:57:03.315612 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=1 SKIP=0 TOTAL=8
[0m23:57:03.317333 [debug] [MainThread]: Command `dbt test` failed at 23:57:03.317333 after 8.60 seconds
[0m23:57:03.318549 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BAF0CC890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BAEE25890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BAEE25E10>]}
[0m23:57:03.319586 [debug] [MainThread]: Flushing usage events
[0m00:00:10.468480 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F988A7E90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F9BCFCD10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F9BF84E10>]}


============================== 00:00:10.484100 | 54eb2598-e044-47f1-91bd-7082056e7b65 ==============================
[0m00:00:10.484100 [info ] [MainThread]: Running with dbt=1.7.7
[0m00:00:10.484100 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt test', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m00:00:12.160248 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '54eb2598-e044-47f1-91bd-7082056e7b65', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025FAE257D10>]}
[0m00:00:12.226900 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '54eb2598-e044-47f1-91bd-7082056e7b65', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025FAE127590>]}
[0m00:00:12.227906 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m00:00:12.237543 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m00:00:12.358961 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:00:12.358961 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://models\example\my_first_dbt_model.sql
[0m00:00:12.577834 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '54eb2598-e044-47f1-91bd-7082056e7b65', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025FAE6A9110>]}
[0m00:00:12.593431 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '54eb2598-e044-47f1-91bd-7082056e7b65', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025FAE637550>]}
[0m00:00:12.593431 [info ] [MainThread]: Found 5 models, 7 snapshots, 8 tests, 9 sources, 0 exposures, 0 metrics, 535 macros, 0 groups, 0 semantic models
[0m00:00:12.593431 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '54eb2598-e044-47f1-91bd-7082056e7b65', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025FAE4E63D0>]}
[0m00:00:12.593431 [info ] [MainThread]: 
[0m00:00:12.609002 [debug] [MainThread]: Databricks adapter: conn: 2609970618576: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (23500, 13752), cmpt: ``, lut: None
[0m00:00:12.609002 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:00:12.611692 [debug] [MainThread]: Databricks adapter: Thread (23500, 13752) using default compute resource.
[0m00:00:12.611692 [debug] [MainThread]: Databricks adapter: conn: 2609970618576: _acquire sess: None, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23500, 13752), cmpt: ``, lut: 1707843612.6116922
[0m00:00:12.611692 [debug] [ThreadPool]: Databricks adapter: conn: 2609972647440: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_saleslt, idle: 0s, acqrelcnt: 0, lang: None, thrd: (23500, 18308), cmpt: ``, lut: None
[0m00:00:12.611692 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m00:00:12.611692 [debug] [ThreadPool]: Databricks adapter: Thread (23500, 18308) using default compute resource.
[0m00:00:12.611692 [debug] [ThreadPool]: Databricks adapter: conn: 2609972647440: _acquire sess: None, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23500, 18308), cmpt: ``, lut: 1707843612.6116922
[0m00:00:12.611692 [debug] [ThreadPool]: Databricks adapter: conn: 2609972647440: get_thread_connection: sess: None, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23500, 18308), cmpt: ``, lut: 1707843612.6116922
[0m00:00:12.611692 [debug] [ThreadPool]: Databricks adapter: conn: 2609972647440: idle check connection: sess: None, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23500, 18308), cmpt: ``, lut: 1707843612.6116922
[0m00:00:12.611692 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:00:12.611692 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m00:00:12.611692 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:00:13.055558 [debug] [ThreadPool]: Databricks adapter: conn: 2609972647440: session opened sess: ffab31db-faa0-4542-80ee-45d1ff064cd3, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23500, 18308), cmpt: ``, lut: 1707843613.055559
[0m00:00:13.243209 [debug] [ThreadPool]: SQL status: OK in 0.6299999952316284 seconds
[0m00:00:13.341657 [debug] [ThreadPool]: Databricks adapter: conn: 2609972647440: get_thread_connection: sess: ffab31db-faa0-4542-80ee-45d1ff064cd3, name: list_hive_metastore_saleslt, idle: 0.2860987186431885s, acqrelcnt: 1, lang: None, thrd: (23500, 18308), cmpt: ``, lut: 1707843613.055559
[0m00:00:13.341657 [debug] [ThreadPool]: Databricks adapter: conn: 2609972647440: idle check connection: sess: ffab31db-faa0-4542-80ee-45d1ff064cd3, name: list_hive_metastore_saleslt, idle: 0.2860987186431885s, acqrelcnt: 1, lang: None, thrd: (23500, 18308), cmpt: ``, lut: 1707843613.055559
[0m00:00:13.341657 [debug] [ThreadPool]: Databricks adapter: conn: 2609972647440: get_thread_connection: sess: ffab31db-faa0-4542-80ee-45d1ff064cd3, name: list_hive_metastore_saleslt, idle: 0.2860987186431885s, acqrelcnt: 1, lang: None, thrd: (23500, 18308), cmpt: ``, lut: 1707843613.055559
[0m00:00:13.341657 [debug] [ThreadPool]: Databricks adapter: conn: 2609972647440: idle check connection: sess: ffab31db-faa0-4542-80ee-45d1ff064cd3, name: list_hive_metastore_saleslt, idle: 0.2860987186431885s, acqrelcnt: 1, lang: None, thrd: (23500, 18308), cmpt: ``, lut: 1707843613.055559
[0m00:00:13.341657 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:00:13.341657 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:00:13.341657 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m00:00:13.606153 [debug] [ThreadPool]: SQL status: OK in 0.25999999046325684 seconds
[0m00:00:13.621866 [debug] [ThreadPool]: Databricks adapter: conn: 2609972647440: get_thread_connection: sess: ffab31db-faa0-4542-80ee-45d1ff064cd3, name: list_hive_metastore_saleslt, idle: 0.5663080215454102s, acqrelcnt: 1, lang: None, thrd: (23500, 18308), cmpt: ``, lut: 1707843613.055559
[0m00:00:13.621866 [debug] [ThreadPool]: Databricks adapter: conn: 2609972647440: idle check connection: sess: ffab31db-faa0-4542-80ee-45d1ff064cd3, name: list_hive_metastore_saleslt, idle: 0.5663080215454102s, acqrelcnt: 1, lang: None, thrd: (23500, 18308), cmpt: ``, lut: 1707843613.055559
[0m00:00:13.621866 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:00:13.621866 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m00:00:13.791649 [debug] [ThreadPool]: SQL status: OK in 0.17000000178813934 seconds
[0m00:00:13.797695 [debug] [ThreadPool]: Databricks adapter: conn: 2609972647440: _release sess: ffab31db-faa0-4542-80ee-45d1ff064cd3, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (23500, 18308), cmpt: ``, lut: 1707843613.7966974
[0m00:00:13.798695 [debug] [ThreadPool]: Databricks adapter: conn: 2609972647440: idle check connection: sess: ffab31db-faa0-4542-80ee-45d1ff064cd3, name: list_hive_metastore_saleslt, idle: 0.0019981861114501953s, acqrelcnt: 0, lang: None, thrd: (23500, 18308), cmpt: ``, lut: 1707843613.7966974
[0m00:00:13.801725 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m00:00:13.801725 [debug] [ThreadPool]: Databricks adapter: conn: 2609972647440: reusing connection list_hive_metastore_saleslt sess: ffab31db-faa0-4542-80ee-45d1ff064cd3, name: list_hive_metastore_snapshots, idle: 0.005028247833251953s, acqrelcnt: 0, lang: None, thrd: (23500, 18308), cmpt: ``, lut: 1707843613.7966974
[0m00:00:13.801725 [debug] [ThreadPool]: Databricks adapter: Thread (23500, 18308) using default compute resource.
[0m00:00:13.802696 [debug] [ThreadPool]: Databricks adapter: conn: 2609972647440: _acquire sess: ffab31db-faa0-4542-80ee-45d1ff064cd3, name: list_hive_metastore_snapshots, idle: 0.005998849868774414s, acqrelcnt: 1, lang: None, thrd: (23500, 18308), cmpt: ``, lut: 1707843613.7966974
[0m00:00:13.804694 [debug] [ThreadPool]: Databricks adapter: conn: 2609972647440: get_thread_connection: sess: ffab31db-faa0-4542-80ee-45d1ff064cd3, name: list_hive_metastore_snapshots, idle: 0.007997512817382812s, acqrelcnt: 1, lang: None, thrd: (23500, 18308), cmpt: ``, lut: 1707843613.7966974
[0m00:00:13.804694 [debug] [ThreadPool]: Databricks adapter: conn: 2609972647440: idle check connection: sess: ffab31db-faa0-4542-80ee-45d1ff064cd3, name: list_hive_metastore_snapshots, idle: 0.007997512817382812s, acqrelcnt: 1, lang: None, thrd: (23500, 18308), cmpt: ``, lut: 1707843613.7966974
[0m00:00:13.805695 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:00:13.805695 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m00:00:13.907401 [debug] [ThreadPool]: SQL status: OK in 0.10000000149011612 seconds
[0m00:00:13.907401 [debug] [ThreadPool]: Databricks adapter: conn: 2609972647440: get_thread_connection: sess: ffab31db-faa0-4542-80ee-45d1ff064cd3, name: list_hive_metastore_snapshots, idle: 0.11070418357849121s, acqrelcnt: 1, lang: None, thrd: (23500, 18308), cmpt: ``, lut: 1707843613.7966974
[0m00:00:13.907401 [debug] [ThreadPool]: Databricks adapter: conn: 2609972647440: idle check connection: sess: ffab31db-faa0-4542-80ee-45d1ff064cd3, name: list_hive_metastore_snapshots, idle: 0.11070418357849121s, acqrelcnt: 1, lang: None, thrd: (23500, 18308), cmpt: ``, lut: 1707843613.7966974
[0m00:00:13.907401 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:00:13.907401 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m00:00:14.047944 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m00:00:14.047944 [debug] [ThreadPool]: Databricks adapter: conn: 2609972647440: get_thread_connection: sess: ffab31db-faa0-4542-80ee-45d1ff064cd3, name: list_hive_metastore_snapshots, idle: 0.25124692916870117s, acqrelcnt: 1, lang: None, thrd: (23500, 18308), cmpt: ``, lut: 1707843613.7966974
[0m00:00:14.047944 [debug] [ThreadPool]: Databricks adapter: conn: 2609972647440: idle check connection: sess: ffab31db-faa0-4542-80ee-45d1ff064cd3, name: list_hive_metastore_snapshots, idle: 0.25124692916870117s, acqrelcnt: 1, lang: None, thrd: (23500, 18308), cmpt: ``, lut: 1707843613.7966974
[0m00:00:14.047944 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:00:14.047944 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m00:00:14.204189 [debug] [ThreadPool]: SQL status: OK in 0.1599999964237213 seconds
[0m00:00:14.204189 [debug] [ThreadPool]: Databricks adapter: conn: 2609972647440: _release sess: ffab31db-faa0-4542-80ee-45d1ff064cd3, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (23500, 18308), cmpt: ``, lut: 1707843614.2041893
[0m00:00:14.204189 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '54eb2598-e044-47f1-91bd-7082056e7b65', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025FAE36B550>]}
[0m00:00:14.204189 [debug] [MainThread]: Databricks adapter: conn: 2609970618576: get_thread_connection: sess: None, name: master, idle: 1.5924971103668213s, acqrelcnt: 1, lang: None, thrd: (23500, 13752), cmpt: ``, lut: 1707843612.6116922
[0m00:00:14.204189 [debug] [MainThread]: Databricks adapter: conn: 2609970618576: idle check connection: sess: None, name: master, idle: 1.5924971103668213s, acqrelcnt: 1, lang: None, thrd: (23500, 13752), cmpt: ``, lut: 1707843612.6116922
[0m00:00:14.204189 [debug] [MainThread]: Databricks adapter: conn: 2609970618576: get_thread_connection: sess: None, name: master, idle: 1.5924971103668213s, acqrelcnt: 1, lang: None, thrd: (23500, 13752), cmpt: ``, lut: 1707843612.6116922
[0m00:00:14.204189 [debug] [MainThread]: Databricks adapter: conn: 2609970618576: idle check connection: sess: None, name: master, idle: 1.5924971103668213s, acqrelcnt: 1, lang: None, thrd: (23500, 13752), cmpt: ``, lut: 1707843612.6116922
[0m00:00:14.204189 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:00:14.204189 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:00:14.204189 [debug] [MainThread]: Databricks adapter: conn: 2609970618576: _release sess: None, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (23500, 13752), cmpt: ``, lut: 1707843614.2041893
[0m00:00:14.204189 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:00:14.220466 [info ] [MainThread]: 
[0m00:00:14.223653 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:00:14.223653 [info ] [Thread-1 (]: 1 of 8 START test not_null_dim_product_product_name ............................ [RUN]
[0m00:00:14.227134 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: Creating DatabricksDBTConnection sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0s, acqrelcnt: 0, lang: None, thrd: (23500, 17032), cmpt: ``, lut: None
[0m00:00:14.228148 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5'
[0m00:00:14.229146 [debug] [Thread-1 (]: Databricks adapter: On thread (23500, 17032): None using default compute resource.
[0m00:00:14.229146 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: _acquire sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843614.229146
[0m00:00:14.230144 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:00:14.246072 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:00:14.248097 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (compile): 00:00:14.230144 => 00:00:14.247556
[0m00:00:14.248097 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:00:14.262250 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:00:14.263248 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.034102439880371094s, acqrelcnt: 1, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843614.229146
[0m00:00:14.264246 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.035100698471069336s, acqrelcnt: 1, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843614.229146
[0m00:00:14.264246 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.035100698471069336s, acqrelcnt: 1, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843614.229146
[0m00:00:14.265245 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.03609967231750488s, acqrelcnt: 1, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843614.229146
[0m00:00:14.265245 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:00:14.266247 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:00:14.266821 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_name
from `hive_metastore`.`saleslt`.`dim_product`
where product_name is null



      
    ) dbt_internal_test
[0m00:00:14.267880 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:00:14.535590 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: session opened sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843614.5355904
[0m00:00:14.873135 [debug] [Thread-1 (]: SQL status: OK in 0.6100000143051147 seconds
[0m00:00:14.880260 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (execute): 00:00:14.249110 => 00:00:14.880260
[0m00:00:14.881328 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: _release sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843614.8813283
[0m00:00:14.882325 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: _release sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843614.8823254
[0m00:00:14.883243 [info ] [Thread-1 (]: 1 of 8 PASS not_null_dim_product_product_name .................................. [[32mPASS[0m in 0.66s]
[0m00:00:14.884253 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:00:14.884253 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:00:14.885303 [info ] [Thread-1 (]: 2 of 8 START test not_null_dim_product_product_sk .............................. [RUN]
[0m00:00:14.886241 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: idle check connection: sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0039157867431640625s, acqrelcnt: 0, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843614.8823254
[0m00:00:14.887352 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, now test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417)
[0m00:00:14.887352 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.005027055740356445s, acqrelcnt: 0, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843614.8823254
[0m00:00:14.888259 [debug] [Thread-1 (]: Databricks adapter: On thread (23500, 17032): None using default compute resource.
[0m00:00:14.888259 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: _acquire sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.005934238433837891s, acqrelcnt: 1, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843614.8823254
[0m00:00:14.888259 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:00:14.892339 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:00:14.893339 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (compile): 00:00:14.889242 => 00:00:14.893339
[0m00:00:14.894187 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:00:14.898307 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:00:14.899219 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: get_thread_connection: sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.01689457893371582s, acqrelcnt: 1, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843614.8823254
[0m00:00:14.899219 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: idle check connection: sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.01689457893371582s, acqrelcnt: 1, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843614.8823254
[0m00:00:14.899219 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:00:14.899219 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_sk
from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is null



      
    ) dbt_internal_test
[0m00:00:15.137776 [debug] [Thread-1 (]: SQL status: OK in 0.23999999463558197 seconds
[0m00:00:15.137776 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (execute): 00:00:14.894187 => 00:00:15.137776
[0m00:00:15.137776 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: _release sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843615.1377769
[0m00:00:15.137776 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: _release sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843615.1377769
[0m00:00:15.137776 [info ] [Thread-1 (]: 2 of 8 PASS not_null_dim_product_product_sk .................................... [[32mPASS[0m in 0.25s]
[0m00:00:15.137776 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:00:15.153452 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:00:15.153452 [info ] [Thread-1 (]: 3 of 8 START test not_null_dim_product_sellstartdate ........................... [RUN]
[0m00:00:15.153452 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: idle check connection: sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.015675067901611328s, acqrelcnt: 0, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843615.1377769
[0m00:00:15.153452 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, now test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546)
[0m00:00:15.153452 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.015675067901611328s, acqrelcnt: 0, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843615.1377769
[0m00:00:15.153452 [debug] [Thread-1 (]: Databricks adapter: On thread (23500, 17032): None using default compute resource.
[0m00:00:15.153452 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: _acquire sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.015675067901611328s, acqrelcnt: 1, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843615.1377769
[0m00:00:15.153452 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:00:15.153452 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:00:15.153452 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (compile): 00:00:15.153452 => 00:00:15.153452
[0m00:00:15.153452 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:00:15.171200 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:00:15.172317 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: get_thread_connection: sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.03454017639160156s, acqrelcnt: 1, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843615.1377769
[0m00:00:15.173232 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: idle check connection: sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.03545546531677246s, acqrelcnt: 1, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843615.1377769
[0m00:00:15.174302 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:00:15.174302 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select sellstartdate
from `hive_metastore`.`saleslt`.`dim_product`
where sellstartdate is null



      
    ) dbt_internal_test
[0m00:00:15.554178 [debug] [Thread-1 (]: SQL status: OK in 0.3799999952316284 seconds
[0m00:00:15.554178 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (execute): 00:00:15.153452 => 00:00:15.554178
[0m00:00:15.554178 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: _release sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843615.5541782
[0m00:00:15.554178 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: _release sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843615.5541782
[0m00:00:15.554178 [info ] [Thread-1 (]: 3 of 8 PASS not_null_dim_product_sellstartdate ................................. [[32mPASS[0m in 0.40s]
[0m00:00:15.569644 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:00:15.569644 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:00:15.569644 [info ] [Thread-1 (]: 4 of 8 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m00:00:15.569644 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: idle check connection: sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.015465974807739258s, acqrelcnt: 0, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843615.5541782
[0m00:00:15.569644 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, now test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710)
[0m00:00:15.569644 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: reusing connection test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.015465974807739258s, acqrelcnt: 0, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843615.5541782
[0m00:00:15.569644 [debug] [Thread-1 (]: Databricks adapter: On thread (23500, 17032): None using default compute resource.
[0m00:00:15.569644 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: _acquire sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.015465974807739258s, acqrelcnt: 1, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843615.5541782
[0m00:00:15.569644 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:00:15.569644 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:00:15.569644 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 00:00:15.569644 => 00:00:15.569644
[0m00:00:15.569644 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:00:15.569644 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:00:15.569644 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: get_thread_connection: sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.015465974807739258s, acqrelcnt: 1, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843615.5541782
[0m00:00:15.569644 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: idle check connection: sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.015465974807739258s, acqrelcnt: 1, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843615.5541782
[0m00:00:15.569644 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:00:15.585272 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m00:00:15.908027 [debug] [Thread-1 (]: SQL status: OK in 0.3199999928474426 seconds
[0m00:00:15.908027 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 00:00:15.569644 => 00:00:15.908027
[0m00:00:15.908027 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: _release sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843615.9080272
[0m00:00:15.908027 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: _release sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843615.9080272
[0m00:00:15.908027 [error] [Thread-1 (]: 4 of 8 FAIL 1 not_null_my_first_dbt_model_id ................................... [[31mFAIL 1[0m in 0.34s]
[0m00:00:15.908027 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:00:15.908027 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:00:15.908027 [info ] [Thread-1 (]: 5 of 8 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m00:00:15.908027 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: idle check connection: sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843615.9080272
[0m00:00:15.908027 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, now test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778)
[0m00:00:15.908027 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: reusing connection test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843615.9080272
[0m00:00:15.908027 [debug] [Thread-1 (]: Databricks adapter: On thread (23500, 17032): None using default compute resource.
[0m00:00:15.908027 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: _acquire sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843615.9080272
[0m00:00:15.908027 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:00:15.927414 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:00:15.929332 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (compile): 00:00:15.908027 => 00:00:15.928414
[0m00:00:15.930388 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:00:15.934428 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:00:15.935426 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: get_thread_connection: sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.027399301528930664s, acqrelcnt: 1, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843615.9080272
[0m00:00:15.936332 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: idle check connection: sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0283052921295166s, acqrelcnt: 1, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843615.9080272
[0m00:00:15.937028 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:00:15.937028 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m00:00:16.187720 [debug] [Thread-1 (]: SQL status: OK in 0.25 seconds
[0m00:00:16.187720 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (execute): 00:00:15.930388 => 00:00:16.187720
[0m00:00:16.187720 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: _release sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843616.1877198
[0m00:00:16.187720 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: _release sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843616.1877198
[0m00:00:16.187720 [info ] [Thread-1 (]: 5 of 8 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.28s]
[0m00:00:16.187720 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:00:16.187720 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:00:16.187720 [info ] [Thread-1 (]: 6 of 8 START test unique_dim_product_product_sk ................................ [RUN]
[0m00:00:16.187720 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: idle check connection: sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843616.1877198
[0m00:00:16.203323 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, now test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c)
[0m00:00:16.203323 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: reusing connection test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.01560354232788086s, acqrelcnt: 0, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843616.1877198
[0m00:00:16.203323 [debug] [Thread-1 (]: Databricks adapter: On thread (23500, 17032): None using default compute resource.
[0m00:00:16.203323 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: _acquire sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.01560354232788086s, acqrelcnt: 1, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843616.1877198
[0m00:00:16.203323 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:00:16.203323 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:00:16.203323 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (compile): 00:00:16.203323 => 00:00:16.203323
[0m00:00:16.203323 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:00:16.203323 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:00:16.203323 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: get_thread_connection: sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.01560354232788086s, acqrelcnt: 1, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843616.1877198
[0m00:00:16.203323 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: idle check connection: sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.01560354232788086s, acqrelcnt: 1, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843616.1877198
[0m00:00:16.203323 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:00:16.203323 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    product_sk as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is not null
group by product_sk
having count(*) > 1



      
    ) dbt_internal_test
[0m00:00:16.573803 [debug] [Thread-1 (]: SQL status: OK in 0.3700000047683716 seconds
[0m00:00:16.573803 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (execute): 00:00:16.203323 => 00:00:16.573803
[0m00:00:16.573803 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: _release sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843616.573803
[0m00:00:16.573803 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: _release sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843616.573803
[0m00:00:16.573803 [info ] [Thread-1 (]: 6 of 8 PASS unique_dim_product_product_sk ...................................... [[32mPASS[0m in 0.39s]
[0m00:00:16.573803 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:00:16.573803 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:00:16.573803 [info ] [Thread-1 (]: 7 of 8 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m00:00:16.573803 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: idle check connection: sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843616.573803
[0m00:00:16.573803 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, now test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321)
[0m00:00:16.573803 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: reusing connection test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843616.573803
[0m00:00:16.573803 [debug] [Thread-1 (]: Databricks adapter: On thread (23500, 17032): None using default compute resource.
[0m00:00:16.589276 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: _acquire sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.01547384262084961s, acqrelcnt: 1, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843616.573803
[0m00:00:16.589276 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:00:16.589276 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:00:16.589276 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (compile): 00:00:16.589276 => 00:00:16.589276
[0m00:00:16.589276 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:00:16.589276 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:00:16.589276 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: get_thread_connection: sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.01547384262084961s, acqrelcnt: 1, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843616.573803
[0m00:00:16.589276 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: idle check connection: sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.01547384262084961s, acqrelcnt: 1, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843616.573803
[0m00:00:16.589276 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:00:16.589276 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m00:00:16.968041 [debug] [Thread-1 (]: SQL status: OK in 0.3799999952316284 seconds
[0m00:00:16.968041 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (execute): 00:00:16.589276 => 00:00:16.968041
[0m00:00:16.968041 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: _release sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843616.9680412
[0m00:00:16.968041 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: _release sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843616.9680412
[0m00:00:16.968041 [info ] [Thread-1 (]: 7 of 8 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.39s]
[0m00:00:16.968041 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:00:16.968041 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:00:16.968041 [info ] [Thread-1 (]: 8 of 8 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m00:00:16.983506 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: idle check connection: sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.015465736389160156s, acqrelcnt: 0, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843616.9680412
[0m00:00:16.984285 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, now test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493)
[0m00:00:16.984504 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: reusing connection test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.016463041305541992s, acqrelcnt: 0, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843616.9680412
[0m00:00:16.984504 [debug] [Thread-1 (]: Databricks adapter: On thread (23500, 17032): None using default compute resource.
[0m00:00:16.984504 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: _acquire sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.016463041305541992s, acqrelcnt: 1, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843616.9680412
[0m00:00:16.984504 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:00:16.984504 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:00:16.984504 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (compile): 00:00:16.984504 => 00:00:16.984504
[0m00:00:16.984504 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:00:16.984504 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:00:16.984504 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: get_thread_connection: sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.016463041305541992s, acqrelcnt: 1, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843616.9680412
[0m00:00:16.984504 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: idle check connection: sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.016463041305541992s, acqrelcnt: 1, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843616.9680412
[0m00:00:16.984504 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:00:16.996797 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m00:00:17.484713 [debug] [Thread-1 (]: SQL status: OK in 0.49000000953674316 seconds
[0m00:00:17.502581 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (execute): 00:00:16.984504 => 00:00:17.502581
[0m00:00:17.502581 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: _release sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843617.5025814
[0m00:00:17.502581 [debug] [Thread-1 (]: Databricks adapter: conn: 2609968456912: _release sess: 8a7346eb-c9a6-427f-9136-05d416364a01, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23500, 17032), cmpt: ``, lut: 1707843617.5025814
[0m00:00:17.502581 [info ] [Thread-1 (]: 8 of 8 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.53s]
[0m00:00:17.502581 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:00:17.502581 [debug] [MainThread]: Databricks adapter: conn: 2609970618576: idle check connection: sess: None, name: master, idle: 3.2983920574188232s, acqrelcnt: 0, lang: None, thrd: (23500, 13752), cmpt: ``, lut: 1707843614.2041893
[0m00:00:17.502581 [debug] [MainThread]: Databricks adapter: conn: 2609970618576: reusing connection master sess: None, name: master, idle: 3.2983920574188232s, acqrelcnt: 0, lang: None, thrd: (23500, 13752), cmpt: ``, lut: 1707843614.2041893
[0m00:00:17.514046 [debug] [MainThread]: Databricks adapter: Thread (23500, 13752) using default compute resource.
[0m00:00:17.514046 [debug] [MainThread]: Databricks adapter: conn: 2609970618576: _acquire sess: None, name: master, idle: 3.3098573684692383s, acqrelcnt: 1, lang: None, thrd: (23500, 13752), cmpt: ``, lut: 1707843614.2041893
[0m00:00:17.515056 [debug] [MainThread]: Databricks adapter: conn: 2609970618576: get_thread_connection: sess: None, name: master, idle: 3.3098573684692383s, acqrelcnt: 1, lang: None, thrd: (23500, 13752), cmpt: ``, lut: 1707843614.2041893
[0m00:00:17.515056 [debug] [MainThread]: Databricks adapter: conn: 2609970618576: idle check connection: sess: None, name: master, idle: 3.3108675479888916s, acqrelcnt: 1, lang: None, thrd: (23500, 13752), cmpt: ``, lut: 1707843614.2041893
[0m00:00:17.515056 [debug] [MainThread]: On master: ROLLBACK
[0m00:00:17.516054 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:00:17.875220 [debug] [MainThread]: Databricks adapter: conn: 2609970618576: session opened sess: 51315e62-9b7f-4ec5-a8a2-3381b4196868, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23500, 13752), cmpt: ``, lut: 1707843617.8752205
[0m00:00:17.875220 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:00:17.875220 [debug] [MainThread]: Databricks adapter: conn: 2609970618576: get_thread_connection: sess: 51315e62-9b7f-4ec5-a8a2-3381b4196868, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23500, 13752), cmpt: ``, lut: 1707843617.8752205
[0m00:00:17.875220 [debug] [MainThread]: Databricks adapter: conn: 2609970618576: idle check connection: sess: 51315e62-9b7f-4ec5-a8a2-3381b4196868, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23500, 13752), cmpt: ``, lut: 1707843617.8752205
[0m00:00:17.890718 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:00:17.890718 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:00:17.890718 [debug] [MainThread]: Databricks adapter: conn: 2609970618576: _release sess: 51315e62-9b7f-4ec5-a8a2-3381b4196868, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (23500, 13752), cmpt: ``, lut: 1707843617.8907182
[0m00:00:17.890718 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:00:17.890718 [debug] [MainThread]: On master: ROLLBACK
[0m00:00:17.890718 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:00:17.890718 [debug] [MainThread]: On master: Close
[0m00:00:17.984561 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m00:00:17.984561 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m00:00:17.984561 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:00:17.984561 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m00:00:18.081729 [debug] [MainThread]: Connection 'test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493' was properly closed.
[0m00:00:18.081729 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: ROLLBACK
[0m00:00:18.081729 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:00:18.081729 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m00:00:18.407547 [info ] [MainThread]: 
[0m00:00:18.407547 [info ] [MainThread]: Finished running 8 tests in 0 hours 0 minutes and 5.80 seconds (5.80s).
[0m00:00:18.423122 [debug] [MainThread]: Command end result
[0m00:00:18.436539 [info ] [MainThread]: 
[0m00:00:18.438723 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:00:18.440389 [info ] [MainThread]: 
[0m00:00:18.441417 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models\example\schema.yml)[0m
[0m00:00:18.443513 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m00:00:18.445055 [info ] [MainThread]: 
[0m00:00:18.446202 [info ] [MainThread]:   compiled Code at target\compiled\medallion_dbt_spark\models\example\schema.yml\not_null_my_first_dbt_model_id.sql
[0m00:00:18.446202 [info ] [MainThread]: 
[0m00:00:18.448604 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=1 SKIP=0 TOTAL=8
[0m00:00:18.449278 [debug] [MainThread]: Command `dbt test` failed at 00:00:18.449278 after 8.03 seconds
[0m00:00:18.449278 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F9B30D610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F9BD9C550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025FAE1D35D0>]}
[0m00:00:18.449278 [debug] [MainThread]: Flushing usage events
[0m00:00:58.028384 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020267C83ED0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020267C92F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020267C77FD0>]}


============================== 00:00:58.028384 | 30fb555d-bb1e-4189-ab87-8042b3df1790 ==============================
[0m00:00:58.028384 [info ] [MainThread]: Running with dbt=1.7.7
[0m00:00:58.028384 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt test', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m00:00:59.256476 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '30fb555d-bb1e-4189-ab87-8042b3df1790', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020279DA5450>]}
[0m00:00:59.334862 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '30fb555d-bb1e-4189-ab87-8042b3df1790', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020279CBB290>]}
[0m00:00:59.335773 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m00:00:59.346733 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m00:00:59.441818 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:00:59.442810 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://models\example\my_first_dbt_model.sql
[0m00:00:59.694108 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '30fb555d-bb1e-4189-ab87-8042b3df1790', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002027A248D10>]}
[0m00:00:59.709125 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '30fb555d-bb1e-4189-ab87-8042b3df1790', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002027A1AD190>]}
[0m00:00:59.709125 [info ] [MainThread]: Found 5 models, 7 snapshots, 8 tests, 9 sources, 0 exposures, 0 metrics, 535 macros, 0 groups, 0 semantic models
[0m00:00:59.724763 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '30fb555d-bb1e-4189-ab87-8042b3df1790', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002027A03A810>]}
[0m00:00:59.724763 [info ] [MainThread]: 
[0m00:00:59.728884 [debug] [MainThread]: Databricks adapter: conn: 2209662255632: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (11408, 26588), cmpt: ``, lut: None
[0m00:00:59.729244 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:00:59.729244 [debug] [MainThread]: Databricks adapter: Thread (11408, 26588) using default compute resource.
[0m00:00:59.730247 [debug] [MainThread]: Databricks adapter: conn: 2209662255632: _acquire sess: None, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (11408, 26588), cmpt: ``, lut: 1707843659.7302473
[0m00:00:59.731958 [debug] [ThreadPool]: Databricks adapter: conn: 2209659680848: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_snapshots, idle: 0s, acqrelcnt: 0, lang: None, thrd: (11408, 15372), cmpt: ``, lut: None
[0m00:00:59.732967 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_snapshots'
[0m00:00:59.732967 [debug] [ThreadPool]: Databricks adapter: Thread (11408, 15372) using default compute resource.
[0m00:00:59.734000 [debug] [ThreadPool]: Databricks adapter: conn: 2209659680848: _acquire sess: None, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (11408, 15372), cmpt: ``, lut: 1707843659.7329671
[0m00:00:59.738052 [debug] [ThreadPool]: Databricks adapter: conn: 2209659680848: get_thread_connection: sess: None, name: list_hive_metastore_snapshots, idle: 0.005084991455078125s, acqrelcnt: 1, lang: None, thrd: (11408, 15372), cmpt: ``, lut: 1707843659.7329671
[0m00:00:59.739054 [debug] [ThreadPool]: Databricks adapter: conn: 2209659680848: idle check connection: sess: None, name: list_hive_metastore_snapshots, idle: 0.006087541580200195s, acqrelcnt: 1, lang: None, thrd: (11408, 15372), cmpt: ``, lut: 1707843659.7329671
[0m00:00:59.739054 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:00:59.739967 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m00:00:59.739967 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:01:00.001822 [debug] [ThreadPool]: Databricks adapter: conn: 2209659680848: session opened sess: d00954c8-1274-4d28-ba3e-379f864942b0, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (11408, 15372), cmpt: ``, lut: 1707843660.0018227
[0m00:01:00.142387 [debug] [ThreadPool]: SQL status: OK in 0.4000000059604645 seconds
[0m00:01:00.242687 [debug] [ThreadPool]: Databricks adapter: conn: 2209659680848: get_thread_connection: sess: d00954c8-1274-4d28-ba3e-379f864942b0, name: list_hive_metastore_snapshots, idle: 0.24086499214172363s, acqrelcnt: 1, lang: None, thrd: (11408, 15372), cmpt: ``, lut: 1707843660.0018227
[0m00:01:00.242687 [debug] [ThreadPool]: Databricks adapter: conn: 2209659680848: idle check connection: sess: d00954c8-1274-4d28-ba3e-379f864942b0, name: list_hive_metastore_snapshots, idle: 0.24086499214172363s, acqrelcnt: 1, lang: None, thrd: (11408, 15372), cmpt: ``, lut: 1707843660.0018227
[0m00:01:00.242687 [debug] [ThreadPool]: Databricks adapter: conn: 2209659680848: get_thread_connection: sess: d00954c8-1274-4d28-ba3e-379f864942b0, name: list_hive_metastore_snapshots, idle: 0.24086499214172363s, acqrelcnt: 1, lang: None, thrd: (11408, 15372), cmpt: ``, lut: 1707843660.0018227
[0m00:01:00.242687 [debug] [ThreadPool]: Databricks adapter: conn: 2209659680848: idle check connection: sess: d00954c8-1274-4d28-ba3e-379f864942b0, name: list_hive_metastore_snapshots, idle: 0.24086499214172363s, acqrelcnt: 1, lang: None, thrd: (11408, 15372), cmpt: ``, lut: 1707843660.0018227
[0m00:01:00.242687 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:01:00.242687 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:01:00.242687 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m00:01:00.398978 [debug] [ThreadPool]: SQL status: OK in 0.1599999964237213 seconds
[0m00:01:00.414558 [debug] [ThreadPool]: Databricks adapter: conn: 2209659680848: get_thread_connection: sess: d00954c8-1274-4d28-ba3e-379f864942b0, name: list_hive_metastore_snapshots, idle: 0.4127357006072998s, acqrelcnt: 1, lang: None, thrd: (11408, 15372), cmpt: ``, lut: 1707843660.0018227
[0m00:01:00.414558 [debug] [ThreadPool]: Databricks adapter: conn: 2209659680848: idle check connection: sess: d00954c8-1274-4d28-ba3e-379f864942b0, name: list_hive_metastore_snapshots, idle: 0.4127357006072998s, acqrelcnt: 1, lang: None, thrd: (11408, 15372), cmpt: ``, lut: 1707843660.0018227
[0m00:01:00.414558 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:01:00.414558 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m00:01:00.586600 [debug] [ThreadPool]: SQL status: OK in 0.17000000178813934 seconds
[0m00:01:00.589228 [debug] [ThreadPool]: Databricks adapter: conn: 2209659680848: _release sess: d00954c8-1274-4d28-ba3e-379f864942b0, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (11408, 15372), cmpt: ``, lut: 1707843660.5892284
[0m00:01:00.589228 [debug] [ThreadPool]: Databricks adapter: conn: 2209659680848: idle check connection: sess: d00954c8-1274-4d28-ba3e-379f864942b0, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (11408, 15372), cmpt: ``, lut: 1707843660.5892284
[0m00:01:00.589228 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now list_hive_metastore_saleslt)
[0m00:01:00.589228 [debug] [ThreadPool]: Databricks adapter: conn: 2209659680848: reusing connection list_hive_metastore_snapshots sess: d00954c8-1274-4d28-ba3e-379f864942b0, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (11408, 15372), cmpt: ``, lut: 1707843660.5892284
[0m00:01:00.597917 [debug] [ThreadPool]: Databricks adapter: Thread (11408, 15372) using default compute resource.
[0m00:01:00.597917 [debug] [ThreadPool]: Databricks adapter: conn: 2209659680848: _acquire sess: d00954c8-1274-4d28-ba3e-379f864942b0, name: list_hive_metastore_saleslt, idle: 0.00868844985961914s, acqrelcnt: 1, lang: None, thrd: (11408, 15372), cmpt: ``, lut: 1707843660.5892284
[0m00:01:00.602425 [debug] [ThreadPool]: Databricks adapter: conn: 2209659680848: get_thread_connection: sess: d00954c8-1274-4d28-ba3e-379f864942b0, name: list_hive_metastore_saleslt, idle: 0.013196706771850586s, acqrelcnt: 1, lang: None, thrd: (11408, 15372), cmpt: ``, lut: 1707843660.5892284
[0m00:01:00.603519 [debug] [ThreadPool]: Databricks adapter: conn: 2209659680848: idle check connection: sess: d00954c8-1274-4d28-ba3e-379f864942b0, name: list_hive_metastore_saleslt, idle: 0.014290809631347656s, acqrelcnt: 1, lang: None, thrd: (11408, 15372), cmpt: ``, lut: 1707843660.5892284
[0m00:01:00.603519 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:01:00.603519 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m00:01:00.740216 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m00:01:00.745193 [debug] [ThreadPool]: Databricks adapter: conn: 2209659680848: get_thread_connection: sess: d00954c8-1274-4d28-ba3e-379f864942b0, name: list_hive_metastore_saleslt, idle: 0.15596485137939453s, acqrelcnt: 1, lang: None, thrd: (11408, 15372), cmpt: ``, lut: 1707843660.5892284
[0m00:01:00.745193 [debug] [ThreadPool]: Databricks adapter: conn: 2209659680848: idle check connection: sess: d00954c8-1274-4d28-ba3e-379f864942b0, name: list_hive_metastore_saleslt, idle: 0.15596485137939453s, acqrelcnt: 1, lang: None, thrd: (11408, 15372), cmpt: ``, lut: 1707843660.5892284
[0m00:01:00.745193 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:01:00.745193 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m00:01:00.859409 [debug] [ThreadPool]: SQL status: OK in 0.10999999940395355 seconds
[0m00:01:00.882963 [debug] [ThreadPool]: Databricks adapter: conn: 2209659680848: get_thread_connection: sess: d00954c8-1274-4d28-ba3e-379f864942b0, name: list_hive_metastore_saleslt, idle: 0.2937347888946533s, acqrelcnt: 1, lang: None, thrd: (11408, 15372), cmpt: ``, lut: 1707843660.5892284
[0m00:01:00.882963 [debug] [ThreadPool]: Databricks adapter: conn: 2209659680848: idle check connection: sess: d00954c8-1274-4d28-ba3e-379f864942b0, name: list_hive_metastore_saleslt, idle: 0.2937347888946533s, acqrelcnt: 1, lang: None, thrd: (11408, 15372), cmpt: ``, lut: 1707843660.5892284
[0m00:01:00.882963 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:01:00.882963 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m00:01:01.054856 [debug] [ThreadPool]: SQL status: OK in 0.17000000178813934 seconds
[0m00:01:01.054856 [debug] [ThreadPool]: Databricks adapter: conn: 2209659680848: _release sess: d00954c8-1274-4d28-ba3e-379f864942b0, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (11408, 15372), cmpt: ``, lut: 1707843661.0548563
[0m00:01:01.054856 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '30fb555d-bb1e-4189-ab87-8042b3df1790', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002027A2021D0>]}
[0m00:01:01.054856 [debug] [MainThread]: Databricks adapter: conn: 2209662255632: get_thread_connection: sess: None, name: master, idle: 1.3246090412139893s, acqrelcnt: 1, lang: None, thrd: (11408, 26588), cmpt: ``, lut: 1707843659.7302473
[0m00:01:01.054856 [debug] [MainThread]: Databricks adapter: conn: 2209662255632: idle check connection: sess: None, name: master, idle: 1.3246090412139893s, acqrelcnt: 1, lang: None, thrd: (11408, 26588), cmpt: ``, lut: 1707843659.7302473
[0m00:01:01.054856 [debug] [MainThread]: Databricks adapter: conn: 2209662255632: get_thread_connection: sess: None, name: master, idle: 1.3246090412139893s, acqrelcnt: 1, lang: None, thrd: (11408, 26588), cmpt: ``, lut: 1707843659.7302473
[0m00:01:01.054856 [debug] [MainThread]: Databricks adapter: conn: 2209662255632: idle check connection: sess: None, name: master, idle: 1.3246090412139893s, acqrelcnt: 1, lang: None, thrd: (11408, 26588), cmpt: ``, lut: 1707843659.7302473
[0m00:01:01.054856 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:01:01.054856 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:01:01.054856 [debug] [MainThread]: Databricks adapter: conn: 2209662255632: _release sess: None, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (11408, 26588), cmpt: ``, lut: 1707843661.0548563
[0m00:01:01.054856 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:01:01.070386 [info ] [MainThread]: 
[0m00:01:01.070386 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:01:01.070386 [info ] [Thread-1 (]: 1 of 8 START test not_null_dim_product_product_name ............................ [RUN]
[0m00:01:01.076900 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: Creating DatabricksDBTConnection sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0s, acqrelcnt: 0, lang: None, thrd: (11408, 9024), cmpt: ``, lut: None
[0m00:01:01.076900 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5'
[0m00:01:01.078013 [debug] [Thread-1 (]: Databricks adapter: On thread (11408, 9024): None using default compute resource.
[0m00:01:01.078013 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: _acquire sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843661.0780132
[0m00:01:01.078738 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:01:01.096083 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:01:01.096083 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (compile): 00:01:01.078738 => 00:01:01.096083
[0m00:01:01.096083 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:01:01.112916 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:01:01.112916 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.034903764724731445s, acqrelcnt: 1, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843661.0780132
[0m00:01:01.112916 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.034903764724731445s, acqrelcnt: 1, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843661.0780132
[0m00:01:01.112916 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.034903764724731445s, acqrelcnt: 1, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843661.0780132
[0m00:01:01.112916 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.034903764724731445s, acqrelcnt: 1, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843661.0780132
[0m00:01:01.112916 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:01:01.112916 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:01:01.112916 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_name
from `hive_metastore`.`saleslt`.`dim_product`
where product_name is null



      
    ) dbt_internal_test
[0m00:01:01.112916 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:01:01.372708 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: session opened sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843661.3727086
[0m00:01:01.653959 [debug] [Thread-1 (]: SQL status: OK in 0.5400000214576721 seconds
[0m00:01:01.669562 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (execute): 00:01:01.096083 => 00:01:01.669562
[0m00:01:01.669562 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: _release sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843661.6695626
[0m00:01:01.669562 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: _release sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843661.6695626
[0m00:01:01.669562 [info ] [Thread-1 (]: 1 of 8 PASS not_null_dim_product_product_name .................................. [[32mPASS[0m in 0.60s]
[0m00:01:01.669562 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:01:01.669562 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:01:01.669562 [info ] [Thread-1 (]: 2 of 8 START test not_null_dim_product_product_sk .............................. [RUN]
[0m00:01:01.669562 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: idle check connection: sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843661.6695626
[0m00:01:01.669562 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, now test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417)
[0m00:01:01.669562 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843661.6695626
[0m00:01:01.669562 [debug] [Thread-1 (]: Databricks adapter: On thread (11408, 9024): None using default compute resource.
[0m00:01:01.669562 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: _acquire sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843661.6695626
[0m00:01:01.669562 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:01:01.691003 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:01:01.691421 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (compile): 00:01:01.685080 => 00:01:01.691421
[0m00:01:01.691421 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:01:01.698234 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:01:01.700077 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: get_thread_connection: sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.02948141098022461s, acqrelcnt: 1, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843661.6695626
[0m00:01:01.700077 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: idle check connection: sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.030514955520629883s, acqrelcnt: 1, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843661.6695626
[0m00:01:01.701119 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:01:01.702086 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_sk
from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is null



      
    ) dbt_internal_test
[0m00:01:01.971650 [debug] [Thread-1 (]: SQL status: OK in 0.27000001072883606 seconds
[0m00:01:01.971650 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (execute): 00:01:01.691421 => 00:01:01.971650
[0m00:01:01.971650 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: _release sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843661.9716506
[0m00:01:01.971650 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: _release sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843661.9716506
[0m00:01:01.971650 [info ] [Thread-1 (]: 2 of 8 PASS not_null_dim_product_product_sk .................................... [[32mPASS[0m in 0.30s]
[0m00:01:01.971650 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:01:01.971650 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:01:01.971650 [info ] [Thread-1 (]: 3 of 8 START test not_null_dim_product_sellstartdate ........................... [RUN]
[0m00:01:01.971650 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: idle check connection: sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843661.9716506
[0m00:01:01.971650 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, now test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546)
[0m00:01:01.971650 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843661.9716506
[0m00:01:01.971650 [debug] [Thread-1 (]: Databricks adapter: On thread (11408, 9024): None using default compute resource.
[0m00:01:01.971650 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: _acquire sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843661.9716506
[0m00:01:01.971650 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:01:01.988211 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:01:01.989180 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (compile): 00:01:01.971650 => 00:01:01.989180
[0m00:01:01.990204 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:01:01.993102 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:01:01.994652 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: get_thread_connection: sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.022470951080322266s, acqrelcnt: 1, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843661.9716506
[0m00:01:01.994652 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: idle check connection: sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.023002147674560547s, acqrelcnt: 1, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843661.9716506
[0m00:01:01.995778 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:01:01.996699 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select sellstartdate
from `hive_metastore`.`saleslt`.`dim_product`
where sellstartdate is null



      
    ) dbt_internal_test
[0m00:01:02.237248 [debug] [Thread-1 (]: SQL status: OK in 0.23999999463558197 seconds
[0m00:01:02.237248 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (execute): 00:01:01.990204 => 00:01:02.237248
[0m00:01:02.237248 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: _release sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843662.2372484
[0m00:01:02.237248 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: _release sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843662.2372484
[0m00:01:02.237248 [info ] [Thread-1 (]: 3 of 8 PASS not_null_dim_product_sellstartdate ................................. [[32mPASS[0m in 0.27s]
[0m00:01:02.237248 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:01:02.237248 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:01:02.237248 [info ] [Thread-1 (]: 4 of 8 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m00:01:02.237248 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: idle check connection: sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843662.2372484
[0m00:01:02.237248 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, now test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710)
[0m00:01:02.252704 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: reusing connection test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.015456199645996094s, acqrelcnt: 0, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843662.2372484
[0m00:01:02.252704 [debug] [Thread-1 (]: Databricks adapter: On thread (11408, 9024): None using default compute resource.
[0m00:01:02.252704 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: _acquire sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.015456199645996094s, acqrelcnt: 1, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843662.2372484
[0m00:01:02.252704 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:01:02.252704 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:01:02.252704 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 00:01:02.252704 => 00:01:02.252704
[0m00:01:02.252704 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:01:02.252704 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:01:02.252704 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: get_thread_connection: sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.015456199645996094s, acqrelcnt: 1, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843662.2372484
[0m00:01:02.252704 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: idle check connection: sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.015456199645996094s, acqrelcnt: 1, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843662.2372484
[0m00:01:02.252704 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:01:02.252704 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m00:01:02.566957 [debug] [Thread-1 (]: SQL status: OK in 0.3100000023841858 seconds
[0m00:01:02.566957 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 00:01:02.252704 => 00:01:02.566957
[0m00:01:02.566957 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: _release sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843662.5669575
[0m00:01:02.566957 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: _release sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843662.5669575
[0m00:01:02.566957 [error] [Thread-1 (]: 4 of 8 FAIL 1 not_null_my_first_dbt_model_id ................................... [[31mFAIL 1[0m in 0.33s]
[0m00:01:02.566957 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:01:02.566957 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:01:02.566957 [info ] [Thread-1 (]: 5 of 8 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m00:01:02.582497 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: idle check connection: sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.015539884567260742s, acqrelcnt: 0, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843662.5669575
[0m00:01:02.582497 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, now test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778)
[0m00:01:02.582497 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: reusing connection test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.015539884567260742s, acqrelcnt: 0, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843662.5669575
[0m00:01:02.582497 [debug] [Thread-1 (]: Databricks adapter: On thread (11408, 9024): None using default compute resource.
[0m00:01:02.582497 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: _acquire sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.015539884567260742s, acqrelcnt: 1, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843662.5669575
[0m00:01:02.582497 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:01:02.582497 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:01:02.582497 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (compile): 00:01:02.582497 => 00:01:02.582497
[0m00:01:02.582497 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:01:02.582497 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:01:02.582497 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: get_thread_connection: sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.015539884567260742s, acqrelcnt: 1, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843662.5669575
[0m00:01:02.582497 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: idle check connection: sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.015539884567260742s, acqrelcnt: 1, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843662.5669575
[0m00:01:02.582497 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:01:02.582497 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m00:01:02.879865 [debug] [Thread-1 (]: SQL status: OK in 0.30000001192092896 seconds
[0m00:01:02.879865 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (execute): 00:01:02.582497 => 00:01:02.879865
[0m00:01:02.879865 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: _release sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843662.8798656
[0m00:01:02.879865 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: _release sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843662.8798656
[0m00:01:02.879865 [info ] [Thread-1 (]: 5 of 8 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.30s]
[0m00:01:02.879865 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:01:02.895536 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:01:02.895536 [info ] [Thread-1 (]: 6 of 8 START test unique_dim_product_product_sk ................................ [RUN]
[0m00:01:02.895536 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: idle check connection: sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0156707763671875s, acqrelcnt: 0, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843662.8798656
[0m00:01:02.895536 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, now test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c)
[0m00:01:02.895536 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: reusing connection test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0156707763671875s, acqrelcnt: 0, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843662.8798656
[0m00:01:02.895536 [debug] [Thread-1 (]: Databricks adapter: On thread (11408, 9024): None using default compute resource.
[0m00:01:02.895536 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: _acquire sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0156707763671875s, acqrelcnt: 1, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843662.8798656
[0m00:01:02.895536 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:01:02.895536 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:01:02.895536 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (compile): 00:01:02.895536 => 00:01:02.895536
[0m00:01:02.895536 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:01:02.895536 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:01:02.895536 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: get_thread_connection: sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0156707763671875s, acqrelcnt: 1, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843662.8798656
[0m00:01:02.895536 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: idle check connection: sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0156707763671875s, acqrelcnt: 1, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843662.8798656
[0m00:01:02.911130 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:01:02.911130 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    product_sk as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is not null
group by product_sk
having count(*) > 1



      
    ) dbt_internal_test
[0m00:01:03.553520 [debug] [Thread-1 (]: SQL status: OK in 0.6399999856948853 seconds
[0m00:01:03.553520 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (execute): 00:01:02.895536 => 00:01:03.553520
[0m00:01:03.553520 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: _release sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843663.5535204
[0m00:01:03.553520 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: _release sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843663.5535204
[0m00:01:03.553520 [info ] [Thread-1 (]: 6 of 8 PASS unique_dim_product_product_sk ...................................... [[32mPASS[0m in 0.66s]
[0m00:01:03.553520 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:01:03.553520 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:01:03.553520 [info ] [Thread-1 (]: 7 of 8 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m00:01:03.553520 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: idle check connection: sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843663.5535204
[0m00:01:03.553520 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, now test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321)
[0m00:01:03.553520 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: reusing connection test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843663.5535204
[0m00:01:03.553520 [debug] [Thread-1 (]: Databricks adapter: On thread (11408, 9024): None using default compute resource.
[0m00:01:03.569017 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: _acquire sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.01549673080444336s, acqrelcnt: 1, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843663.5535204
[0m00:01:03.569017 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:01:03.576303 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:01:03.577023 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (compile): 00:01:03.569838 => 00:01:03.577023
[0m00:01:03.577023 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:01:03.582741 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:01:03.583677 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: get_thread_connection: sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.030156850814819336s, acqrelcnt: 1, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843663.5535204
[0m00:01:03.584621 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: idle check connection: sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.03110051155090332s, acqrelcnt: 1, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843663.5535204
[0m00:01:03.585627 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:01:03.585627 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m00:01:03.915881 [debug] [Thread-1 (]: SQL status: OK in 0.33000001311302185 seconds
[0m00:01:03.915881 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (execute): 00:01:03.577023 => 00:01:03.915881
[0m00:01:03.915881 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: _release sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843663.9158812
[0m00:01:03.915881 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: _release sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843663.9158812
[0m00:01:03.915881 [info ] [Thread-1 (]: 7 of 8 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.36s]
[0m00:01:03.915881 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:01:03.915881 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:01:03.915881 [info ] [Thread-1 (]: 8 of 8 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m00:01:03.930891 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: idle check connection: sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.015009880065917969s, acqrelcnt: 0, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843663.9158812
[0m00:01:03.930891 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, now test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493)
[0m00:01:03.930891 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: reusing connection test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.015009880065917969s, acqrelcnt: 0, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843663.9158812
[0m00:01:03.930891 [debug] [Thread-1 (]: Databricks adapter: On thread (11408, 9024): None using default compute resource.
[0m00:01:03.930891 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: _acquire sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.015009880065917969s, acqrelcnt: 1, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843663.9158812
[0m00:01:03.930891 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:01:03.930891 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:01:03.930891 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (compile): 00:01:03.930891 => 00:01:03.930891
[0m00:01:03.930891 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:01:03.930891 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:01:03.930891 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: get_thread_connection: sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.015009880065917969s, acqrelcnt: 1, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843663.9158812
[0m00:01:03.930891 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: idle check connection: sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.015009880065917969s, acqrelcnt: 1, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843663.9158812
[0m00:01:03.930891 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:01:03.930891 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m00:01:04.316073 [debug] [Thread-1 (]: SQL status: OK in 0.38999998569488525 seconds
[0m00:01:04.316073 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (execute): 00:01:03.930891 => 00:01:04.316073
[0m00:01:04.331661 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: _release sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843664.3160737
[0m00:01:04.331661 [debug] [Thread-1 (]: Databricks adapter: conn: 2209661643664: _release sess: 0212e66a-ffa0-4a40-acd9-fbcf640eab47, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (11408, 9024), cmpt: ``, lut: 1707843664.3316615
[0m00:01:04.331661 [info ] [Thread-1 (]: 8 of 8 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.40s]
[0m00:01:04.331661 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:01:04.331661 [debug] [MainThread]: Databricks adapter: conn: 2209662255632: idle check connection: sess: None, name: master, idle: 3.2768051624298096s, acqrelcnt: 0, lang: None, thrd: (11408, 26588), cmpt: ``, lut: 1707843661.0548563
[0m00:01:04.331661 [debug] [MainThread]: Databricks adapter: conn: 2209662255632: reusing connection master sess: None, name: master, idle: 3.2768051624298096s, acqrelcnt: 0, lang: None, thrd: (11408, 26588), cmpt: ``, lut: 1707843661.0548563
[0m00:01:04.331661 [debug] [MainThread]: Databricks adapter: Thread (11408, 26588) using default compute resource.
[0m00:01:04.331661 [debug] [MainThread]: Databricks adapter: conn: 2209662255632: _acquire sess: None, name: master, idle: 3.2768051624298096s, acqrelcnt: 1, lang: None, thrd: (11408, 26588), cmpt: ``, lut: 1707843661.0548563
[0m00:01:04.331661 [debug] [MainThread]: Databricks adapter: conn: 2209662255632: get_thread_connection: sess: None, name: master, idle: 3.2768051624298096s, acqrelcnt: 1, lang: None, thrd: (11408, 26588), cmpt: ``, lut: 1707843661.0548563
[0m00:01:04.331661 [debug] [MainThread]: Databricks adapter: conn: 2209662255632: idle check connection: sess: None, name: master, idle: 3.2768051624298096s, acqrelcnt: 1, lang: None, thrd: (11408, 26588), cmpt: ``, lut: 1707843661.0548563
[0m00:01:04.331661 [debug] [MainThread]: On master: ROLLBACK
[0m00:01:04.331661 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:01:04.606560 [debug] [MainThread]: Databricks adapter: conn: 2209662255632: session opened sess: 8c671f2f-9854-4132-a25f-c88b48ceb5be, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (11408, 26588), cmpt: ``, lut: 1707843664.6065607
[0m00:01:04.606560 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:01:04.606560 [debug] [MainThread]: Databricks adapter: conn: 2209662255632: get_thread_connection: sess: 8c671f2f-9854-4132-a25f-c88b48ceb5be, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (11408, 26588), cmpt: ``, lut: 1707843664.6065607
[0m00:01:04.606560 [debug] [MainThread]: Databricks adapter: conn: 2209662255632: idle check connection: sess: 8c671f2f-9854-4132-a25f-c88b48ceb5be, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (11408, 26588), cmpt: ``, lut: 1707843664.6065607
[0m00:01:04.606560 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:01:04.622028 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:01:04.622028 [debug] [MainThread]: Databricks adapter: conn: 2209662255632: _release sess: 8c671f2f-9854-4132-a25f-c88b48ceb5be, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (11408, 26588), cmpt: ``, lut: 1707843664.6220288
[0m00:01:04.622028 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:01:04.622028 [debug] [MainThread]: On master: ROLLBACK
[0m00:01:04.622028 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:01:04.622028 [debug] [MainThread]: On master: Close
[0m00:01:04.731404 [debug] [MainThread]: Connection 'list_hive_metastore_saleslt' was properly closed.
[0m00:01:04.731404 [debug] [MainThread]: On list_hive_metastore_saleslt: ROLLBACK
[0m00:01:04.731404 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:01:04.731404 [debug] [MainThread]: On list_hive_metastore_saleslt: Close
[0m00:01:04.809684 [debug] [MainThread]: Connection 'test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493' was properly closed.
[0m00:01:04.809684 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: ROLLBACK
[0m00:01:04.809684 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:01:04.809684 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m00:01:04.903405 [info ] [MainThread]: 
[0m00:01:04.903405 [info ] [MainThread]: Finished running 8 tests in 0 hours 0 minutes and 5.18 seconds (5.18s).
[0m00:01:04.903405 [debug] [MainThread]: Command end result
[0m00:01:04.933055 [info ] [MainThread]: 
[0m00:01:04.935070 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:01:04.937295 [info ] [MainThread]: 
[0m00:01:04.938303 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models\example\schema.yml)[0m
[0m00:01:04.939302 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m00:01:04.940544 [info ] [MainThread]: 
[0m00:01:04.940972 [info ] [MainThread]:   compiled Code at target\compiled\medallion_dbt_spark\models\example\schema.yml\not_null_my_first_dbt_model_id.sql
[0m00:01:04.942126 [info ] [MainThread]: 
[0m00:01:04.942126 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=1 SKIP=0 TOTAL=8
[0m00:01:04.946310 [debug] [MainThread]: Command `dbt test` failed at 00:01:04.946310 after 6.97 seconds
[0m00:01:04.947404 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202675ECE10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020267C46BD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020267693610>]}
[0m00:01:04.948408 [debug] [MainThread]: Flushing usage events
[0m00:02:25.365187 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD52F34E50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD52F83390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD52F83CD0>]}


============================== 00:02:25.380897 | d6b03fd7-4057-48fd-b215-f88c8dbc73db ==============================
[0m00:02:25.380897 [info ] [MainThread]: Running with dbt=1.7.7
[0m00:02:25.380897 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt test', 'send_anonymous_usage_stats': 'True'}
[0m00:02:26.557993 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd6b03fd7-4057-48fd-b215-f88c8dbc73db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD6508F550>]}
[0m00:02:26.620902 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd6b03fd7-4057-48fd-b215-f88c8dbc73db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD625BCE10>]}
[0m00:02:26.620902 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m00:02:26.636435 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m00:02:26.727262 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:02:26.728255 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://models\example\my_first_dbt_model.sql
[0m00:02:26.976343 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd6b03fd7-4057-48fd-b215-f88c8dbc73db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD654DF750>]}
[0m00:02:26.991925 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd6b03fd7-4057-48fd-b215-f88c8dbc73db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD652AC710>]}
[0m00:02:26.991925 [info ] [MainThread]: Found 5 models, 7 snapshots, 8 tests, 9 sources, 0 exposures, 0 metrics, 535 macros, 0 groups, 0 semantic models
[0m00:02:26.991925 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd6b03fd7-4057-48fd-b215-f88c8dbc73db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD655B5150>]}
[0m00:02:26.991925 [info ] [MainThread]: 
[0m00:02:26.991925 [debug] [MainThread]: Databricks adapter: conn: 2050396936400: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (2348, 500), cmpt: ``, lut: None
[0m00:02:27.008766 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:02:27.008766 [debug] [MainThread]: Databricks adapter: Thread (2348, 500) using default compute resource.
[0m00:02:27.009933 [debug] [MainThread]: Databricks adapter: conn: 2050396936400: _acquire sess: None, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (2348, 500), cmpt: ``, lut: 1707843747.0099335
[0m00:02:27.012943 [debug] [ThreadPool]: Databricks adapter: conn: 2050395607376: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_saleslt, idle: 0s, acqrelcnt: 0, lang: None, thrd: (2348, 7840), cmpt: ``, lut: None
[0m00:02:27.014243 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m00:02:27.014743 [debug] [ThreadPool]: Databricks adapter: Thread (2348, 7840) using default compute resource.
[0m00:02:27.014743 [debug] [ThreadPool]: Databricks adapter: conn: 2050395607376: _acquire sess: None, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (2348, 7840), cmpt: ``, lut: 1707843747.0147433
[0m00:02:27.016250 [debug] [ThreadPool]: Databricks adapter: conn: 2050395607376: get_thread_connection: sess: None, name: list_hive_metastore_saleslt, idle: 0.0015070438385009766s, acqrelcnt: 1, lang: None, thrd: (2348, 7840), cmpt: ``, lut: 1707843747.0147433
[0m00:02:27.016250 [debug] [ThreadPool]: Databricks adapter: conn: 2050395607376: idle check connection: sess: None, name: list_hive_metastore_saleslt, idle: 0.0015070438385009766s, acqrelcnt: 1, lang: None, thrd: (2348, 7840), cmpt: ``, lut: 1707843747.0147433
[0m00:02:27.016250 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:02:27.016250 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m00:02:27.023408 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:02:27.928551 [debug] [ThreadPool]: Databricks adapter: conn: 2050395607376: session opened sess: 06f5c933-d9d1-4b55-b9b2-51e396a3577d, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (2348, 7840), cmpt: ``, lut: 1707843747.9285512
[0m00:02:28.393546 [debug] [ThreadPool]: SQL status: OK in 1.3700000047683716 seconds
[0m00:02:28.471678 [debug] [ThreadPool]: Databricks adapter: conn: 2050395607376: get_thread_connection: sess: 06f5c933-d9d1-4b55-b9b2-51e396a3577d, name: list_hive_metastore_saleslt, idle: 0.5431270599365234s, acqrelcnt: 1, lang: None, thrd: (2348, 7840), cmpt: ``, lut: 1707843747.9285512
[0m00:02:28.471678 [debug] [ThreadPool]: Databricks adapter: conn: 2050395607376: idle check connection: sess: 06f5c933-d9d1-4b55-b9b2-51e396a3577d, name: list_hive_metastore_saleslt, idle: 0.5431270599365234s, acqrelcnt: 1, lang: None, thrd: (2348, 7840), cmpt: ``, lut: 1707843747.9285512
[0m00:02:28.471678 [debug] [ThreadPool]: Databricks adapter: conn: 2050395607376: get_thread_connection: sess: 06f5c933-d9d1-4b55-b9b2-51e396a3577d, name: list_hive_metastore_saleslt, idle: 0.5431270599365234s, acqrelcnt: 1, lang: None, thrd: (2348, 7840), cmpt: ``, lut: 1707843747.9285512
[0m00:02:28.471678 [debug] [ThreadPool]: Databricks adapter: conn: 2050395607376: idle check connection: sess: 06f5c933-d9d1-4b55-b9b2-51e396a3577d, name: list_hive_metastore_saleslt, idle: 0.5431270599365234s, acqrelcnt: 1, lang: None, thrd: (2348, 7840), cmpt: ``, lut: 1707843747.9285512
[0m00:02:28.471678 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:02:28.471678 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:02:28.471678 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m00:02:28.703333 [debug] [ThreadPool]: SQL status: OK in 0.23000000417232513 seconds
[0m00:02:28.709326 [debug] [ThreadPool]: Databricks adapter: conn: 2050395607376: get_thread_connection: sess: 06f5c933-d9d1-4b55-b9b2-51e396a3577d, name: list_hive_metastore_saleslt, idle: 0.7807750701904297s, acqrelcnt: 1, lang: None, thrd: (2348, 7840), cmpt: ``, lut: 1707843747.9285512
[0m00:02:28.709326 [debug] [ThreadPool]: Databricks adapter: conn: 2050395607376: idle check connection: sess: 06f5c933-d9d1-4b55-b9b2-51e396a3577d, name: list_hive_metastore_saleslt, idle: 0.7807750701904297s, acqrelcnt: 1, lang: None, thrd: (2348, 7840), cmpt: ``, lut: 1707843747.9285512
[0m00:02:28.709326 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:02:28.709326 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m00:02:28.867634 [debug] [ThreadPool]: SQL status: OK in 0.1599999964237213 seconds
[0m00:02:28.867634 [debug] [ThreadPool]: Databricks adapter: conn: 2050395607376: _release sess: 06f5c933-d9d1-4b55-b9b2-51e396a3577d, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (2348, 7840), cmpt: ``, lut: 1707843748.867635
[0m00:02:28.867634 [debug] [ThreadPool]: Databricks adapter: conn: 2050395607376: idle check connection: sess: 06f5c933-d9d1-4b55-b9b2-51e396a3577d, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (2348, 7840), cmpt: ``, lut: 1707843748.867635
[0m00:02:28.867634 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m00:02:28.883090 [debug] [ThreadPool]: Databricks adapter: conn: 2050395607376: reusing connection list_hive_metastore_saleslt sess: 06f5c933-d9d1-4b55-b9b2-51e396a3577d, name: list_hive_metastore_snapshots, idle: 0.015455245971679688s, acqrelcnt: 0, lang: None, thrd: (2348, 7840), cmpt: ``, lut: 1707843748.867635
[0m00:02:28.883090 [debug] [ThreadPool]: Databricks adapter: Thread (2348, 7840) using default compute resource.
[0m00:02:28.883090 [debug] [ThreadPool]: Databricks adapter: conn: 2050395607376: _acquire sess: 06f5c933-d9d1-4b55-b9b2-51e396a3577d, name: list_hive_metastore_snapshots, idle: 0.015455245971679688s, acqrelcnt: 1, lang: None, thrd: (2348, 7840), cmpt: ``, lut: 1707843748.867635
[0m00:02:28.883090 [debug] [ThreadPool]: Databricks adapter: conn: 2050395607376: get_thread_connection: sess: 06f5c933-d9d1-4b55-b9b2-51e396a3577d, name: list_hive_metastore_snapshots, idle: 0.015455245971679688s, acqrelcnt: 1, lang: None, thrd: (2348, 7840), cmpt: ``, lut: 1707843748.867635
[0m00:02:28.883090 [debug] [ThreadPool]: Databricks adapter: conn: 2050395607376: idle check connection: sess: 06f5c933-d9d1-4b55-b9b2-51e396a3577d, name: list_hive_metastore_snapshots, idle: 0.015455245971679688s, acqrelcnt: 1, lang: None, thrd: (2348, 7840), cmpt: ``, lut: 1707843748.867635
[0m00:02:28.883090 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:02:28.883090 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m00:02:28.992618 [debug] [ThreadPool]: SQL status: OK in 0.10999999940395355 seconds
[0m00:02:29.008089 [debug] [ThreadPool]: Databricks adapter: conn: 2050395607376: get_thread_connection: sess: 06f5c933-d9d1-4b55-b9b2-51e396a3577d, name: list_hive_metastore_snapshots, idle: 0.14045453071594238s, acqrelcnt: 1, lang: None, thrd: (2348, 7840), cmpt: ``, lut: 1707843748.867635
[0m00:02:29.008089 [debug] [ThreadPool]: Databricks adapter: conn: 2050395607376: idle check connection: sess: 06f5c933-d9d1-4b55-b9b2-51e396a3577d, name: list_hive_metastore_snapshots, idle: 0.14045453071594238s, acqrelcnt: 1, lang: None, thrd: (2348, 7840), cmpt: ``, lut: 1707843748.867635
[0m00:02:29.008089 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:02:29.008089 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m00:02:29.117592 [debug] [ThreadPool]: SQL status: OK in 0.10999999940395355 seconds
[0m00:02:29.133091 [debug] [ThreadPool]: Databricks adapter: conn: 2050395607376: get_thread_connection: sess: 06f5c933-d9d1-4b55-b9b2-51e396a3577d, name: list_hive_metastore_snapshots, idle: 0.2654569149017334s, acqrelcnt: 1, lang: None, thrd: (2348, 7840), cmpt: ``, lut: 1707843748.867635
[0m00:02:29.133091 [debug] [ThreadPool]: Databricks adapter: conn: 2050395607376: idle check connection: sess: 06f5c933-d9d1-4b55-b9b2-51e396a3577d, name: list_hive_metastore_snapshots, idle: 0.2654569149017334s, acqrelcnt: 1, lang: None, thrd: (2348, 7840), cmpt: ``, lut: 1707843748.867635
[0m00:02:29.133091 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:02:29.133091 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m00:02:29.273783 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m00:02:29.289471 [debug] [ThreadPool]: Databricks adapter: conn: 2050395607376: _release sess: 06f5c933-d9d1-4b55-b9b2-51e396a3577d, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (2348, 7840), cmpt: ``, lut: 1707843749.273784
[0m00:02:29.289471 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd6b03fd7-4057-48fd-b215-f88c8dbc73db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD65627DD0>]}
[0m00:02:29.289471 [debug] [MainThread]: Databricks adapter: conn: 2050396936400: get_thread_connection: sess: None, name: master, idle: 2.2795374393463135s, acqrelcnt: 1, lang: None, thrd: (2348, 500), cmpt: ``, lut: 1707843747.0099335
[0m00:02:29.289471 [debug] [MainThread]: Databricks adapter: conn: 2050396936400: idle check connection: sess: None, name: master, idle: 2.2795374393463135s, acqrelcnt: 1, lang: None, thrd: (2348, 500), cmpt: ``, lut: 1707843747.0099335
[0m00:02:29.289471 [debug] [MainThread]: Databricks adapter: conn: 2050396936400: get_thread_connection: sess: None, name: master, idle: 2.2795374393463135s, acqrelcnt: 1, lang: None, thrd: (2348, 500), cmpt: ``, lut: 1707843747.0099335
[0m00:02:29.289471 [debug] [MainThread]: Databricks adapter: conn: 2050396936400: idle check connection: sess: None, name: master, idle: 2.2795374393463135s, acqrelcnt: 1, lang: None, thrd: (2348, 500), cmpt: ``, lut: 1707843747.0099335
[0m00:02:29.289471 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:02:29.289471 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:02:29.289471 [debug] [MainThread]: Databricks adapter: conn: 2050396936400: _release sess: None, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (2348, 500), cmpt: ``, lut: 1707843749.289471
[0m00:02:29.289471 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:02:29.289471 [info ] [MainThread]: 
[0m00:02:29.289471 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:02:29.289471 [info ] [Thread-1 (]: 1 of 8 START test not_null_dim_product_product_name ............................ [RUN]
[0m00:02:29.305233 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: Creating DatabricksDBTConnection sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0s, acqrelcnt: 0, lang: None, thrd: (2348, 22428), cmpt: ``, lut: None
[0m00:02:29.306272 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5'
[0m00:02:29.306400 [debug] [Thread-1 (]: Databricks adapter: On thread (2348, 22428): None using default compute resource.
[0m00:02:29.307419 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: _acquire sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843749.3074195
[0m00:02:29.307419 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:02:29.326805 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:02:29.326805 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (compile): 00:02:29.308455 => 00:02:29.326805
[0m00:02:29.326805 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:02:29.343512 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:02:29.343512 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.036092519760131836s, acqrelcnt: 1, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843749.3074195
[0m00:02:29.343512 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.036092519760131836s, acqrelcnt: 1, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843749.3074195
[0m00:02:29.343512 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.036092519760131836s, acqrelcnt: 1, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843749.3074195
[0m00:02:29.343512 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.036092519760131836s, acqrelcnt: 1, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843749.3074195
[0m00:02:29.343512 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:02:29.343512 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:02:29.343512 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_name
from `hive_metastore`.`saleslt`.`dim_product`
where product_name is null



      
    ) dbt_internal_test
[0m00:02:29.343512 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:02:29.553730 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: session opened sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843749.55373
[0m00:02:29.807186 [debug] [Thread-1 (]: SQL status: OK in 0.46000000834465027 seconds
[0m00:02:29.807186 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (execute): 00:02:29.326805 => 00:02:29.807186
[0m00:02:29.807186 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: _release sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843749.8071864
[0m00:02:29.822697 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: _release sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843749.8226976
[0m00:02:29.822697 [info ] [Thread-1 (]: 1 of 8 PASS not_null_dim_product_product_name .................................. [[32mPASS[0m in 0.53s]
[0m00:02:29.822697 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:02:29.822697 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:02:29.822697 [info ] [Thread-1 (]: 2 of 8 START test not_null_dim_product_product_sk .............................. [RUN]
[0m00:02:29.822697 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: idle check connection: sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843749.8226976
[0m00:02:29.822697 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, now test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417)
[0m00:02:29.822697 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843749.8226976
[0m00:02:29.822697 [debug] [Thread-1 (]: Databricks adapter: On thread (2348, 22428): None using default compute resource.
[0m00:02:29.822697 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: _acquire sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843749.8226976
[0m00:02:29.822697 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:02:29.822697 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:02:29.822697 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (compile): 00:02:29.822697 => 00:02:29.822697
[0m00:02:29.822697 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:02:29.838306 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:02:29.839221 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: get_thread_connection: sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.01652383804321289s, acqrelcnt: 1, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843749.8226976
[0m00:02:29.840232 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: idle check connection: sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.017534494400024414s, acqrelcnt: 1, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843749.8226976
[0m00:02:29.840232 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:02:29.841285 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_sk
from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is null



      
    ) dbt_internal_test
[0m00:02:30.101844 [debug] [Thread-1 (]: SQL status: OK in 0.25999999046325684 seconds
[0m00:02:30.101844 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (execute): 00:02:29.822697 => 00:02:30.101844
[0m00:02:30.101844 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: _release sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843750.1018443
[0m00:02:30.101844 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: _release sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843750.1018443
[0m00:02:30.101844 [info ] [Thread-1 (]: 2 of 8 PASS not_null_dim_product_product_sk .................................... [[32mPASS[0m in 0.28s]
[0m00:02:30.117309 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:02:30.117309 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:02:30.117309 [info ] [Thread-1 (]: 3 of 8 START test not_null_dim_product_sellstartdate ........................... [RUN]
[0m00:02:30.117309 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: idle check connection: sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.015465021133422852s, acqrelcnt: 0, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843750.1018443
[0m00:02:30.117309 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, now test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546)
[0m00:02:30.117309 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.015465021133422852s, acqrelcnt: 0, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843750.1018443
[0m00:02:30.117309 [debug] [Thread-1 (]: Databricks adapter: On thread (2348, 22428): None using default compute resource.
[0m00:02:30.117309 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: _acquire sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.015465021133422852s, acqrelcnt: 1, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843750.1018443
[0m00:02:30.117309 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:02:30.117309 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:02:30.117309 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (compile): 00:02:30.117309 => 00:02:30.117309
[0m00:02:30.117309 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:02:30.132926 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:02:30.132926 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: get_thread_connection: sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0310821533203125s, acqrelcnt: 1, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843750.1018443
[0m00:02:30.132926 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: idle check connection: sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0310821533203125s, acqrelcnt: 1, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843750.1018443
[0m00:02:30.132926 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:02:30.132926 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select sellstartdate
from `hive_metastore`.`saleslt`.`dim_product`
where sellstartdate is null



      
    ) dbt_internal_test
[0m00:02:30.374444 [debug] [Thread-1 (]: SQL status: OK in 0.23999999463558197 seconds
[0m00:02:30.374444 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (execute): 00:02:30.117309 => 00:02:30.374444
[0m00:02:30.374444 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: _release sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843750.3744442
[0m00:02:30.374444 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: _release sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843750.3744442
[0m00:02:30.374444 [info ] [Thread-1 (]: 3 of 8 PASS not_null_dim_product_sellstartdate ................................. [[32mPASS[0m in 0.26s]
[0m00:02:30.374444 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:02:30.374444 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:02:30.374444 [info ] [Thread-1 (]: 4 of 8 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m00:02:30.374444 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: idle check connection: sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843750.3744442
[0m00:02:30.374444 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, now test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710)
[0m00:02:30.374444 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: reusing connection test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843750.3744442
[0m00:02:30.374444 [debug] [Thread-1 (]: Databricks adapter: On thread (2348, 22428): None using default compute resource.
[0m00:02:30.374444 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: _acquire sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843750.3744442
[0m00:02:30.374444 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:02:30.392215 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:02:30.394032 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 00:02:30.374444 => 00:02:30.394032
[0m00:02:30.394032 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:02:30.396442 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:02:30.396442 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: get_thread_connection: sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.02199840545654297s, acqrelcnt: 1, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843750.3744442
[0m00:02:30.396442 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: idle check connection: sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.02199840545654297s, acqrelcnt: 1, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843750.3744442
[0m00:02:30.396442 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:02:30.396442 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m00:02:30.699649 [debug] [Thread-1 (]: SQL status: OK in 0.30000001192092896 seconds
[0m00:02:30.699649 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 00:02:30.395040 => 00:02:30.699649
[0m00:02:30.699649 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: _release sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843750.6996493
[0m00:02:30.699649 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: _release sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843750.6996493
[0m00:02:30.699649 [error] [Thread-1 (]: 4 of 8 FAIL 1 not_null_my_first_dbt_model_id ................................... [[31mFAIL 1[0m in 0.33s]
[0m00:02:30.699649 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:02:30.699649 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:02:30.699649 [info ] [Thread-1 (]: 5 of 8 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m00:02:30.699649 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: idle check connection: sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843750.6996493
[0m00:02:30.715201 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, now test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778)
[0m00:02:30.715201 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: reusing connection test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.015552043914794922s, acqrelcnt: 0, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843750.6996493
[0m00:02:30.715201 [debug] [Thread-1 (]: Databricks adapter: On thread (2348, 22428): None using default compute resource.
[0m00:02:30.715201 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: _acquire sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.015552043914794922s, acqrelcnt: 1, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843750.6996493
[0m00:02:30.715201 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:02:30.715201 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:02:30.715201 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (compile): 00:02:30.715201 => 00:02:30.715201
[0m00:02:30.715201 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:02:30.715201 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:02:30.715201 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: get_thread_connection: sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.015552043914794922s, acqrelcnt: 1, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843750.6996493
[0m00:02:30.715201 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: idle check connection: sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.015552043914794922s, acqrelcnt: 1, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843750.6996493
[0m00:02:30.730757 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:02:30.730757 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m00:02:30.984546 [debug] [Thread-1 (]: SQL status: OK in 0.25 seconds
[0m00:02:31.000189 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (execute): 00:02:30.715201 => 00:02:30.984546
[0m00:02:31.000189 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: _release sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843751.000189
[0m00:02:31.000189 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: _release sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843751.000189
[0m00:02:31.000189 [info ] [Thread-1 (]: 5 of 8 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.30s]
[0m00:02:31.015814 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:02:31.015814 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:02:31.015814 [info ] [Thread-1 (]: 6 of 8 START test unique_dim_product_product_sk ................................ [RUN]
[0m00:02:31.015814 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: idle check connection: sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.015625476837158203s, acqrelcnt: 0, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843751.000189
[0m00:02:31.015814 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, now test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c)
[0m00:02:31.015814 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: reusing connection test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.015625476837158203s, acqrelcnt: 0, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843751.000189
[0m00:02:31.015814 [debug] [Thread-1 (]: Databricks adapter: On thread (2348, 22428): None using default compute resource.
[0m00:02:31.015814 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: _acquire sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.015625476837158203s, acqrelcnt: 1, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843751.000189
[0m00:02:31.015814 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:02:31.041686 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:02:31.043272 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (compile): 00:02:31.031436 => 00:02:31.043272
[0m00:02:31.043272 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:02:31.047180 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:02:31.047180 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: get_thread_connection: sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.046991586685180664s, acqrelcnt: 1, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843751.000189
[0m00:02:31.047180 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: idle check connection: sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.046991586685180664s, acqrelcnt: 1, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843751.000189
[0m00:02:31.047180 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:02:31.047180 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    product_sk as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is not null
group by product_sk
having count(*) > 1



      
    ) dbt_internal_test
[0m00:02:31.423912 [debug] [Thread-1 (]: SQL status: OK in 0.3799999952316284 seconds
[0m00:02:31.434413 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (execute): 00:02:31.043272 => 00:02:31.434413
[0m00:02:31.434413 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: _release sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843751.434414
[0m00:02:31.434413 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: _release sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843751.434414
[0m00:02:31.434413 [info ] [Thread-1 (]: 6 of 8 PASS unique_dim_product_product_sk ...................................... [[32mPASS[0m in 0.42s]
[0m00:02:31.434413 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:02:31.439922 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:02:31.439922 [info ] [Thread-1 (]: 7 of 8 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m00:02:31.439922 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: idle check connection: sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0055084228515625s, acqrelcnt: 0, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843751.434414
[0m00:02:31.439922 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, now test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321)
[0m00:02:31.439922 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: reusing connection test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0055084228515625s, acqrelcnt: 0, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843751.434414
[0m00:02:31.439922 [debug] [Thread-1 (]: Databricks adapter: On thread (2348, 22428): None using default compute resource.
[0m00:02:31.439922 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: _acquire sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0055084228515625s, acqrelcnt: 1, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843751.434414
[0m00:02:31.439922 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:02:31.439922 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:02:31.439922 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (compile): 00:02:31.439922 => 00:02:31.439922
[0m00:02:31.439922 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:02:31.439922 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:02:31.455555 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: get_thread_connection: sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.02114105224609375s, acqrelcnt: 1, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843751.434414
[0m00:02:31.456552 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: idle check connection: sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.02213907241821289s, acqrelcnt: 1, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843751.434414
[0m00:02:31.457378 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:02:31.457378 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m00:02:31.829078 [debug] [Thread-1 (]: SQL status: OK in 0.3700000047683716 seconds
[0m00:02:31.829078 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (execute): 00:02:31.439922 => 00:02:31.829078
[0m00:02:31.844705 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: _release sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843751.8447056
[0m00:02:31.844705 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: _release sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843751.8447056
[0m00:02:31.844705 [info ] [Thread-1 (]: 7 of 8 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.40s]
[0m00:02:31.844705 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:02:31.844705 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:02:31.844705 [info ] [Thread-1 (]: 8 of 8 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m00:02:31.844705 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: idle check connection: sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843751.8447056
[0m00:02:31.844705 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, now test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493)
[0m00:02:31.844705 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: reusing connection test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843751.8447056
[0m00:02:31.844705 [debug] [Thread-1 (]: Databricks adapter: On thread (2348, 22428): None using default compute resource.
[0m00:02:31.844705 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: _acquire sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843751.8447056
[0m00:02:31.844705 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:02:31.844705 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:02:31.844705 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (compile): 00:02:31.844705 => 00:02:31.844705
[0m00:02:31.844705 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:02:31.860330 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:02:31.861133 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: get_thread_connection: sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.01642775535583496s, acqrelcnt: 1, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843751.8447056
[0m00:02:31.862008 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: idle check connection: sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.017302989959716797s, acqrelcnt: 1, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843751.8447056
[0m00:02:31.863005 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:02:31.863005 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m00:02:32.221446 [debug] [Thread-1 (]: SQL status: OK in 0.36000001430511475 seconds
[0m00:02:32.221446 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (execute): 00:02:31.844705 => 00:02:32.221446
[0m00:02:32.221446 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: _release sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843752.221446
[0m00:02:32.221446 [debug] [Thread-1 (]: Databricks adapter: conn: 2050396209680: _release sess: 5a0c26d6-3e12-471a-93a5-2343bedb468f, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (2348, 22428), cmpt: ``, lut: 1707843752.221446
[0m00:02:32.221446 [info ] [Thread-1 (]: 8 of 8 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.38s]
[0m00:02:32.221446 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:02:32.232953 [debug] [MainThread]: Databricks adapter: conn: 2050396936400: idle check connection: sess: None, name: master, idle: 2.9434821605682373s, acqrelcnt: 0, lang: None, thrd: (2348, 500), cmpt: ``, lut: 1707843749.289471
[0m00:02:32.232953 [debug] [MainThread]: Databricks adapter: conn: 2050396936400: reusing connection master sess: None, name: master, idle: 2.9434821605682373s, acqrelcnt: 0, lang: None, thrd: (2348, 500), cmpt: ``, lut: 1707843749.289471
[0m00:02:32.232953 [debug] [MainThread]: Databricks adapter: Thread (2348, 500) using default compute resource.
[0m00:02:32.232953 [debug] [MainThread]: Databricks adapter: conn: 2050396936400: _acquire sess: None, name: master, idle: 2.9434821605682373s, acqrelcnt: 1, lang: None, thrd: (2348, 500), cmpt: ``, lut: 1707843749.289471
[0m00:02:32.232953 [debug] [MainThread]: Databricks adapter: conn: 2050396936400: get_thread_connection: sess: None, name: master, idle: 2.9434821605682373s, acqrelcnt: 1, lang: None, thrd: (2348, 500), cmpt: ``, lut: 1707843749.289471
[0m00:02:32.232953 [debug] [MainThread]: Databricks adapter: conn: 2050396936400: idle check connection: sess: None, name: master, idle: 2.9434821605682373s, acqrelcnt: 1, lang: None, thrd: (2348, 500), cmpt: ``, lut: 1707843749.289471
[0m00:02:32.237474 [debug] [MainThread]: On master: ROLLBACK
[0m00:02:32.237474 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:02:32.459990 [debug] [MainThread]: Databricks adapter: conn: 2050396936400: session opened sess: 9edf4ac9-b032-43d5-9de2-d73c352c7c22, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (2348, 500), cmpt: ``, lut: 1707843752.4442751
[0m00:02:32.460564 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:02:32.461573 [debug] [MainThread]: Databricks adapter: conn: 2050396936400: get_thread_connection: sess: 9edf4ac9-b032-43d5-9de2-d73c352c7c22, name: master, idle: 0.016289234161376953s, acqrelcnt: 1, lang: None, thrd: (2348, 500), cmpt: ``, lut: 1707843752.4442751
[0m00:02:32.462049 [debug] [MainThread]: Databricks adapter: conn: 2050396936400: idle check connection: sess: 9edf4ac9-b032-43d5-9de2-d73c352c7c22, name: master, idle: 0.017774343490600586s, acqrelcnt: 1, lang: None, thrd: (2348, 500), cmpt: ``, lut: 1707843752.4442751
[0m00:02:32.462049 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:02:32.462741 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:02:32.462741 [debug] [MainThread]: Databricks adapter: conn: 2050396936400: _release sess: 9edf4ac9-b032-43d5-9de2-d73c352c7c22, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (2348, 500), cmpt: ``, lut: 1707843752.4627416
[0m00:02:32.464481 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:02:32.464481 [debug] [MainThread]: On master: ROLLBACK
[0m00:02:32.464481 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:02:32.464481 [debug] [MainThread]: On master: Close
[0m00:02:32.529619 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m00:02:32.529619 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m00:02:32.529619 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:02:32.529619 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m00:02:32.607736 [debug] [MainThread]: Connection 'test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493' was properly closed.
[0m00:02:32.607736 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: ROLLBACK
[0m00:02:32.607736 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:02:32.618084 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m00:02:32.686110 [info ] [MainThread]: 
[0m00:02:32.686110 [info ] [MainThread]: Finished running 8 tests in 0 hours 0 minutes and 5.69 seconds (5.69s).
[0m00:02:32.686110 [debug] [MainThread]: Command end result
[0m00:02:32.707717 [info ] [MainThread]: 
[0m00:02:32.707717 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:02:32.713013 [info ] [MainThread]: 
[0m00:02:32.714075 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models\example\schema.yml)[0m
[0m00:02:32.717146 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m00:02:32.718060 [info ] [MainThread]: 
[0m00:02:32.719067 [info ] [MainThread]:   compiled Code at target\compiled\medallion_dbt_spark\models\example\schema.yml\not_null_my_first_dbt_model_id.sql
[0m00:02:32.719067 [info ] [MainThread]: 
[0m00:02:32.719067 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=1 SKIP=0 TOTAL=8
[0m00:02:32.719067 [debug] [MainThread]: Command `dbt test` failed at 00:02:32.719067 after 7.39 seconds
[0m00:02:32.719067 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD528F3A90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD52C52910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD4FCA3950>]}
[0m00:02:32.719067 [debug] [MainThread]: Flushing usage events
[0m00:04:23.702200 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023666415D10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002366645FF50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023666415010>]}


============================== 00:04:23.702200 | fadf141e-f91c-45b4-bcb4-f83bff25f114 ==============================
[0m00:04:23.702200 [info ] [MainThread]: Running with dbt=1.7.7
[0m00:04:23.717217 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt test', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m00:04:25.004530 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fadf141e-f91c-45b4-bcb4-f83bff25f114', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023678623A50>]}
[0m00:04:25.067035 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fadf141e-f91c-45b4-bcb4-f83bff25f114', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002367853ED90>]}
[0m00:04:25.067035 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m00:04:25.082747 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m00:04:25.188277 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:04:25.188277 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://models\example\my_first_dbt_model.sql
[0m00:04:25.412427 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fadf141e-f91c-45b4-bcb4-f83bff25f114', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023678B45150>]}
[0m00:04:25.428055 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fadf141e-f91c-45b4-bcb4-f83bff25f114', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023678A63050>]}
[0m00:04:25.428055 [info ] [MainThread]: Found 5 models, 7 snapshots, 8 tests, 9 sources, 0 exposures, 0 metrics, 535 macros, 0 groups, 0 semantic models
[0m00:04:25.428055 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fadf141e-f91c-45b4-bcb4-f83bff25f114', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000236784B61D0>]}
[0m00:04:25.428055 [info ] [MainThread]: 
[0m00:04:25.428055 [debug] [MainThread]: Databricks adapter: conn: 2432974596688: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (26608, 11836), cmpt: ``, lut: None
[0m00:04:25.428055 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:04:25.428055 [debug] [MainThread]: Databricks adapter: Thread (26608, 11836) using default compute resource.
[0m00:04:25.428055 [debug] [MainThread]: Databricks adapter: conn: 2432974596688: _acquire sess: None, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (26608, 11836), cmpt: ``, lut: 1707843865.4280553
[0m00:04:25.428055 [debug] [ThreadPool]: Databricks adapter: conn: 2432973077712: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_saleslt, idle: 0s, acqrelcnt: 0, lang: None, thrd: (26608, 12732), cmpt: ``, lut: None
[0m00:04:25.443590 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m00:04:25.444389 [debug] [ThreadPool]: Databricks adapter: Thread (26608, 12732) using default compute resource.
[0m00:04:25.444495 [debug] [ThreadPool]: Databricks adapter: conn: 2432973077712: _acquire sess: None, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (26608, 12732), cmpt: ``, lut: 1707843865.4444952
[0m00:04:25.450432 [debug] [ThreadPool]: Databricks adapter: conn: 2432973077712: get_thread_connection: sess: None, name: list_hive_metastore_saleslt, idle: 0.005937099456787109s, acqrelcnt: 1, lang: None, thrd: (26608, 12732), cmpt: ``, lut: 1707843865.4444952
[0m00:04:25.451548 [debug] [ThreadPool]: Databricks adapter: conn: 2432973077712: idle check connection: sess: None, name: list_hive_metastore_saleslt, idle: 0.007053375244140625s, acqrelcnt: 1, lang: None, thrd: (26608, 12732), cmpt: ``, lut: 1707843865.4444952
[0m00:04:25.452557 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:04:25.452557 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m00:04:25.453463 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:04:25.883880 [debug] [ThreadPool]: Databricks adapter: conn: 2432973077712: session opened sess: 5dfb8c95-1166-40e6-bcae-287975b54c3e, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (26608, 12732), cmpt: ``, lut: 1707843865.8838801
[0m00:04:26.281514 [debug] [ThreadPool]: SQL status: OK in 0.8299999833106995 seconds
[0m00:04:26.354085 [debug] [ThreadPool]: Databricks adapter: conn: 2432973077712: get_thread_connection: sess: 5dfb8c95-1166-40e6-bcae-287975b54c3e, name: list_hive_metastore_saleslt, idle: 0.47020506858825684s, acqrelcnt: 1, lang: None, thrd: (26608, 12732), cmpt: ``, lut: 1707843865.8838801
[0m00:04:26.369730 [debug] [ThreadPool]: Databricks adapter: conn: 2432973077712: idle check connection: sess: 5dfb8c95-1166-40e6-bcae-287975b54c3e, name: list_hive_metastore_saleslt, idle: 0.48585057258605957s, acqrelcnt: 1, lang: None, thrd: (26608, 12732), cmpt: ``, lut: 1707843865.8838801
[0m00:04:26.370418 [debug] [ThreadPool]: Databricks adapter: conn: 2432973077712: get_thread_connection: sess: 5dfb8c95-1166-40e6-bcae-287975b54c3e, name: list_hive_metastore_saleslt, idle: 0.4865386486053467s, acqrelcnt: 1, lang: None, thrd: (26608, 12732), cmpt: ``, lut: 1707843865.8838801
[0m00:04:26.370418 [debug] [ThreadPool]: Databricks adapter: conn: 2432973077712: idle check connection: sess: 5dfb8c95-1166-40e6-bcae-287975b54c3e, name: list_hive_metastore_saleslt, idle: 0.4865386486053467s, acqrelcnt: 1, lang: None, thrd: (26608, 12732), cmpt: ``, lut: 1707843865.8838801
[0m00:04:26.370418 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:04:26.370418 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:04:26.370418 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m00:04:26.570736 [debug] [ThreadPool]: SQL status: OK in 0.20000000298023224 seconds
[0m00:04:26.578083 [debug] [ThreadPool]: Databricks adapter: conn: 2432973077712: get_thread_connection: sess: 5dfb8c95-1166-40e6-bcae-287975b54c3e, name: list_hive_metastore_saleslt, idle: 0.6942031383514404s, acqrelcnt: 1, lang: None, thrd: (26608, 12732), cmpt: ``, lut: 1707843865.8838801
[0m00:04:26.579059 [debug] [ThreadPool]: Databricks adapter: conn: 2432973077712: idle check connection: sess: 5dfb8c95-1166-40e6-bcae-287975b54c3e, name: list_hive_metastore_saleslt, idle: 0.695178747177124s, acqrelcnt: 1, lang: None, thrd: (26608, 12732), cmpt: ``, lut: 1707843865.8838801
[0m00:04:26.579059 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:04:26.579996 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m00:04:26.746200 [debug] [ThreadPool]: SQL status: OK in 0.17000000178813934 seconds
[0m00:04:26.746200 [debug] [ThreadPool]: Databricks adapter: conn: 2432973077712: _release sess: 5dfb8c95-1166-40e6-bcae-287975b54c3e, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (26608, 12732), cmpt: ``, lut: 1707843866.7462006
[0m00:04:26.746200 [debug] [ThreadPool]: Databricks adapter: conn: 2432973077712: idle check connection: sess: 5dfb8c95-1166-40e6-bcae-287975b54c3e, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (26608, 12732), cmpt: ``, lut: 1707843866.7462006
[0m00:04:26.761741 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m00:04:26.761741 [debug] [ThreadPool]: Databricks adapter: conn: 2432973077712: reusing connection list_hive_metastore_saleslt sess: 5dfb8c95-1166-40e6-bcae-287975b54c3e, name: list_hive_metastore_snapshots, idle: 0.01554107666015625s, acqrelcnt: 0, lang: None, thrd: (26608, 12732), cmpt: ``, lut: 1707843866.7462006
[0m00:04:26.761741 [debug] [ThreadPool]: Databricks adapter: Thread (26608, 12732) using default compute resource.
[0m00:04:26.761741 [debug] [ThreadPool]: Databricks adapter: conn: 2432973077712: _acquire sess: 5dfb8c95-1166-40e6-bcae-287975b54c3e, name: list_hive_metastore_snapshots, idle: 0.01554107666015625s, acqrelcnt: 1, lang: None, thrd: (26608, 12732), cmpt: ``, lut: 1707843866.7462006
[0m00:04:26.761741 [debug] [ThreadPool]: Databricks adapter: conn: 2432973077712: get_thread_connection: sess: 5dfb8c95-1166-40e6-bcae-287975b54c3e, name: list_hive_metastore_snapshots, idle: 0.01554107666015625s, acqrelcnt: 1, lang: None, thrd: (26608, 12732), cmpt: ``, lut: 1707843866.7462006
[0m00:04:26.761741 [debug] [ThreadPool]: Databricks adapter: conn: 2432973077712: idle check connection: sess: 5dfb8c95-1166-40e6-bcae-287975b54c3e, name: list_hive_metastore_snapshots, idle: 0.01554107666015625s, acqrelcnt: 1, lang: None, thrd: (26608, 12732), cmpt: ``, lut: 1707843866.7462006
[0m00:04:26.761741 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:04:26.761741 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m00:04:26.856705 [debug] [ThreadPool]: SQL status: OK in 0.09000000357627869 seconds
[0m00:04:26.873439 [debug] [ThreadPool]: Databricks adapter: conn: 2432973077712: get_thread_connection: sess: 5dfb8c95-1166-40e6-bcae-287975b54c3e, name: list_hive_metastore_snapshots, idle: 0.12723898887634277s, acqrelcnt: 1, lang: None, thrd: (26608, 12732), cmpt: ``, lut: 1707843866.7462006
[0m00:04:26.874441 [debug] [ThreadPool]: Databricks adapter: conn: 2432973077712: idle check connection: sess: 5dfb8c95-1166-40e6-bcae-287975b54c3e, name: list_hive_metastore_snapshots, idle: 0.12824153900146484s, acqrelcnt: 1, lang: None, thrd: (26608, 12732), cmpt: ``, lut: 1707843866.7462006
[0m00:04:26.874441 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:04:26.875442 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m00:04:26.998678 [debug] [ThreadPool]: SQL status: OK in 0.10999999940395355 seconds
[0m00:04:26.998678 [debug] [ThreadPool]: Databricks adapter: conn: 2432973077712: get_thread_connection: sess: 5dfb8c95-1166-40e6-bcae-287975b54c3e, name: list_hive_metastore_snapshots, idle: 0.25247764587402344s, acqrelcnt: 1, lang: None, thrd: (26608, 12732), cmpt: ``, lut: 1707843866.7462006
[0m00:04:26.998678 [debug] [ThreadPool]: Databricks adapter: conn: 2432973077712: idle check connection: sess: 5dfb8c95-1166-40e6-bcae-287975b54c3e, name: list_hive_metastore_snapshots, idle: 0.25247764587402344s, acqrelcnt: 1, lang: None, thrd: (26608, 12732), cmpt: ``, lut: 1707843866.7462006
[0m00:04:26.998678 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:04:26.998678 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m00:04:27.154936 [debug] [ThreadPool]: SQL status: OK in 0.1599999964237213 seconds
[0m00:04:27.154936 [debug] [ThreadPool]: Databricks adapter: conn: 2432973077712: _release sess: 5dfb8c95-1166-40e6-bcae-287975b54c3e, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (26608, 12732), cmpt: ``, lut: 1707843867.1549366
[0m00:04:27.154936 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fadf141e-f91c-45b4-bcb4-f83bff25f114', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023678A0AD10>]}
[0m00:04:27.170586 [debug] [MainThread]: Databricks adapter: conn: 2432974596688: get_thread_connection: sess: None, name: master, idle: 1.7425312995910645s, acqrelcnt: 1, lang: None, thrd: (26608, 11836), cmpt: ``, lut: 1707843865.4280553
[0m00:04:27.170586 [debug] [MainThread]: Databricks adapter: conn: 2432974596688: idle check connection: sess: None, name: master, idle: 1.7425312995910645s, acqrelcnt: 1, lang: None, thrd: (26608, 11836), cmpt: ``, lut: 1707843865.4280553
[0m00:04:27.170586 [debug] [MainThread]: Databricks adapter: conn: 2432974596688: get_thread_connection: sess: None, name: master, idle: 1.7425312995910645s, acqrelcnt: 1, lang: None, thrd: (26608, 11836), cmpt: ``, lut: 1707843865.4280553
[0m00:04:27.170586 [debug] [MainThread]: Databricks adapter: conn: 2432974596688: idle check connection: sess: None, name: master, idle: 1.7425312995910645s, acqrelcnt: 1, lang: None, thrd: (26608, 11836), cmpt: ``, lut: 1707843865.4280553
[0m00:04:27.170586 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:04:27.170586 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:04:27.170586 [debug] [MainThread]: Databricks adapter: conn: 2432974596688: _release sess: None, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (26608, 11836), cmpt: ``, lut: 1707843867.1705866
[0m00:04:27.170586 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:04:27.170586 [info ] [MainThread]: 
[0m00:04:27.170586 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:04:27.170586 [info ] [Thread-1 (]: 1 of 8 START test not_null_dim_product_product_name ............................ [RUN]
[0m00:04:27.185548 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: Creating DatabricksDBTConnection sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0s, acqrelcnt: 0, lang: None, thrd: (26608, 11568), cmpt: ``, lut: None
[0m00:04:27.185548 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5'
[0m00:04:27.186627 [debug] [Thread-1 (]: Databricks adapter: On thread (26608, 11568): None using default compute resource.
[0m00:04:27.186627 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: _acquire sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843867.1866276
[0m00:04:27.187634 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:04:27.206918 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:04:27.206918 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (compile): 00:04:27.187634 => 00:04:27.206918
[0m00:04:27.206918 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:04:27.223432 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:04:27.223432 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.036805152893066406s, acqrelcnt: 1, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843867.1866276
[0m00:04:27.223432 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.036805152893066406s, acqrelcnt: 1, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843867.1866276
[0m00:04:27.223432 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.036805152893066406s, acqrelcnt: 1, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843867.1866276
[0m00:04:27.223432 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.036805152893066406s, acqrelcnt: 1, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843867.1866276
[0m00:04:27.223432 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:04:27.223432 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:04:27.223432 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_name
from `hive_metastore`.`saleslt`.`dim_product`
where product_name is null



      
    ) dbt_internal_test
[0m00:04:27.223432 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:04:27.485584 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: session opened sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843867.4855845
[0m00:04:27.798071 [debug] [Thread-1 (]: SQL status: OK in 0.5699999928474426 seconds
[0m00:04:27.813686 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (execute): 00:04:27.206918 => 00:04:27.813686
[0m00:04:27.813686 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: _release sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843867.8136864
[0m00:04:27.813686 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: _release sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843867.8136864
[0m00:04:27.813686 [info ] [Thread-1 (]: 1 of 8 PASS not_null_dim_product_product_name .................................. [[32mPASS[0m in 0.64s]
[0m00:04:27.813686 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:04:27.813686 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:04:27.813686 [info ] [Thread-1 (]: 2 of 8 START test not_null_dim_product_product_sk .............................. [RUN]
[0m00:04:27.813686 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: idle check connection: sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843867.8136864
[0m00:04:27.813686 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, now test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417)
[0m00:04:27.813686 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843867.8136864
[0m00:04:27.813686 [debug] [Thread-1 (]: Databricks adapter: On thread (26608, 11568): None using default compute resource.
[0m00:04:27.813686 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: _acquire sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843867.8136864
[0m00:04:27.813686 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:04:27.832418 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:04:27.834385 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (compile): 00:04:27.813686 => 00:04:27.833476
[0m00:04:27.834970 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:04:27.835370 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:04:27.840112 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: get_thread_connection: sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.026426076889038086s, acqrelcnt: 1, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843867.8136864
[0m00:04:27.841121 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: idle check connection: sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.026426076889038086s, acqrelcnt: 1, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843867.8136864
[0m00:04:27.841121 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:04:27.842136 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_sk
from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is null



      
    ) dbt_internal_test
[0m00:04:28.128216 [debug] [Thread-1 (]: SQL status: OK in 0.28999999165534973 seconds
[0m00:04:28.128216 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (execute): 00:04:27.835290 => 00:04:28.128216
[0m00:04:28.128216 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: _release sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843868.1282165
[0m00:04:28.128216 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: _release sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843868.1282165
[0m00:04:28.128216 [info ] [Thread-1 (]: 2 of 8 PASS not_null_dim_product_product_sk .................................... [[32mPASS[0m in 0.31s]
[0m00:04:28.128216 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:04:28.128216 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:04:28.128216 [info ] [Thread-1 (]: 3 of 8 START test not_null_dim_product_sellstartdate ........................... [RUN]
[0m00:04:28.128216 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: idle check connection: sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843868.1282165
[0m00:04:28.128216 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, now test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546)
[0m00:04:28.143735 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.015518903732299805s, acqrelcnt: 0, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843868.1282165
[0m00:04:28.143735 [debug] [Thread-1 (]: Databricks adapter: On thread (26608, 11568): None using default compute resource.
[0m00:04:28.143735 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: _acquire sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.015518903732299805s, acqrelcnt: 1, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843868.1282165
[0m00:04:28.143735 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:04:28.143735 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:04:28.143735 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (compile): 00:04:28.143735 => 00:04:28.143735
[0m00:04:28.143735 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:04:28.143735 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:04:28.143735 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: get_thread_connection: sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.015518903732299805s, acqrelcnt: 1, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843868.1282165
[0m00:04:28.143735 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: idle check connection: sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.015518903732299805s, acqrelcnt: 1, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843868.1282165
[0m00:04:28.143735 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:04:28.143735 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select sellstartdate
from `hive_metastore`.`saleslt`.`dim_product`
where sellstartdate is null



      
    ) dbt_internal_test
[0m00:04:28.435743 [debug] [Thread-1 (]: SQL status: OK in 0.28999999165534973 seconds
[0m00:04:28.442597 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (execute): 00:04:28.143735 => 00:04:28.442597
[0m00:04:28.443597 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: _release sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843868.4425972
[0m00:04:28.443597 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: _release sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843868.443597
[0m00:04:28.444596 [info ] [Thread-1 (]: 3 of 8 PASS not_null_dim_product_sellstartdate ................................. [[32mPASS[0m in 0.32s]
[0m00:04:28.445410 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:04:28.446498 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:04:28.446498 [info ] [Thread-1 (]: 4 of 8 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m00:04:28.448417 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: idle check connection: sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.004820346832275391s, acqrelcnt: 0, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843868.443597
[0m00:04:28.448417 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, now test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710)
[0m00:04:28.449416 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: reusing connection test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0058193206787109375s, acqrelcnt: 0, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843868.443597
[0m00:04:28.449416 [debug] [Thread-1 (]: Databricks adapter: On thread (26608, 11568): None using default compute resource.
[0m00:04:28.450416 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: _acquire sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0058193206787109375s, acqrelcnt: 1, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843868.443597
[0m00:04:28.450416 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:04:28.455323 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:04:28.456544 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 00:04:28.450416 => 00:04:28.456544
[0m00:04:28.457201 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:04:28.460221 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:04:28.460221 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: get_thread_connection: sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.01662421226501465s, acqrelcnt: 1, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843868.443597
[0m00:04:28.460221 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: idle check connection: sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.01662421226501465s, acqrelcnt: 1, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843868.443597
[0m00:04:28.460221 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:04:28.460221 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m00:04:28.778242 [debug] [Thread-1 (]: SQL status: OK in 0.3199999928474426 seconds
[0m00:04:28.778242 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 00:04:28.457201 => 00:04:28.778242
[0m00:04:28.778242 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: _release sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843868.778242
[0m00:04:28.778242 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: _release sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843868.778242
[0m00:04:28.778242 [error] [Thread-1 (]: 4 of 8 FAIL 1 not_null_my_first_dbt_model_id ................................... [[31mFAIL 1[0m in 0.33s]
[0m00:04:28.778242 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:04:28.778242 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:04:28.778242 [info ] [Thread-1 (]: 5 of 8 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m00:04:28.778242 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: idle check connection: sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843868.778242
[0m00:04:28.778242 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, now test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778)
[0m00:04:28.778242 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: reusing connection test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843868.778242
[0m00:04:28.793753 [debug] [Thread-1 (]: Databricks adapter: On thread (26608, 11568): None using default compute resource.
[0m00:04:28.793753 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: _acquire sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.015511751174926758s, acqrelcnt: 1, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843868.778242
[0m00:04:28.793753 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:04:28.793753 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:04:28.793753 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (compile): 00:04:28.793753 => 00:04:28.793753
[0m00:04:28.793753 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:04:28.809346 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:04:28.810535 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: get_thread_connection: sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.03229355812072754s, acqrelcnt: 1, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843868.778242
[0m00:04:28.811669 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: idle check connection: sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.03342747688293457s, acqrelcnt: 1, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843868.778242
[0m00:04:28.812694 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:04:28.813696 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m00:04:29.101431 [debug] [Thread-1 (]: SQL status: OK in 0.28999999165534973 seconds
[0m00:04:29.116997 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (execute): 00:04:28.793753 => 00:04:29.116997
[0m00:04:29.116997 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: _release sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843869.1169975
[0m00:04:29.116997 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: _release sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843869.1169975
[0m00:04:29.116997 [info ] [Thread-1 (]: 5 of 8 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.34s]
[0m00:04:29.116997 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:04:29.116997 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:04:29.116997 [info ] [Thread-1 (]: 6 of 8 START test unique_dim_product_product_sk ................................ [RUN]
[0m00:04:29.116997 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: idle check connection: sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843869.1169975
[0m00:04:29.116997 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, now test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c)
[0m00:04:29.116997 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: reusing connection test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843869.1169975
[0m00:04:29.116997 [debug] [Thread-1 (]: Databricks adapter: On thread (26608, 11568): None using default compute resource.
[0m00:04:29.132522 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: _acquire sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.015525579452514648s, acqrelcnt: 1, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843869.1169975
[0m00:04:29.133275 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:04:29.142500 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:04:29.144160 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (compile): 00:04:29.133275 => 00:04:29.144160
[0m00:04:29.145143 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:04:29.149243 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:04:29.150334 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: get_thread_connection: sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.03333640098571777s, acqrelcnt: 1, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843869.1169975
[0m00:04:29.150334 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: idle check connection: sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.03333640098571777s, acqrelcnt: 1, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843869.1169975
[0m00:04:29.150334 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:04:29.150334 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    product_sk as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is not null
group by product_sk
having count(*) > 1



      
    ) dbt_internal_test
[0m00:04:29.556724 [debug] [Thread-1 (]: SQL status: OK in 0.4099999964237213 seconds
[0m00:04:29.556724 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (execute): 00:04:29.146245 => 00:04:29.556724
[0m00:04:29.556724 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: _release sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843869.5567243
[0m00:04:29.556724 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: _release sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843869.5567243
[0m00:04:29.556724 [info ] [Thread-1 (]: 6 of 8 PASS unique_dim_product_product_sk ...................................... [[32mPASS[0m in 0.44s]
[0m00:04:29.556724 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:04:29.556724 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:04:29.556724 [info ] [Thread-1 (]: 7 of 8 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m00:04:29.556724 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: idle check connection: sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843869.5567243
[0m00:04:29.556724 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, now test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321)
[0m00:04:29.556724 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: reusing connection test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843869.5567243
[0m00:04:29.556724 [debug] [Thread-1 (]: Databricks adapter: On thread (26608, 11568): None using default compute resource.
[0m00:04:29.556724 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: _acquire sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843869.5567243
[0m00:04:29.556724 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:04:29.574252 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:04:29.575211 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (compile): 00:04:29.556724 => 00:04:29.575211
[0m00:04:29.575211 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:04:29.578101 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:04:29.578101 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: get_thread_connection: sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0213775634765625s, acqrelcnt: 1, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843869.5567243
[0m00:04:29.578101 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: idle check connection: sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0213775634765625s, acqrelcnt: 1, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843869.5567243
[0m00:04:29.578101 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:04:29.578101 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m00:04:29.946715 [debug] [Thread-1 (]: SQL status: OK in 0.3700000047683716 seconds
[0m00:04:29.946715 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (execute): 00:04:29.576421 => 00:04:29.946715
[0m00:04:29.946715 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: _release sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843869.9467149
[0m00:04:29.946715 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: _release sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843869.9467149
[0m00:04:29.946715 [info ] [Thread-1 (]: 7 of 8 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.39s]
[0m00:04:29.946715 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:04:29.946715 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:04:29.962336 [info ] [Thread-1 (]: 8 of 8 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m00:04:29.962336 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: idle check connection: sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.01562190055847168s, acqrelcnt: 0, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843869.9467149
[0m00:04:29.962336 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, now test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493)
[0m00:04:29.962336 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: reusing connection test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.01562190055847168s, acqrelcnt: 0, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843869.9467149
[0m00:04:29.962336 [debug] [Thread-1 (]: Databricks adapter: On thread (26608, 11568): None using default compute resource.
[0m00:04:29.962336 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: _acquire sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.01562190055847168s, acqrelcnt: 1, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843869.9467149
[0m00:04:29.962336 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:04:29.962336 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:04:29.962336 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (compile): 00:04:29.962336 => 00:04:29.962336
[0m00:04:29.962336 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:04:29.962336 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:04:29.962336 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: get_thread_connection: sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.01562190055847168s, acqrelcnt: 1, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843869.9467149
[0m00:04:29.962336 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: idle check connection: sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.01562190055847168s, acqrelcnt: 1, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843869.9467149
[0m00:04:29.962336 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:04:29.962336 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m00:04:30.408084 [debug] [Thread-1 (]: SQL status: OK in 0.44999998807907104 seconds
[0m00:04:30.408084 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (execute): 00:04:29.962336 => 00:04:30.408084
[0m00:04:30.408084 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: _release sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843870.408084
[0m00:04:30.408084 [debug] [Thread-1 (]: Databricks adapter: conn: 2432972632656: _release sess: 2374c80e-d670-4bb6-acbc-f12550dcf2b2, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26608, 11568), cmpt: ``, lut: 1707843870.408084
[0m00:04:30.408084 [info ] [Thread-1 (]: 8 of 8 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.45s]
[0m00:04:30.408084 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:04:30.408084 [debug] [MainThread]: Databricks adapter: conn: 2432974596688: idle check connection: sess: None, name: master, idle: 3.237497329711914s, acqrelcnt: 0, lang: None, thrd: (26608, 11836), cmpt: ``, lut: 1707843867.1705866
[0m00:04:30.408084 [debug] [MainThread]: Databricks adapter: conn: 2432974596688: reusing connection master sess: None, name: master, idle: 3.237497329711914s, acqrelcnt: 0, lang: None, thrd: (26608, 11836), cmpt: ``, lut: 1707843867.1705866
[0m00:04:30.423571 [debug] [MainThread]: Databricks adapter: Thread (26608, 11836) using default compute resource.
[0m00:04:30.423571 [debug] [MainThread]: Databricks adapter: conn: 2432974596688: _acquire sess: None, name: master, idle: 3.2529845237731934s, acqrelcnt: 1, lang: None, thrd: (26608, 11836), cmpt: ``, lut: 1707843867.1705866
[0m00:04:30.423571 [debug] [MainThread]: Databricks adapter: conn: 2432974596688: get_thread_connection: sess: None, name: master, idle: 3.2529845237731934s, acqrelcnt: 1, lang: None, thrd: (26608, 11836), cmpt: ``, lut: 1707843867.1705866
[0m00:04:30.423571 [debug] [MainThread]: Databricks adapter: conn: 2432974596688: idle check connection: sess: None, name: master, idle: 3.2529845237731934s, acqrelcnt: 1, lang: None, thrd: (26608, 11836), cmpt: ``, lut: 1707843867.1705866
[0m00:04:30.423571 [debug] [MainThread]: On master: ROLLBACK
[0m00:04:30.423571 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:04:30.672236 [debug] [MainThread]: Databricks adapter: conn: 2432974596688: session opened sess: 47583797-e20a-49a5-ae65-5a98fea91304, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (26608, 11836), cmpt: ``, lut: 1707843870.672236
[0m00:04:30.672236 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:04:30.672236 [debug] [MainThread]: Databricks adapter: conn: 2432974596688: get_thread_connection: sess: 47583797-e20a-49a5-ae65-5a98fea91304, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (26608, 11836), cmpt: ``, lut: 1707843870.672236
[0m00:04:30.672236 [debug] [MainThread]: Databricks adapter: conn: 2432974596688: idle check connection: sess: 47583797-e20a-49a5-ae65-5a98fea91304, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (26608, 11836), cmpt: ``, lut: 1707843870.672236
[0m00:04:30.672236 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:04:30.672236 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:04:30.672236 [debug] [MainThread]: Databricks adapter: conn: 2432974596688: _release sess: 47583797-e20a-49a5-ae65-5a98fea91304, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (26608, 11836), cmpt: ``, lut: 1707843870.672236
[0m00:04:30.672236 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:04:30.672236 [debug] [MainThread]: On master: ROLLBACK
[0m00:04:30.672236 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:04:30.687683 [debug] [MainThread]: On master: Close
[0m00:04:30.812843 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m00:04:30.812843 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m00:04:30.812843 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:04:30.812843 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m00:04:30.890946 [debug] [MainThread]: Connection 'test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493' was properly closed.
[0m00:04:30.890946 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: ROLLBACK
[0m00:04:30.890946 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:04:30.890946 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m00:04:30.984737 [info ] [MainThread]: 
[0m00:04:30.984737 [info ] [MainThread]: Finished running 8 tests in 0 hours 0 minutes and 5.56 seconds (5.56s).
[0m00:04:30.984737 [debug] [MainThread]: Command end result
[0m00:04:31.012794 [info ] [MainThread]: 
[0m00:04:31.013716 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:04:31.015027 [info ] [MainThread]: 
[0m00:04:31.015755 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models\example\schema.yml)[0m
[0m00:04:31.016800 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m00:04:31.018430 [info ] [MainThread]: 
[0m00:04:31.018430 [info ] [MainThread]:   compiled Code at target\compiled\medallion_dbt_spark\models\example\schema.yml\not_null_my_first_dbt_model_id.sql
[0m00:04:31.018430 [info ] [MainThread]: 
[0m00:04:31.018430 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=1 SKIP=0 TOTAL=8
[0m00:04:31.018430 [debug] [MainThread]: Command `dbt test` failed at 00:04:31.018430 after 7.35 seconds
[0m00:04:31.018430 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000236664148D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002366641C510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002366645FD90>]}
[0m00:04:31.018430 [debug] [MainThread]: Flushing usage events
[0m00:07:33.461436 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017365364E90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017365483590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000173652D9F90>]}


============================== 00:07:33.461436 | ed75d7e7-38d2-4260-88a8-2bba7c5ca70a ==============================
[0m00:07:33.461436 [info ] [MainThread]: Running with dbt=1.7.7
[0m00:07:33.477072 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt test', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m00:07:34.651783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ed75d7e7-38d2-4260-88a8-2bba7c5ca70a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000173778F2890>]}
[0m00:07:34.739019 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ed75d7e7-38d2-4260-88a8-2bba7c5ca70a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017365790590>]}
[0m00:07:34.746443 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m00:07:34.748740 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m00:07:34.869261 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 3 files changed.
[0m00:07:34.870252 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://models\marts\sales\dim_sales.yml
[0m00:07:34.871166 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://models\marts\product\dim_product.yml
[0m00:07:34.871166 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://models\example\schema.yml
[0m00:07:35.096509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ed75d7e7-38d2-4260-88a8-2bba7c5ca70a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000173779D9550>]}
[0m00:07:35.112153 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ed75d7e7-38d2-4260-88a8-2bba7c5ca70a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017377C9DCD0>]}
[0m00:07:35.112153 [info ] [MainThread]: Found 5 models, 7 snapshots, 10 tests, 9 sources, 0 exposures, 0 metrics, 535 macros, 0 groups, 0 semantic models
[0m00:07:35.112153 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ed75d7e7-38d2-4260-88a8-2bba7c5ca70a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017377ADCAD0>]}
[0m00:07:35.112153 [info ] [MainThread]: 
[0m00:07:35.127675 [debug] [MainThread]: Databricks adapter: conn: 1595437895056: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (14596, 9832), cmpt: ``, lut: None
[0m00:07:35.127675 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:07:35.127675 [debug] [MainThread]: Databricks adapter: Thread (14596, 9832) using default compute resource.
[0m00:07:35.127675 [debug] [MainThread]: Databricks adapter: conn: 1595437895056: _acquire sess: None, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (14596, 9832), cmpt: ``, lut: 1707844055.1276755
[0m00:07:35.127675 [debug] [ThreadPool]: Databricks adapter: conn: 1595439631440: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_snapshots, idle: 0s, acqrelcnt: 0, lang: None, thrd: (14596, 17120), cmpt: ``, lut: None
[0m00:07:35.127675 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_snapshots'
[0m00:07:35.127675 [debug] [ThreadPool]: Databricks adapter: Thread (14596, 17120) using default compute resource.
[0m00:07:35.127675 [debug] [ThreadPool]: Databricks adapter: conn: 1595439631440: _acquire sess: None, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (14596, 17120), cmpt: ``, lut: 1707844055.1276755
[0m00:07:35.213667 [debug] [ThreadPool]: Databricks adapter: conn: 1595439631440: get_thread_connection: sess: None, name: list_hive_metastore_snapshots, idle: 0.08599185943603516s, acqrelcnt: 1, lang: None, thrd: (14596, 17120), cmpt: ``, lut: 1707844055.1276755
[0m00:07:35.213667 [debug] [ThreadPool]: Databricks adapter: conn: 1595439631440: idle check connection: sess: None, name: list_hive_metastore_snapshots, idle: 0.08599185943603516s, acqrelcnt: 1, lang: None, thrd: (14596, 17120), cmpt: ``, lut: 1707844055.1276755
[0m00:07:35.213667 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:07:35.213667 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m00:07:35.213667 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:07:35.609751 [debug] [ThreadPool]: Databricks adapter: conn: 1595439631440: session opened sess: 4c402e9d-2c24-4814-8f5c-49e61c541493, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (14596, 17120), cmpt: ``, lut: 1707844055.6097517
[0m00:07:35.893623 [debug] [ThreadPool]: SQL status: OK in 0.6800000071525574 seconds
[0m00:07:35.909159 [debug] [ThreadPool]: Databricks adapter: conn: 1595439631440: get_thread_connection: sess: 4c402e9d-2c24-4814-8f5c-49e61c541493, name: list_hive_metastore_snapshots, idle: 0.299407958984375s, acqrelcnt: 1, lang: None, thrd: (14596, 17120), cmpt: ``, lut: 1707844055.6097517
[0m00:07:35.909159 [debug] [ThreadPool]: Databricks adapter: conn: 1595439631440: idle check connection: sess: 4c402e9d-2c24-4814-8f5c-49e61c541493, name: list_hive_metastore_snapshots, idle: 0.299407958984375s, acqrelcnt: 1, lang: None, thrd: (14596, 17120), cmpt: ``, lut: 1707844055.6097517
[0m00:07:35.909159 [debug] [ThreadPool]: Databricks adapter: conn: 1595439631440: get_thread_connection: sess: 4c402e9d-2c24-4814-8f5c-49e61c541493, name: list_hive_metastore_snapshots, idle: 0.299407958984375s, acqrelcnt: 1, lang: None, thrd: (14596, 17120), cmpt: ``, lut: 1707844055.6097517
[0m00:07:35.909159 [debug] [ThreadPool]: Databricks adapter: conn: 1595439631440: idle check connection: sess: 4c402e9d-2c24-4814-8f5c-49e61c541493, name: list_hive_metastore_snapshots, idle: 0.299407958984375s, acqrelcnt: 1, lang: None, thrd: (14596, 17120), cmpt: ``, lut: 1707844055.6097517
[0m00:07:35.909159 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:07:35.909159 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:07:35.909159 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m00:07:36.049886 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m00:07:36.049886 [debug] [ThreadPool]: Databricks adapter: conn: 1595439631440: get_thread_connection: sess: 4c402e9d-2c24-4814-8f5c-49e61c541493, name: list_hive_metastore_snapshots, idle: 0.44013500213623047s, acqrelcnt: 1, lang: None, thrd: (14596, 17120), cmpt: ``, lut: 1707844055.6097517
[0m00:07:36.049886 [debug] [ThreadPool]: Databricks adapter: conn: 1595439631440: idle check connection: sess: 4c402e9d-2c24-4814-8f5c-49e61c541493, name: list_hive_metastore_snapshots, idle: 0.44013500213623047s, acqrelcnt: 1, lang: None, thrd: (14596, 17120), cmpt: ``, lut: 1707844055.6097517
[0m00:07:36.049886 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:07:36.049886 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m00:07:36.332439 [debug] [ThreadPool]: SQL status: OK in 0.2800000011920929 seconds
[0m00:07:36.332439 [debug] [ThreadPool]: Databricks adapter: conn: 1595439631440: _release sess: 4c402e9d-2c24-4814-8f5c-49e61c541493, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (14596, 17120), cmpt: ``, lut: 1707844056.3324392
[0m00:07:36.332439 [debug] [ThreadPool]: Databricks adapter: conn: 1595439631440: idle check connection: sess: 4c402e9d-2c24-4814-8f5c-49e61c541493, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (14596, 17120), cmpt: ``, lut: 1707844056.3324392
[0m00:07:36.332439 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now list_hive_metastore_saleslt)
[0m00:07:36.332439 [debug] [ThreadPool]: Databricks adapter: conn: 1595439631440: reusing connection list_hive_metastore_snapshots sess: 4c402e9d-2c24-4814-8f5c-49e61c541493, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (14596, 17120), cmpt: ``, lut: 1707844056.3324392
[0m00:07:36.332439 [debug] [ThreadPool]: Databricks adapter: Thread (14596, 17120) using default compute resource.
[0m00:07:36.332439 [debug] [ThreadPool]: Databricks adapter: conn: 1595439631440: _acquire sess: 4c402e9d-2c24-4814-8f5c-49e61c541493, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (14596, 17120), cmpt: ``, lut: 1707844056.3324392
[0m00:07:36.332439 [debug] [ThreadPool]: Databricks adapter: conn: 1595439631440: get_thread_connection: sess: 4c402e9d-2c24-4814-8f5c-49e61c541493, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (14596, 17120), cmpt: ``, lut: 1707844056.3324392
[0m00:07:36.332439 [debug] [ThreadPool]: Databricks adapter: conn: 1595439631440: idle check connection: sess: 4c402e9d-2c24-4814-8f5c-49e61c541493, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (14596, 17120), cmpt: ``, lut: 1707844056.3324392
[0m00:07:36.332439 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:07:36.332439 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m00:07:36.536463 [debug] [ThreadPool]: SQL status: OK in 0.20000000298023224 seconds
[0m00:07:36.536463 [debug] [ThreadPool]: Databricks adapter: conn: 1595439631440: get_thread_connection: sess: 4c402e9d-2c24-4814-8f5c-49e61c541493, name: list_hive_metastore_saleslt, idle: 0.2040238380432129s, acqrelcnt: 1, lang: None, thrd: (14596, 17120), cmpt: ``, lut: 1707844056.3324392
[0m00:07:36.536463 [debug] [ThreadPool]: Databricks adapter: conn: 1595439631440: idle check connection: sess: 4c402e9d-2c24-4814-8f5c-49e61c541493, name: list_hive_metastore_saleslt, idle: 0.2040238380432129s, acqrelcnt: 1, lang: None, thrd: (14596, 17120), cmpt: ``, lut: 1707844056.3324392
[0m00:07:36.536463 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:07:36.536463 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m00:07:36.677141 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m00:07:36.677141 [debug] [ThreadPool]: Databricks adapter: conn: 1595439631440: get_thread_connection: sess: 4c402e9d-2c24-4814-8f5c-49e61c541493, name: list_hive_metastore_saleslt, idle: 0.34470200538635254s, acqrelcnt: 1, lang: None, thrd: (14596, 17120), cmpt: ``, lut: 1707844056.3324392
[0m00:07:36.677141 [debug] [ThreadPool]: Databricks adapter: conn: 1595439631440: idle check connection: sess: 4c402e9d-2c24-4814-8f5c-49e61c541493, name: list_hive_metastore_saleslt, idle: 0.34470200538635254s, acqrelcnt: 1, lang: None, thrd: (14596, 17120), cmpt: ``, lut: 1707844056.3324392
[0m00:07:36.677141 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:07:36.677141 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m00:07:36.833257 [debug] [ThreadPool]: SQL status: OK in 0.1599999964237213 seconds
[0m00:07:36.833257 [debug] [ThreadPool]: Databricks adapter: conn: 1595439631440: _release sess: 4c402e9d-2c24-4814-8f5c-49e61c541493, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (14596, 17120), cmpt: ``, lut: 1707844056.8332572
[0m00:07:36.833257 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ed75d7e7-38d2-4260-88a8-2bba7c5ca70a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017377DED950>]}
[0m00:07:36.833257 [debug] [MainThread]: Databricks adapter: conn: 1595437895056: get_thread_connection: sess: None, name: master, idle: 1.7055816650390625s, acqrelcnt: 1, lang: None, thrd: (14596, 9832), cmpt: ``, lut: 1707844055.1276755
[0m00:07:36.833257 [debug] [MainThread]: Databricks adapter: conn: 1595437895056: idle check connection: sess: None, name: master, idle: 1.7055816650390625s, acqrelcnt: 1, lang: None, thrd: (14596, 9832), cmpt: ``, lut: 1707844055.1276755
[0m00:07:36.833257 [debug] [MainThread]: Databricks adapter: conn: 1595437895056: get_thread_connection: sess: None, name: master, idle: 1.7055816650390625s, acqrelcnt: 1, lang: None, thrd: (14596, 9832), cmpt: ``, lut: 1707844055.1276755
[0m00:07:36.833257 [debug] [MainThread]: Databricks adapter: conn: 1595437895056: idle check connection: sess: None, name: master, idle: 1.7055816650390625s, acqrelcnt: 1, lang: None, thrd: (14596, 9832), cmpt: ``, lut: 1707844055.1276755
[0m00:07:36.833257 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:07:36.833257 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:07:36.833257 [debug] [MainThread]: Databricks adapter: conn: 1595437895056: _release sess: None, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (14596, 9832), cmpt: ``, lut: 1707844056.8332572
[0m00:07:36.833257 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:07:36.848880 [info ] [MainThread]: 
[0m00:07:36.848880 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_category.1cbc6e05d3
[0m00:07:36.848880 [info ] [Thread-1 (]: 1 of 10 START test not_null_dim_product_category ............................... [RUN]
[0m00:07:36.848880 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: Creating DatabricksDBTConnection sess: None, name: test.medallion_dbt_spark.not_null_dim_product_category.1cbc6e05d3, idle: 0s, acqrelcnt: 0, lang: None, thrd: (14596, 26072), cmpt: ``, lut: None
[0m00:07:36.848880 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.medallion_dbt_spark.not_null_dim_product_category.1cbc6e05d3'
[0m00:07:36.848880 [debug] [Thread-1 (]: Databricks adapter: On thread (14596, 26072): None using default compute resource.
[0m00:07:36.848880 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: _acquire sess: None, name: test.medallion_dbt_spark.not_null_dim_product_category.1cbc6e05d3, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844056.848881
[0m00:07:36.848880 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_category.1cbc6e05d3
[0m00:07:36.871485 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_category.1cbc6e05d3"
[0m00:07:36.871485 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_category.1cbc6e05d3 (compile): 00:07:36.848880 => 00:07:36.871485
[0m00:07:36.871485 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_category.1cbc6e05d3
[0m00:07:36.889062 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_category.1cbc6e05d3"
[0m00:07:36.890986 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_category.1cbc6e05d3, idle: 0.04210543632507324s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844056.848881
[0m00:07:36.891989 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_category.1cbc6e05d3, idle: 0.04210543632507324s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844056.848881
[0m00:07:36.892133 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_category.1cbc6e05d3, idle: 0.04325294494628906s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844056.848881
[0m00:07:36.892133 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_category.1cbc6e05d3, idle: 0.04325294494628906s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844056.848881
[0m00:07:36.892133 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:07:36.892133 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_category.1cbc6e05d3"
[0m00:07:36.892133 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_category.1cbc6e05d3: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_category.1cbc6e05d3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select category
from `hive_metastore`.`saleslt`.`dim_product`
where category is null



      
    ) dbt_internal_test
[0m00:07:36.892133 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:07:37.154517 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: session opened sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_category.1cbc6e05d3, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844057.154517
[0m00:07:37.655188 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_category.1cbc6e05d3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select category
from `hive_metastore`.`saleslt`.`dim_product`
where category is null



      
    ) dbt_internal_test
[0m00:07:37.655188 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `category` cannot be resolved. Did you mean one of the following? [`Weight`, `Size`, `model`, `ListPrice`, `description`].; line 15 pos 6
[0m00:07:37.655188 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `category` cannot be resolved. Did you mean one of the following? [`Weight`, `Size`, `model`, `ListPrice`, `description`].; line 15 pos 6
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:694)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:571)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:422)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:400)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:385)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:434)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `category` cannot be resolved. Did you mean one of the following? [`Weight`, `Size`, `model`, `ListPrice`, `description`].; line 15 pos 6
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:671)
	... 35 more

[0m00:07:37.655188 [debug] [Thread-1 (]: Databricks adapter: operation-id: f3b0c74d-38d9-4edd-bec7-7d2941a5065b
[0m00:07:37.655188 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_category.1cbc6e05d3 (execute): 00:07:36.871485 => 00:07:37.655188
[0m00:07:37.655188 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: _release sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_category.1cbc6e05d3, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844057.6551883
[0m00:07:37.686307 [debug] [Thread-1 (]: Runtime Error in test not_null_dim_product_category (models\marts\product\dim_product.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `category` cannot be resolved. Did you mean one of the following? [`Weight`, `Size`, `model`, `ListPrice`, `description`].; line 15 pos 6
[0m00:07:37.686307 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: _release sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_category.1cbc6e05d3, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844057.686308
[0m00:07:37.701890 [error] [Thread-1 (]: 1 of 10 ERROR not_null_dim_product_category .................................... [[31mERROR[0m in 0.85s]
[0m00:07:37.701890 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_category.1cbc6e05d3
[0m00:07:37.701890 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_model.8b3629a32f
[0m00:07:37.701890 [info ] [Thread-1 (]: 2 of 10 START test not_null_dim_product_model .................................. [RUN]
[0m00:07:37.701890 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: idle check connection: sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_category.1cbc6e05d3, idle: 0.01558232307434082s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844057.686308
[0m00:07:37.701890 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_category.1cbc6e05d3, now test.medallion_dbt_spark.not_null_dim_product_model.8b3629a32f)
[0m00:07:37.701890 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: reusing connection test.medallion_dbt_spark.not_null_dim_product_category.1cbc6e05d3 sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_model.8b3629a32f, idle: 0.01558232307434082s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844057.686308
[0m00:07:37.701890 [debug] [Thread-1 (]: Databricks adapter: On thread (14596, 26072): None using default compute resource.
[0m00:07:37.701890 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: _acquire sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_model.8b3629a32f, idle: 0.01558232307434082s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844057.686308
[0m00:07:37.701890 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_model.8b3629a32f
[0m00:07:37.701890 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_model.8b3629a32f"
[0m00:07:37.718534 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_model.8b3629a32f (compile): 00:07:37.701890 => 00:07:37.718534
[0m00:07:37.719546 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_model.8b3629a32f
[0m00:07:37.723687 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_model.8b3629a32f"
[0m00:07:37.725281 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: get_thread_connection: sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_model.8b3629a32f, idle: 0.03861594200134277s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844057.686308
[0m00:07:37.725281 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: idle check connection: sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_model.8b3629a32f, idle: 0.038973331451416016s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844057.686308
[0m00:07:37.726423 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_model.8b3629a32f"
[0m00:07:37.727311 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_model.8b3629a32f: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_model.8b3629a32f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select model
from `hive_metastore`.`saleslt`.`dim_product`
where model is null



      
    ) dbt_internal_test
[0m00:07:38.075212 [debug] [Thread-1 (]: SQL status: OK in 0.3499999940395355 seconds
[0m00:07:38.075212 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_model.8b3629a32f (execute): 00:07:37.719546 => 00:07:38.075212
[0m00:07:38.075212 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: _release sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_model.8b3629a32f, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844058.075212
[0m00:07:38.075212 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: _release sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_model.8b3629a32f, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844058.075212
[0m00:07:38.075212 [info ] [Thread-1 (]: 2 of 10 PASS not_null_dim_product_model ........................................ [[32mPASS[0m in 0.37s]
[0m00:07:38.090663 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_model.8b3629a32f
[0m00:07:38.090663 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:07:38.090663 [info ] [Thread-1 (]: 3 of 10 START test not_null_dim_product_product_name ........................... [RUN]
[0m00:07:38.090663 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: idle check connection: sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_model.8b3629a32f, idle: 0.015451431274414062s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844058.075212
[0m00:07:38.090663 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_model.8b3629a32f, now test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5)
[0m00:07:38.090663 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: reusing connection test.medallion_dbt_spark.not_null_dim_product_model.8b3629a32f sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.015451431274414062s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844058.075212
[0m00:07:38.090663 [debug] [Thread-1 (]: Databricks adapter: On thread (14596, 26072): None using default compute resource.
[0m00:07:38.090663 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: _acquire sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.015451431274414062s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844058.075212
[0m00:07:38.090663 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:07:38.090663 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:07:38.090663 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (compile): 00:07:38.090663 => 00:07:38.090663
[0m00:07:38.090663 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:07:38.090663 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:07:38.090663 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: get_thread_connection: sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.015451431274414062s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844058.075212
[0m00:07:38.090663 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: idle check connection: sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.015451431274414062s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844058.075212
[0m00:07:38.090663 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:07:38.106288 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_name
from `hive_metastore`.`saleslt`.`dim_product`
where product_name is null



      
    ) dbt_internal_test
[0m00:07:38.356807 [debug] [Thread-1 (]: SQL status: OK in 0.25 seconds
[0m00:07:38.372387 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (execute): 00:07:38.090663 => 00:07:38.372387
[0m00:07:38.372387 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: _release sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844058.3723872
[0m00:07:38.372387 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: _release sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844058.3723872
[0m00:07:38.372387 [info ] [Thread-1 (]: 3 of 10 PASS not_null_dim_product_product_name ................................. [[32mPASS[0m in 0.28s]
[0m00:07:38.372387 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:07:38.372387 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:07:38.387909 [info ] [Thread-1 (]: 4 of 10 START test not_null_dim_product_product_sk ............................. [RUN]
[0m00:07:38.387909 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: idle check connection: sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.015522003173828125s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844058.3723872
[0m00:07:38.387909 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, now test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417)
[0m00:07:38.387909 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.015522003173828125s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844058.3723872
[0m00:07:38.394957 [debug] [Thread-1 (]: Databricks adapter: On thread (14596, 26072): None using default compute resource.
[0m00:07:38.396069 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: _acquire sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.022570371627807617s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844058.3723872
[0m00:07:38.396807 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:07:38.407198 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:07:38.410149 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (compile): 00:07:38.398196 => 00:07:38.408438
[0m00:07:38.410492 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:07:38.414678 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:07:38.417700 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: get_thread_connection: sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.04531288146972656s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844058.3723872
[0m00:07:38.418705 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: idle check connection: sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.046318769454956055s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844058.3723872
[0m00:07:38.419701 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:07:38.419701 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_sk
from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is null



      
    ) dbt_internal_test
[0m00:07:38.656977 [debug] [Thread-1 (]: SQL status: OK in 0.23999999463558197 seconds
[0m00:07:38.656977 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (execute): 00:07:38.411500 => 00:07:38.656977
[0m00:07:38.656977 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: _release sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844058.656977
[0m00:07:38.656977 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: _release sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844058.656977
[0m00:07:38.656977 [info ] [Thread-1 (]: 4 of 10 PASS not_null_dim_product_product_sk ................................... [[32mPASS[0m in 0.27s]
[0m00:07:38.656977 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:07:38.656977 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:07:38.656977 [info ] [Thread-1 (]: 5 of 10 START test not_null_dim_product_sellstartdate .......................... [RUN]
[0m00:07:38.672470 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: idle check connection: sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.015493631362915039s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844058.656977
[0m00:07:38.672470 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, now test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546)
[0m00:07:38.672470 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.015493631362915039s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844058.656977
[0m00:07:38.672470 [debug] [Thread-1 (]: Databricks adapter: On thread (14596, 26072): None using default compute resource.
[0m00:07:38.672470 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: _acquire sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.015493631362915039s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844058.656977
[0m00:07:38.672470 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:07:38.672470 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:07:38.672470 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (compile): 00:07:38.672470 => 00:07:38.672470
[0m00:07:38.672470 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:07:38.672470 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:07:38.672470 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: get_thread_connection: sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.015493631362915039s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844058.656977
[0m00:07:38.672470 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: idle check connection: sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.015493631362915039s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844058.656977
[0m00:07:38.672470 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:07:38.672470 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select sellstartdate
from `hive_metastore`.`saleslt`.`dim_product`
where sellstartdate is null



      
    ) dbt_internal_test
[0m00:07:38.925196 [debug] [Thread-1 (]: SQL status: OK in 0.25 seconds
[0m00:07:38.925196 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (execute): 00:07:38.672470 => 00:07:38.925196
[0m00:07:38.925196 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: _release sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844058.925197
[0m00:07:38.940700 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: _release sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844058.940701
[0m00:07:38.940700 [info ] [Thread-1 (]: 5 of 10 PASS not_null_dim_product_sellstartdate ................................ [[32mPASS[0m in 0.27s]
[0m00:07:38.940700 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:07:38.940700 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:07:38.940700 [info ] [Thread-1 (]: 6 of 10 START test not_null_my_first_dbt_model_id .............................. [RUN]
[0m00:07:38.940700 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: idle check connection: sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844058.940701
[0m00:07:38.940700 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, now test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710)
[0m00:07:38.940700 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: reusing connection test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844058.940701
[0m00:07:38.940700 [debug] [Thread-1 (]: Databricks adapter: On thread (14596, 26072): None using default compute resource.
[0m00:07:38.940700 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: _acquire sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844058.940701
[0m00:07:38.940700 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:07:38.940700 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:07:38.940700 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 00:07:38.940700 => 00:07:38.940700
[0m00:07:38.940700 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:07:38.940700 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:07:38.956317 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: get_thread_connection: sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.015616655349731445s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844058.940701
[0m00:07:38.957283 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: idle check connection: sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.015616655349731445s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844058.940701
[0m00:07:38.957283 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:07:38.958245 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m00:07:39.274394 [debug] [Thread-1 (]: SQL status: OK in 0.3199999928474426 seconds
[0m00:07:39.274394 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 00:07:38.940700 => 00:07:39.274394
[0m00:07:39.274394 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: _release sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844059.2743945
[0m00:07:39.274394 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: _release sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844059.2743945
[0m00:07:39.274394 [error] [Thread-1 (]: 6 of 10 FAIL 1 not_null_my_first_dbt_model_id .................................. [[31mFAIL 1[0m in 0.33s]
[0m00:07:39.274394 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:07:39.274394 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:07:39.274394 [info ] [Thread-1 (]: 7 of 10 START test not_null_my_second_dbt_model_id ............................. [RUN]
[0m00:07:39.274394 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: idle check connection: sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844059.2743945
[0m00:07:39.274394 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, now test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778)
[0m00:07:39.274394 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: reusing connection test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844059.2743945
[0m00:07:39.274394 [debug] [Thread-1 (]: Databricks adapter: On thread (14596, 26072): None using default compute resource.
[0m00:07:39.274394 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: _acquire sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844059.2743945
[0m00:07:39.274394 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:07:39.291058 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:07:39.292203 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (compile): 00:07:39.274394 => 00:07:39.292203
[0m00:07:39.293235 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:07:39.295992 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:07:39.296135 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: get_thread_connection: sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.02174091339111328s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844059.2743945
[0m00:07:39.296135 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: idle check connection: sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.02174091339111328s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844059.2743945
[0m00:07:39.296135 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:07:39.296135 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m00:07:39.566174 [debug] [Thread-1 (]: SQL status: OK in 0.27000001072883606 seconds
[0m00:07:39.566174 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (execute): 00:07:39.293235 => 00:07:39.566174
[0m00:07:39.566174 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: _release sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844059.5661743
[0m00:07:39.566174 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: _release sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844059.5661743
[0m00:07:39.566174 [info ] [Thread-1 (]: 7 of 10 PASS not_null_my_second_dbt_model_id ................................... [[32mPASS[0m in 0.29s]
[0m00:07:39.566174 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:07:39.566174 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:07:39.566174 [info ] [Thread-1 (]: 8 of 10 START test unique_dim_product_product_sk ............................... [RUN]
[0m00:07:39.566174 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: idle check connection: sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844059.5661743
[0m00:07:39.566174 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, now test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c)
[0m00:07:39.581716 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: reusing connection test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844059.5661743
[0m00:07:39.581716 [debug] [Thread-1 (]: Databricks adapter: On thread (14596, 26072): None using default compute resource.
[0m00:07:39.581716 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: _acquire sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.015541791915893555s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844059.5661743
[0m00:07:39.581716 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:07:39.581716 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:07:39.581716 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (compile): 00:07:39.581716 => 00:07:39.581716
[0m00:07:39.581716 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:07:39.581716 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:07:39.581716 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: get_thread_connection: sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.015541791915893555s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844059.5661743
[0m00:07:39.581716 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: idle check connection: sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.015541791915893555s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844059.5661743
[0m00:07:39.581716 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:07:39.581716 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    product_sk as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is not null
group by product_sk
having count(*) > 1



      
    ) dbt_internal_test
[0m00:07:39.999812 [debug] [Thread-1 (]: SQL status: OK in 0.41999998688697815 seconds
[0m00:07:39.999812 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (execute): 00:07:39.581716 => 00:07:39.999812
[0m00:07:39.999812 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: _release sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844059.9998121
[0m00:07:39.999812 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: _release sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844059.9998121
[0m00:07:39.999812 [info ] [Thread-1 (]: 8 of 10 PASS unique_dim_product_product_sk ..................................... [[32mPASS[0m in 0.43s]
[0m00:07:39.999812 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:07:39.999812 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:07:39.999812 [info ] [Thread-1 (]: 9 of 10 START test unique_my_first_dbt_model_id ................................ [RUN]
[0m00:07:39.999812 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: idle check connection: sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844059.9998121
[0m00:07:39.999812 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, now test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321)
[0m00:07:39.999812 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: reusing connection test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844059.9998121
[0m00:07:39.999812 [debug] [Thread-1 (]: Databricks adapter: On thread (14596, 26072): None using default compute resource.
[0m00:07:39.999812 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: _acquire sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844059.9998121
[0m00:07:39.999812 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:07:40.015431 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:07:40.017243 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (compile): 00:07:39.999812 => 00:07:40.016234
[0m00:07:40.017555 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:07:40.020629 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:07:40.021024 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: get_thread_connection: sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.021212339401245117s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844059.9998121
[0m00:07:40.021024 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: idle check connection: sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.021212339401245117s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844059.9998121
[0m00:07:40.021024 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:07:40.021024 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m00:07:40.341468 [debug] [Thread-1 (]: SQL status: OK in 0.3199999928474426 seconds
[0m00:07:40.356966 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (execute): 00:07:40.017555 => 00:07:40.356966
[0m00:07:40.356966 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: _release sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844060.356967
[0m00:07:40.356966 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: _release sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844060.356967
[0m00:07:40.356966 [info ] [Thread-1 (]: 9 of 10 PASS unique_my_first_dbt_model_id ...................................... [[32mPASS[0m in 0.36s]
[0m00:07:40.356966 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:07:40.356966 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:07:40.356966 [info ] [Thread-1 (]: 10 of 10 START test unique_my_second_dbt_model_id .............................. [RUN]
[0m00:07:40.356966 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: idle check connection: sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844060.356967
[0m00:07:40.356966 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, now test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493)
[0m00:07:40.356966 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: reusing connection test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844060.356967
[0m00:07:40.356966 [debug] [Thread-1 (]: Databricks adapter: On thread (14596, 26072): None using default compute resource.
[0m00:07:40.356966 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: _acquire sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844060.356967
[0m00:07:40.356966 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:07:40.372671 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:07:40.373988 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (compile): 00:07:40.356966 => 00:07:40.373988
[0m00:07:40.374998 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:07:40.378003 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:07:40.378351 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: get_thread_connection: sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.021384239196777344s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844060.356967
[0m00:07:40.379912 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: idle check connection: sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.022944927215576172s, acqrelcnt: 1, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844060.356967
[0m00:07:40.379912 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:07:40.379912 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m00:07:40.713516 [debug] [Thread-1 (]: SQL status: OK in 0.33000001311302185 seconds
[0m00:07:40.729084 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (execute): 00:07:40.374998 => 00:07:40.729084
[0m00:07:40.729084 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: _release sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844060.7290843
[0m00:07:40.729084 [debug] [Thread-1 (]: Databricks adapter: conn: 1595444405520: _release sess: 07075f02-25b3-4ac6-b496-873e6c231cf1, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (14596, 26072), cmpt: ``, lut: 1707844060.7290843
[0m00:07:40.729084 [info ] [Thread-1 (]: 10 of 10 PASS unique_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.37s]
[0m00:07:40.729084 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:07:40.729084 [debug] [MainThread]: Databricks adapter: conn: 1595437895056: idle check connection: sess: None, name: master, idle: 3.895827054977417s, acqrelcnt: 0, lang: None, thrd: (14596, 9832), cmpt: ``, lut: 1707844056.8332572
[0m00:07:40.729084 [debug] [MainThread]: Databricks adapter: conn: 1595437895056: reusing connection master sess: None, name: master, idle: 3.895827054977417s, acqrelcnt: 0, lang: None, thrd: (14596, 9832), cmpt: ``, lut: 1707844056.8332572
[0m00:07:40.729084 [debug] [MainThread]: Databricks adapter: Thread (14596, 9832) using default compute resource.
[0m00:07:40.729084 [debug] [MainThread]: Databricks adapter: conn: 1595437895056: _acquire sess: None, name: master, idle: 3.895827054977417s, acqrelcnt: 1, lang: None, thrd: (14596, 9832), cmpt: ``, lut: 1707844056.8332572
[0m00:07:40.729084 [debug] [MainThread]: Databricks adapter: conn: 1595437895056: get_thread_connection: sess: None, name: master, idle: 3.895827054977417s, acqrelcnt: 1, lang: None, thrd: (14596, 9832), cmpt: ``, lut: 1707844056.8332572
[0m00:07:40.729084 [debug] [MainThread]: Databricks adapter: conn: 1595437895056: idle check connection: sess: None, name: master, idle: 3.895827054977417s, acqrelcnt: 1, lang: None, thrd: (14596, 9832), cmpt: ``, lut: 1707844056.8332572
[0m00:07:40.729084 [debug] [MainThread]: On master: ROLLBACK
[0m00:07:40.729084 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:07:40.963349 [debug] [MainThread]: Databricks adapter: conn: 1595437895056: session opened sess: 7d0993a6-18a2-49ac-81cb-88a514e622c7, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (14596, 9832), cmpt: ``, lut: 1707844060.9633493
[0m00:07:40.963349 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:07:40.963349 [debug] [MainThread]: Databricks adapter: conn: 1595437895056: get_thread_connection: sess: 7d0993a6-18a2-49ac-81cb-88a514e622c7, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (14596, 9832), cmpt: ``, lut: 1707844060.9633493
[0m00:07:40.978847 [debug] [MainThread]: Databricks adapter: conn: 1595437895056: idle check connection: sess: 7d0993a6-18a2-49ac-81cb-88a514e622c7, name: master, idle: 0.015497922897338867s, acqrelcnt: 1, lang: None, thrd: (14596, 9832), cmpt: ``, lut: 1707844060.9633493
[0m00:07:40.978847 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:07:40.978847 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:07:40.978847 [debug] [MainThread]: Databricks adapter: conn: 1595437895056: _release sess: 7d0993a6-18a2-49ac-81cb-88a514e622c7, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (14596, 9832), cmpt: ``, lut: 1707844060.9788473
[0m00:07:40.978847 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:07:40.978847 [debug] [MainThread]: On master: ROLLBACK
[0m00:07:40.978847 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:07:40.978847 [debug] [MainThread]: On master: Close
[0m00:07:41.057107 [debug] [MainThread]: Connection 'list_hive_metastore_saleslt' was properly closed.
[0m00:07:41.057107 [debug] [MainThread]: On list_hive_metastore_saleslt: ROLLBACK
[0m00:07:41.057107 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:07:41.057107 [debug] [MainThread]: On list_hive_metastore_saleslt: Close
[0m00:07:41.136822 [debug] [MainThread]: Connection 'test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493' was properly closed.
[0m00:07:41.136822 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: ROLLBACK
[0m00:07:41.136822 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:07:41.136822 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m00:07:41.214943 [info ] [MainThread]: 
[0m00:07:41.214943 [info ] [MainThread]: Finished running 10 tests in 0 hours 0 minutes and 6.09 seconds (6.09s).
[0m00:07:41.214943 [debug] [MainThread]: Command end result
[0m00:07:41.230565 [info ] [MainThread]: 
[0m00:07:41.230565 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m00:07:41.242195 [info ] [MainThread]: 
[0m00:07:41.243331 [error] [MainThread]:   Runtime Error in test not_null_dim_product_category (models\marts\product\dim_product.yml)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `category` cannot be resolved. Did you mean one of the following? [`Weight`, `Size`, `model`, `ListPrice`, `description`].; line 15 pos 6
[0m00:07:41.247874 [info ] [MainThread]: 
[0m00:07:41.248297 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models\example\schema.yml)[0m
[0m00:07:41.249714 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m00:07:41.250537 [info ] [MainThread]: 
[0m00:07:41.251547 [info ] [MainThread]:   compiled Code at target\compiled\medallion_dbt_spark\models\example\schema.yml\not_null_my_first_dbt_model_id.sql
[0m00:07:41.253084 [info ] [MainThread]: 
[0m00:07:41.254093 [info ] [MainThread]: Done. PASS=8 WARN=0 ERROR=2 SKIP=0 TOTAL=10
[0m00:07:41.256110 [debug] [MainThread]: Command `dbt test` failed at 00:07:41.256110 after 7.82 seconds
[0m00:07:41.256110 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017364EB4250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000173650C0410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000173650C2F50>]}
[0m00:07:41.257091 [debug] [MainThread]: Flushing usage events
[0m00:08:23.908225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C0B8A6FA50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C0B876C210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C0B8844AD0>]}


============================== 00:08:23.923843 | 35360ba1-76c4-4a55-bad0-602ca82a6899 ==============================
[0m00:08:23.923843 [info ] [MainThread]: Running with dbt=1.7.7
[0m00:08:23.923843 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt test', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m00:08:25.101339 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '35360ba1-76c4-4a55-bad0-602ca82a6899', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C0CAF0E690>]}
[0m00:08:25.165735 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '35360ba1-76c4-4a55-bad0-602ca82a6899', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C0CAF35410>]}
[0m00:08:25.165735 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m00:08:25.165735 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m00:08:25.252083 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:08:25.252083 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://models\marts\product\dim_product.yml
[0m00:08:25.505723 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '35360ba1-76c4-4a55-bad0-602ca82a6899', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C0CB0F1F90>]}
[0m00:08:25.521348 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '35360ba1-76c4-4a55-bad0-602ca82a6899', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C0CB4B2850>]}
[0m00:08:25.521348 [info ] [MainThread]: Found 5 models, 7 snapshots, 8 tests, 9 sources, 0 exposures, 0 metrics, 535 macros, 0 groups, 0 semantic models
[0m00:08:25.521348 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '35360ba1-76c4-4a55-bad0-602ca82a6899', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C0CB1AA890>]}
[0m00:08:25.521348 [info ] [MainThread]: 
[0m00:08:25.536975 [debug] [MainThread]: Databricks adapter: conn: 1927552684944: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (23940, 15240), cmpt: ``, lut: None
[0m00:08:25.536975 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:08:25.536975 [debug] [MainThread]: Databricks adapter: Thread (23940, 15240) using default compute resource.
[0m00:08:25.536975 [debug] [MainThread]: Databricks adapter: conn: 1927552684944: _acquire sess: None, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23940, 15240), cmpt: ``, lut: 1707844105.5369751
[0m00:08:25.536975 [debug] [ThreadPool]: Databricks adapter: conn: 1927552291792: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_saleslt, idle: 0s, acqrelcnt: 0, lang: None, thrd: (23940, 8356), cmpt: ``, lut: None
[0m00:08:25.536975 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m00:08:25.536975 [debug] [ThreadPool]: Databricks adapter: Thread (23940, 8356) using default compute resource.
[0m00:08:25.536975 [debug] [ThreadPool]: Databricks adapter: conn: 1927552291792: _acquire sess: None, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23940, 8356), cmpt: ``, lut: 1707844105.5369751
[0m00:08:25.536975 [debug] [ThreadPool]: Databricks adapter: conn: 1927552291792: get_thread_connection: sess: None, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23940, 8356), cmpt: ``, lut: 1707844105.5369751
[0m00:08:25.536975 [debug] [ThreadPool]: Databricks adapter: conn: 1927552291792: idle check connection: sess: None, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23940, 8356), cmpt: ``, lut: 1707844105.5369751
[0m00:08:25.536975 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:08:25.536975 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m00:08:25.536975 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:08:25.855200 [debug] [ThreadPool]: Databricks adapter: conn: 1927552291792: session opened sess: 6f8fabe3-e940-4dec-b469-981dbbe53fe8, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23940, 8356), cmpt: ``, lut: 1707844105.8552008
[0m00:08:25.964665 [debug] [ThreadPool]: SQL status: OK in 0.4300000071525574 seconds
[0m00:08:25.980212 [debug] [ThreadPool]: Databricks adapter: conn: 1927552291792: get_thread_connection: sess: 6f8fabe3-e940-4dec-b469-981dbbe53fe8, name: list_hive_metastore_saleslt, idle: 0.12501168251037598s, acqrelcnt: 1, lang: None, thrd: (23940, 8356), cmpt: ``, lut: 1707844105.8552008
[0m00:08:25.980212 [debug] [ThreadPool]: Databricks adapter: conn: 1927552291792: idle check connection: sess: 6f8fabe3-e940-4dec-b469-981dbbe53fe8, name: list_hive_metastore_saleslt, idle: 0.12501168251037598s, acqrelcnt: 1, lang: None, thrd: (23940, 8356), cmpt: ``, lut: 1707844105.8552008
[0m00:08:25.980212 [debug] [ThreadPool]: Databricks adapter: conn: 1927552291792: get_thread_connection: sess: 6f8fabe3-e940-4dec-b469-981dbbe53fe8, name: list_hive_metastore_saleslt, idle: 0.12501168251037598s, acqrelcnt: 1, lang: None, thrd: (23940, 8356), cmpt: ``, lut: 1707844105.8552008
[0m00:08:25.980212 [debug] [ThreadPool]: Databricks adapter: conn: 1927552291792: idle check connection: sess: 6f8fabe3-e940-4dec-b469-981dbbe53fe8, name: list_hive_metastore_saleslt, idle: 0.12501168251037598s, acqrelcnt: 1, lang: None, thrd: (23940, 8356), cmpt: ``, lut: 1707844105.8552008
[0m00:08:25.980212 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:08:25.980212 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:08:25.980212 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m00:08:26.120951 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m00:08:26.136529 [debug] [ThreadPool]: Databricks adapter: conn: 1927552291792: get_thread_connection: sess: 6f8fabe3-e940-4dec-b469-981dbbe53fe8, name: list_hive_metastore_saleslt, idle: 0.2813291549682617s, acqrelcnt: 1, lang: None, thrd: (23940, 8356), cmpt: ``, lut: 1707844105.8552008
[0m00:08:26.136529 [debug] [ThreadPool]: Databricks adapter: conn: 1927552291792: idle check connection: sess: 6f8fabe3-e940-4dec-b469-981dbbe53fe8, name: list_hive_metastore_saleslt, idle: 0.2813291549682617s, acqrelcnt: 1, lang: None, thrd: (23940, 8356), cmpt: ``, lut: 1707844105.8552008
[0m00:08:26.136529 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:08:26.136529 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m00:08:26.295640 [debug] [ThreadPool]: SQL status: OK in 0.1599999964237213 seconds
[0m00:08:26.295640 [debug] [ThreadPool]: Databricks adapter: conn: 1927552291792: _release sess: 6f8fabe3-e940-4dec-b469-981dbbe53fe8, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (23940, 8356), cmpt: ``, lut: 1707844106.29564
[0m00:08:26.295640 [debug] [ThreadPool]: Databricks adapter: conn: 1927552291792: idle check connection: sess: 6f8fabe3-e940-4dec-b469-981dbbe53fe8, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (23940, 8356), cmpt: ``, lut: 1707844106.29564
[0m00:08:26.295640 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m00:08:26.295640 [debug] [ThreadPool]: Databricks adapter: conn: 1927552291792: reusing connection list_hive_metastore_saleslt sess: 6f8fabe3-e940-4dec-b469-981dbbe53fe8, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (23940, 8356), cmpt: ``, lut: 1707844106.29564
[0m00:08:26.295640 [debug] [ThreadPool]: Databricks adapter: Thread (23940, 8356) using default compute resource.
[0m00:08:26.295640 [debug] [ThreadPool]: Databricks adapter: conn: 1927552291792: _acquire sess: 6f8fabe3-e940-4dec-b469-981dbbe53fe8, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23940, 8356), cmpt: ``, lut: 1707844106.29564
[0m00:08:26.295640 [debug] [ThreadPool]: Databricks adapter: conn: 1927552291792: get_thread_connection: sess: 6f8fabe3-e940-4dec-b469-981dbbe53fe8, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23940, 8356), cmpt: ``, lut: 1707844106.29564
[0m00:08:26.295640 [debug] [ThreadPool]: Databricks adapter: conn: 1927552291792: idle check connection: sess: 6f8fabe3-e940-4dec-b469-981dbbe53fe8, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23940, 8356), cmpt: ``, lut: 1707844106.29564
[0m00:08:26.295640 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:08:26.295640 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m00:08:26.436275 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m00:08:26.436275 [debug] [ThreadPool]: Databricks adapter: conn: 1927552291792: get_thread_connection: sess: 6f8fabe3-e940-4dec-b469-981dbbe53fe8, name: list_hive_metastore_snapshots, idle: 0.14063572883605957s, acqrelcnt: 1, lang: None, thrd: (23940, 8356), cmpt: ``, lut: 1707844106.29564
[0m00:08:26.451907 [debug] [ThreadPool]: Databricks adapter: conn: 1927552291792: idle check connection: sess: 6f8fabe3-e940-4dec-b469-981dbbe53fe8, name: list_hive_metastore_snapshots, idle: 0.15626764297485352s, acqrelcnt: 1, lang: None, thrd: (23940, 8356), cmpt: ``, lut: 1707844106.29564
[0m00:08:26.451907 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:08:26.451907 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m00:08:26.561888 [debug] [ThreadPool]: SQL status: OK in 0.10999999940395355 seconds
[0m00:08:26.561888 [debug] [ThreadPool]: Databricks adapter: conn: 1927552291792: get_thread_connection: sess: 6f8fabe3-e940-4dec-b469-981dbbe53fe8, name: list_hive_metastore_snapshots, idle: 0.2662484645843506s, acqrelcnt: 1, lang: None, thrd: (23940, 8356), cmpt: ``, lut: 1707844106.29564
[0m00:08:26.576988 [debug] [ThreadPool]: Databricks adapter: conn: 1927552291792: idle check connection: sess: 6f8fabe3-e940-4dec-b469-981dbbe53fe8, name: list_hive_metastore_snapshots, idle: 0.2662484645843506s, acqrelcnt: 1, lang: None, thrd: (23940, 8356), cmpt: ``, lut: 1707844106.29564
[0m00:08:26.576988 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:08:26.577995 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m00:08:26.720206 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m00:08:26.720206 [debug] [ThreadPool]: Databricks adapter: conn: 1927552291792: _release sess: 6f8fabe3-e940-4dec-b469-981dbbe53fe8, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (23940, 8356), cmpt: ``, lut: 1707844106.7202065
[0m00:08:26.720206 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '35360ba1-76c4-4a55-bad0-602ca82a6899', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C0CB39F110>]}
[0m00:08:26.720206 [debug] [MainThread]: Databricks adapter: conn: 1927552684944: get_thread_connection: sess: None, name: master, idle: 1.1832313537597656s, acqrelcnt: 1, lang: None, thrd: (23940, 15240), cmpt: ``, lut: 1707844105.5369751
[0m00:08:26.720206 [debug] [MainThread]: Databricks adapter: conn: 1927552684944: idle check connection: sess: None, name: master, idle: 1.1832313537597656s, acqrelcnt: 1, lang: None, thrd: (23940, 15240), cmpt: ``, lut: 1707844105.5369751
[0m00:08:26.720206 [debug] [MainThread]: Databricks adapter: conn: 1927552684944: get_thread_connection: sess: None, name: master, idle: 1.1832313537597656s, acqrelcnt: 1, lang: None, thrd: (23940, 15240), cmpt: ``, lut: 1707844105.5369751
[0m00:08:26.720206 [debug] [MainThread]: Databricks adapter: conn: 1927552684944: idle check connection: sess: None, name: master, idle: 1.1832313537597656s, acqrelcnt: 1, lang: None, thrd: (23940, 15240), cmpt: ``, lut: 1707844105.5369751
[0m00:08:26.720206 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:08:26.735839 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:08:26.735839 [debug] [MainThread]: Databricks adapter: conn: 1927552684944: _release sess: None, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (23940, 15240), cmpt: ``, lut: 1707844106.7358396
[0m00:08:26.736855 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:08:26.738035 [info ] [MainThread]: 
[0m00:08:26.741661 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:08:26.742680 [info ] [Thread-1 (]: 1 of 8 START test not_null_dim_product_product_name ............................ [RUN]
[0m00:08:26.744448 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: Creating DatabricksDBTConnection sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0s, acqrelcnt: 0, lang: None, thrd: (23940, 3972), cmpt: ``, lut: None
[0m00:08:26.744448 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5'
[0m00:08:26.745456 [debug] [Thread-1 (]: Databricks adapter: On thread (23940, 3972): None using default compute resource.
[0m00:08:26.745456 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: _acquire sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844106.745457
[0m00:08:26.746461 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:08:26.763031 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:08:26.764951 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (compile): 00:08:26.746461 => 00:08:26.764035
[0m00:08:26.764951 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:08:26.776224 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:08:26.776224 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.030768156051635742s, acqrelcnt: 1, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844106.745457
[0m00:08:26.776224 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.030768156051635742s, acqrelcnt: 1, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844106.745457
[0m00:08:26.776224 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.030768156051635742s, acqrelcnt: 1, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844106.745457
[0m00:08:26.776224 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.030768156051635742s, acqrelcnt: 1, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844106.745457
[0m00:08:26.776224 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:08:26.786766 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:08:26.786766 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_name
from `hive_metastore`.`saleslt`.`dim_product`
where product_name is null



      
    ) dbt_internal_test
[0m00:08:26.787823 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:08:27.074761 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: session opened sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844107.0747612
[0m00:08:27.349190 [debug] [Thread-1 (]: SQL status: OK in 0.5600000023841858 seconds
[0m00:08:27.349190 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (execute): 00:08:26.764951 => 00:08:27.349190
[0m00:08:27.349190 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: _release sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844107.3491907
[0m00:08:27.349190 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: _release sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844107.3491907
[0m00:08:27.349190 [info ] [Thread-1 (]: 1 of 8 PASS not_null_dim_product_product_name .................................. [[32mPASS[0m in 0.61s]
[0m00:08:27.349190 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:08:27.349190 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:08:27.349190 [info ] [Thread-1 (]: 2 of 8 START test not_null_dim_product_product_sk .............................. [RUN]
[0m00:08:27.364709 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: idle check connection: sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.015519142150878906s, acqrelcnt: 0, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844107.3491907
[0m00:08:27.364709 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, now test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417)
[0m00:08:27.364709 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.015519142150878906s, acqrelcnt: 0, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844107.3491907
[0m00:08:27.364709 [debug] [Thread-1 (]: Databricks adapter: On thread (23940, 3972): None using default compute resource.
[0m00:08:27.364709 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: _acquire sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.015519142150878906s, acqrelcnt: 1, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844107.3491907
[0m00:08:27.364709 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:08:27.364709 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:08:27.364709 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (compile): 00:08:27.364709 => 00:08:27.364709
[0m00:08:27.364709 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:08:27.364709 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:08:27.364709 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: get_thread_connection: sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.015519142150878906s, acqrelcnt: 1, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844107.3491907
[0m00:08:27.364709 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: idle check connection: sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.015519142150878906s, acqrelcnt: 1, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844107.3491907
[0m00:08:27.364709 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:08:27.364709 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_sk
from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is null



      
    ) dbt_internal_test
[0m00:08:27.630335 [debug] [Thread-1 (]: SQL status: OK in 0.27000001072883606 seconds
[0m00:08:27.630335 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (execute): 00:08:27.364709 => 00:08:27.630335
[0m00:08:27.630335 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: _release sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844107.6303353
[0m00:08:27.645832 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: _release sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844107.6458325
[0m00:08:27.645832 [info ] [Thread-1 (]: 2 of 8 PASS not_null_dim_product_product_sk .................................... [[32mPASS[0m in 0.28s]
[0m00:08:27.645832 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:08:27.645832 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:08:27.645832 [info ] [Thread-1 (]: 3 of 8 START test not_null_dim_product_sellstartdate ........................... [RUN]
[0m00:08:27.645832 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: idle check connection: sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844107.6458325
[0m00:08:27.645832 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, now test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546)
[0m00:08:27.645832 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844107.6458325
[0m00:08:27.645832 [debug] [Thread-1 (]: Databricks adapter: On thread (23940, 3972): None using default compute resource.
[0m00:08:27.645832 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: _acquire sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844107.6458325
[0m00:08:27.645832 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:08:27.645832 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:08:27.645832 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (compile): 00:08:27.645832 => 00:08:27.645832
[0m00:08:27.645832 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:08:27.645832 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:08:27.661540 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: get_thread_connection: sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.015707969665527344s, acqrelcnt: 1, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844107.6458325
[0m00:08:27.662550 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: idle check connection: sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.016717910766601562s, acqrelcnt: 1, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844107.6458325
[0m00:08:27.663560 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:08:27.663560 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select sellstartdate
from `hive_metastore`.`saleslt`.`dim_product`
where sellstartdate is null



      
    ) dbt_internal_test
[0m00:08:27.895834 [debug] [Thread-1 (]: SQL status: OK in 0.23000000417232513 seconds
[0m00:08:27.895834 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (execute): 00:08:27.645832 => 00:08:27.895834
[0m00:08:27.895834 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: _release sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844107.8958347
[0m00:08:27.895834 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: _release sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844107.8958347
[0m00:08:27.895834 [info ] [Thread-1 (]: 3 of 8 PASS not_null_dim_product_sellstartdate ................................. [[32mPASS[0m in 0.25s]
[0m00:08:27.895834 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:08:27.895834 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:08:27.895834 [info ] [Thread-1 (]: 4 of 8 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m00:08:27.911373 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: idle check connection: sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.015538692474365234s, acqrelcnt: 0, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844107.8958347
[0m00:08:27.911373 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, now test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710)
[0m00:08:27.911373 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: reusing connection test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.015538692474365234s, acqrelcnt: 0, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844107.8958347
[0m00:08:27.911373 [debug] [Thread-1 (]: Databricks adapter: On thread (23940, 3972): None using default compute resource.
[0m00:08:27.911373 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: _acquire sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.015538692474365234s, acqrelcnt: 1, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844107.8958347
[0m00:08:27.911373 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:08:27.911373 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:08:27.911373 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 00:08:27.911373 => 00:08:27.911373
[0m00:08:27.911373 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:08:27.911373 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:08:27.911373 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: get_thread_connection: sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.015538692474365234s, acqrelcnt: 1, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844107.8958347
[0m00:08:27.911373 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: idle check connection: sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.015538692474365234s, acqrelcnt: 1, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844107.8958347
[0m00:08:27.911373 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:08:27.911373 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m00:08:28.209310 [debug] [Thread-1 (]: SQL status: OK in 0.30000001192092896 seconds
[0m00:08:28.209310 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 00:08:27.911373 => 00:08:28.209310
[0m00:08:28.209310 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: _release sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844108.2093103
[0m00:08:28.209310 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: _release sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844108.2093103
[0m00:08:28.209310 [error] [Thread-1 (]: 4 of 8 FAIL 1 not_null_my_first_dbt_model_id ................................... [[31mFAIL 1[0m in 0.31s]
[0m00:08:28.209310 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:08:28.209310 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:08:28.209310 [info ] [Thread-1 (]: 5 of 8 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m00:08:28.209310 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: idle check connection: sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844108.2093103
[0m00:08:28.209310 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, now test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778)
[0m00:08:28.224801 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: reusing connection test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.01549077033996582s, acqrelcnt: 0, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844108.2093103
[0m00:08:28.224801 [debug] [Thread-1 (]: Databricks adapter: On thread (23940, 3972): None using default compute resource.
[0m00:08:28.224801 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: _acquire sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.01549077033996582s, acqrelcnt: 1, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844108.2093103
[0m00:08:28.224801 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:08:28.224801 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:08:28.224801 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (compile): 00:08:28.224801 => 00:08:28.224801
[0m00:08:28.224801 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:08:28.224801 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:08:28.224801 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: get_thread_connection: sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.01549077033996582s, acqrelcnt: 1, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844108.2093103
[0m00:08:28.224801 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: idle check connection: sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.01549077033996582s, acqrelcnt: 1, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844108.2093103
[0m00:08:28.224801 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:08:28.224801 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m00:08:28.620007 [debug] [Thread-1 (]: SQL status: OK in 0.4000000059604645 seconds
[0m00:08:28.620007 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (execute): 00:08:28.224801 => 00:08:28.620007
[0m00:08:28.620007 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: _release sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844108.6200073
[0m00:08:28.635633 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: _release sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844108.6200073
[0m00:08:28.635633 [info ] [Thread-1 (]: 5 of 8 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.43s]
[0m00:08:28.635633 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:08:28.635633 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:08:28.635633 [info ] [Thread-1 (]: 6 of 8 START test unique_dim_product_product_sk ................................ [RUN]
[0m00:08:28.635633 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: idle check connection: sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.01562666893005371s, acqrelcnt: 0, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844108.6200073
[0m00:08:28.635633 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, now test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c)
[0m00:08:28.635633 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: reusing connection test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.01562666893005371s, acqrelcnt: 0, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844108.6200073
[0m00:08:28.635633 [debug] [Thread-1 (]: Databricks adapter: On thread (23940, 3972): None using default compute resource.
[0m00:08:28.635633 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: _acquire sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.01562666893005371s, acqrelcnt: 1, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844108.6200073
[0m00:08:28.635633 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:08:28.651537 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:08:28.653606 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (compile): 00:08:28.635633 => 00:08:28.652597
[0m00:08:28.654653 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:08:28.657772 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:08:28.661489 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: get_thread_connection: sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.04148244857788086s, acqrelcnt: 1, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844108.6200073
[0m00:08:28.662484 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: idle check connection: sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.04247689247131348s, acqrelcnt: 1, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844108.6200073
[0m00:08:28.662484 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:08:28.663485 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    product_sk as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is not null
group by product_sk
having count(*) > 1



      
    ) dbt_internal_test
[0m00:08:29.027973 [debug] [Thread-1 (]: SQL status: OK in 0.36000001430511475 seconds
[0m00:08:29.027973 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (execute): 00:08:28.654653 => 00:08:29.027973
[0m00:08:29.027973 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: _release sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844109.027974
[0m00:08:29.027973 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: _release sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844109.027974
[0m00:08:29.027973 [info ] [Thread-1 (]: 6 of 8 PASS unique_dim_product_product_sk ...................................... [[32mPASS[0m in 0.39s]
[0m00:08:29.027973 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:08:29.027973 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:08:29.027973 [info ] [Thread-1 (]: 7 of 8 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m00:08:29.027973 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: idle check connection: sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844109.027974
[0m00:08:29.043542 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, now test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321)
[0m00:08:29.043542 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: reusing connection test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.015568733215332031s, acqrelcnt: 0, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844109.027974
[0m00:08:29.043542 [debug] [Thread-1 (]: Databricks adapter: On thread (23940, 3972): None using default compute resource.
[0m00:08:29.043542 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: _acquire sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.015568733215332031s, acqrelcnt: 1, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844109.027974
[0m00:08:29.043542 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:08:29.043542 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:08:29.043542 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (compile): 00:08:29.043542 => 00:08:29.043542
[0m00:08:29.043542 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:08:29.043542 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:08:29.043542 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: get_thread_connection: sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.015568733215332031s, acqrelcnt: 1, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844109.027974
[0m00:08:29.043542 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: idle check connection: sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.015568733215332031s, acqrelcnt: 1, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844109.027974
[0m00:08:29.043542 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:08:29.043542 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m00:08:29.387810 [debug] [Thread-1 (]: SQL status: OK in 0.3400000035762787 seconds
[0m00:08:29.403278 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (execute): 00:08:29.043542 => 00:08:29.403278
[0m00:08:29.403278 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: _release sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844109.4032788
[0m00:08:29.403278 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: _release sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844109.4032788
[0m00:08:29.403278 [info ] [Thread-1 (]: 7 of 8 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.38s]
[0m00:08:29.403278 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:08:29.403278 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:08:29.403278 [info ] [Thread-1 (]: 8 of 8 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m00:08:29.403278 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: idle check connection: sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844109.4032788
[0m00:08:29.403278 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, now test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493)
[0m00:08:29.403278 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: reusing connection test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844109.4032788
[0m00:08:29.403278 [debug] [Thread-1 (]: Databricks adapter: On thread (23940, 3972): None using default compute resource.
[0m00:08:29.403278 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: _acquire sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844109.4032788
[0m00:08:29.403278 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:08:29.403278 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:08:29.403278 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (compile): 00:08:29.403278 => 00:08:29.403278
[0m00:08:29.403278 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:08:29.420746 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:08:29.421868 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: get_thread_connection: sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.01858997344970703s, acqrelcnt: 1, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844109.4032788
[0m00:08:29.422935 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: idle check connection: sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.01965618133544922s, acqrelcnt: 1, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844109.4032788
[0m00:08:29.422935 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:08:29.424013 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m00:08:29.759147 [debug] [Thread-1 (]: SQL status: OK in 0.33000001311302185 seconds
[0m00:08:29.774610 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (execute): 00:08:29.403278 => 00:08:29.774610
[0m00:08:29.774610 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: _release sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844109.7746098
[0m00:08:29.774610 [debug] [Thread-1 (]: Databricks adapter: conn: 1927554496592: _release sess: dac80b20-5451-4d9e-9715-6748183f8e09, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23940, 3972), cmpt: ``, lut: 1707844109.7746098
[0m00:08:29.774610 [info ] [Thread-1 (]: 8 of 8 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.37s]
[0m00:08:29.774610 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:08:29.774610 [debug] [MainThread]: Databricks adapter: conn: 1927552684944: idle check connection: sess: None, name: master, idle: 3.0387701988220215s, acqrelcnt: 0, lang: None, thrd: (23940, 15240), cmpt: ``, lut: 1707844106.7358396
[0m00:08:29.774610 [debug] [MainThread]: Databricks adapter: conn: 1927552684944: reusing connection master sess: None, name: master, idle: 3.0387701988220215s, acqrelcnt: 0, lang: None, thrd: (23940, 15240), cmpt: ``, lut: 1707844106.7358396
[0m00:08:29.774610 [debug] [MainThread]: Databricks adapter: Thread (23940, 15240) using default compute resource.
[0m00:08:29.774610 [debug] [MainThread]: Databricks adapter: conn: 1927552684944: _acquire sess: None, name: master, idle: 3.0387701988220215s, acqrelcnt: 1, lang: None, thrd: (23940, 15240), cmpt: ``, lut: 1707844106.7358396
[0m00:08:29.774610 [debug] [MainThread]: Databricks adapter: conn: 1927552684944: get_thread_connection: sess: None, name: master, idle: 3.0387701988220215s, acqrelcnt: 1, lang: None, thrd: (23940, 15240), cmpt: ``, lut: 1707844106.7358396
[0m00:08:29.774610 [debug] [MainThread]: Databricks adapter: conn: 1927552684944: idle check connection: sess: None, name: master, idle: 3.0387701988220215s, acqrelcnt: 1, lang: None, thrd: (23940, 15240), cmpt: ``, lut: 1707844106.7358396
[0m00:08:29.774610 [debug] [MainThread]: On master: ROLLBACK
[0m00:08:29.774610 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:08:30.033255 [debug] [MainThread]: Databricks adapter: conn: 1927552684944: session opened sess: e57e5134-70f5-4a61-9c36-ca99aca06873, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23940, 15240), cmpt: ``, lut: 1707844110.033255
[0m00:08:30.033255 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:08:30.033255 [debug] [MainThread]: Databricks adapter: conn: 1927552684944: get_thread_connection: sess: e57e5134-70f5-4a61-9c36-ca99aca06873, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23940, 15240), cmpt: ``, lut: 1707844110.033255
[0m00:08:30.033255 [debug] [MainThread]: Databricks adapter: conn: 1927552684944: idle check connection: sess: e57e5134-70f5-4a61-9c36-ca99aca06873, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23940, 15240), cmpt: ``, lut: 1707844110.033255
[0m00:08:30.033255 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:08:30.033255 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:08:30.033255 [debug] [MainThread]: Databricks adapter: conn: 1927552684944: _release sess: e57e5134-70f5-4a61-9c36-ca99aca06873, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (23940, 15240), cmpt: ``, lut: 1707844110.033255
[0m00:08:30.033255 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:08:30.033255 [debug] [MainThread]: On master: ROLLBACK
[0m00:08:30.033255 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:08:30.033255 [debug] [MainThread]: On master: Close
[0m00:08:30.127013 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m00:08:30.142570 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m00:08:30.142570 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:08:30.142570 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m00:08:30.236352 [debug] [MainThread]: Connection 'test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493' was properly closed.
[0m00:08:30.236352 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: ROLLBACK
[0m00:08:30.236352 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:08:30.236352 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m00:08:30.330067 [info ] [MainThread]: 
[0m00:08:30.330067 [info ] [MainThread]: Finished running 8 tests in 0 hours 0 minutes and 4.81 seconds (4.81s).
[0m00:08:30.330067 [debug] [MainThread]: Command end result
[0m00:08:30.350035 [info ] [MainThread]: 
[0m00:08:30.350035 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:08:30.351705 [info ] [MainThread]: 
[0m00:08:30.351705 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models\example\schema.yml)[0m
[0m00:08:30.351705 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m00:08:30.351705 [info ] [MainThread]: 
[0m00:08:30.351705 [info ] [MainThread]:   compiled Code at target\compiled\medallion_dbt_spark\models\example\schema.yml\not_null_my_first_dbt_model_id.sql
[0m00:08:30.356765 [info ] [MainThread]: 
[0m00:08:30.357774 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=1 SKIP=0 TOTAL=8
[0m00:08:30.360865 [debug] [MainThread]: Command `dbt test` failed at 00:08:30.359778 after 6.48 seconds
[0m00:08:30.360865 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C0B8DFE110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C0B8C2EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C0B8824550>]}
[0m00:08:30.361830 [debug] [MainThread]: Flushing usage events
[0m00:13:10.921136 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027F95083850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027F955C5E50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027F95083C90>]}


============================== 00:13:10.921136 | d9f14f78-6a1c-4302-bcce-beee15c5a7fe ==============================
[0m00:13:10.921136 [info ] [MainThread]: Running with dbt=1.7.7
[0m00:13:10.921136 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt test', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m00:13:12.146896 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd9f14f78-6a1c-4302-bcce-beee15c5a7fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027FA7A58A50>]}
[0m00:13:12.209390 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd9f14f78-6a1c-4302-bcce-beee15c5a7fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027F95976450>]}
[0m00:13:12.209390 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m00:13:12.209390 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m00:13:12.311735 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:13:12.313718 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://models\example\my_first_dbt_model.sql
[0m00:13:12.523102 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd9f14f78-6a1c-4302-bcce-beee15c5a7fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027FA7FF4A50>]}
[0m00:13:12.538705 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd9f14f78-6a1c-4302-bcce-beee15c5a7fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027FA8043990>]}
[0m00:13:12.538705 [info ] [MainThread]: Found 5 models, 7 snapshots, 8 tests, 9 sources, 0 exposures, 0 metrics, 535 macros, 0 groups, 0 semantic models
[0m00:13:12.538705 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd9f14f78-6a1c-4302-bcce-beee15c5a7fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027FA7D37390>]}
[0m00:13:12.538705 [info ] [MainThread]: 
[0m00:13:12.538705 [debug] [MainThread]: Databricks adapter: conn: 2747299546960: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (23652, 8684), cmpt: ``, lut: None
[0m00:13:12.538705 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:13:12.538705 [debug] [MainThread]: Databricks adapter: Thread (23652, 8684) using default compute resource.
[0m00:13:12.538705 [debug] [MainThread]: Databricks adapter: conn: 2747299546960: _acquire sess: None, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23652, 8684), cmpt: ``, lut: 1707844392.5387053
[0m00:13:12.538705 [debug] [ThreadPool]: Databricks adapter: conn: 2747302462608: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_saleslt, idle: 0s, acqrelcnt: 0, lang: None, thrd: (23652, 17356), cmpt: ``, lut: None
[0m00:13:12.538705 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt'
[0m00:13:12.538705 [debug] [ThreadPool]: Databricks adapter: Thread (23652, 17356) using default compute resource.
[0m00:13:12.538705 [debug] [ThreadPool]: Databricks adapter: conn: 2747302462608: _acquire sess: None, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23652, 17356), cmpt: ``, lut: 1707844392.5387053
[0m00:13:12.557351 [debug] [ThreadPool]: Databricks adapter: conn: 2747302462608: get_thread_connection: sess: None, name: list_hive_metastore_saleslt, idle: 0.018646240234375s, acqrelcnt: 1, lang: None, thrd: (23652, 17356), cmpt: ``, lut: 1707844392.5387053
[0m00:13:12.558358 [debug] [ThreadPool]: Databricks adapter: conn: 2747302462608: idle check connection: sess: None, name: list_hive_metastore_saleslt, idle: 0.0196530818939209s, acqrelcnt: 1, lang: None, thrd: (23652, 17356), cmpt: ``, lut: 1707844392.5387053
[0m00:13:12.559267 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:13:12.559750 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m00:13:12.559905 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:13:12.993935 [debug] [ThreadPool]: Databricks adapter: conn: 2747302462608: session opened sess: 23c0eba9-a1cd-49ff-8203-6673074e6cc5, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23652, 17356), cmpt: ``, lut: 1707844392.993935
[0m00:13:13.353764 [debug] [ThreadPool]: SQL status: OK in 0.7900000214576721 seconds
[0m00:13:13.449781 [debug] [ThreadPool]: Databricks adapter: conn: 2747302462608: get_thread_connection: sess: 23c0eba9-a1cd-49ff-8203-6673074e6cc5, name: list_hive_metastore_saleslt, idle: 0.45584630966186523s, acqrelcnt: 1, lang: None, thrd: (23652, 17356), cmpt: ``, lut: 1707844392.993935
[0m00:13:13.449781 [debug] [ThreadPool]: Databricks adapter: conn: 2747302462608: idle check connection: sess: 23c0eba9-a1cd-49ff-8203-6673074e6cc5, name: list_hive_metastore_saleslt, idle: 0.45584630966186523s, acqrelcnt: 1, lang: None, thrd: (23652, 17356), cmpt: ``, lut: 1707844392.993935
[0m00:13:13.449781 [debug] [ThreadPool]: Databricks adapter: conn: 2747302462608: get_thread_connection: sess: 23c0eba9-a1cd-49ff-8203-6673074e6cc5, name: list_hive_metastore_saleslt, idle: 0.45584630966186523s, acqrelcnt: 1, lang: None, thrd: (23652, 17356), cmpt: ``, lut: 1707844392.993935
[0m00:13:13.449781 [debug] [ThreadPool]: Databricks adapter: conn: 2747302462608: idle check connection: sess: 23c0eba9-a1cd-49ff-8203-6673074e6cc5, name: list_hive_metastore_saleslt, idle: 0.45584630966186523s, acqrelcnt: 1, lang: None, thrd: (23652, 17356), cmpt: ``, lut: 1707844392.993935
[0m00:13:13.449781 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:13:13.449781 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:13:13.449781 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m00:13:13.620552 [debug] [ThreadPool]: SQL status: OK in 0.17000000178813934 seconds
[0m00:13:13.620552 [debug] [ThreadPool]: Databricks adapter: conn: 2747302462608: get_thread_connection: sess: 23c0eba9-a1cd-49ff-8203-6673074e6cc5, name: list_hive_metastore_saleslt, idle: 0.6266179084777832s, acqrelcnt: 1, lang: None, thrd: (23652, 17356), cmpt: ``, lut: 1707844392.993935
[0m00:13:13.620552 [debug] [ThreadPool]: Databricks adapter: conn: 2747302462608: idle check connection: sess: 23c0eba9-a1cd-49ff-8203-6673074e6cc5, name: list_hive_metastore_saleslt, idle: 0.6266179084777832s, acqrelcnt: 1, lang: None, thrd: (23652, 17356), cmpt: ``, lut: 1707844392.993935
[0m00:13:13.620552 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:13:13.620552 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m00:13:13.808716 [debug] [ThreadPool]: SQL status: OK in 0.1899999976158142 seconds
[0m00:13:13.808716 [debug] [ThreadPool]: Databricks adapter: conn: 2747302462608: _release sess: 23c0eba9-a1cd-49ff-8203-6673074e6cc5, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (23652, 17356), cmpt: ``, lut: 1707844393.8087168
[0m00:13:13.808716 [debug] [ThreadPool]: Databricks adapter: conn: 2747302462608: idle check connection: sess: 23c0eba9-a1cd-49ff-8203-6673074e6cc5, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (23652, 17356), cmpt: ``, lut: 1707844393.8087168
[0m00:13:13.808716 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_snapshots)
[0m00:13:13.808716 [debug] [ThreadPool]: Databricks adapter: conn: 2747302462608: reusing connection list_hive_metastore_saleslt sess: 23c0eba9-a1cd-49ff-8203-6673074e6cc5, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (23652, 17356), cmpt: ``, lut: 1707844393.8087168
[0m00:13:13.808716 [debug] [ThreadPool]: Databricks adapter: Thread (23652, 17356) using default compute resource.
[0m00:13:13.808716 [debug] [ThreadPool]: Databricks adapter: conn: 2747302462608: _acquire sess: 23c0eba9-a1cd-49ff-8203-6673074e6cc5, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23652, 17356), cmpt: ``, lut: 1707844393.8087168
[0m00:13:13.824336 [debug] [ThreadPool]: Databricks adapter: conn: 2747302462608: get_thread_connection: sess: 23c0eba9-a1cd-49ff-8203-6673074e6cc5, name: list_hive_metastore_snapshots, idle: 0.015620231628417969s, acqrelcnt: 1, lang: None, thrd: (23652, 17356), cmpt: ``, lut: 1707844393.8087168
[0m00:13:13.824336 [debug] [ThreadPool]: Databricks adapter: conn: 2747302462608: idle check connection: sess: 23c0eba9-a1cd-49ff-8203-6673074e6cc5, name: list_hive_metastore_snapshots, idle: 0.015620231628417969s, acqrelcnt: 1, lang: None, thrd: (23652, 17356), cmpt: ``, lut: 1707844393.8087168
[0m00:13:13.824336 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:13:13.824336 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m00:13:14.108392 [debug] [ThreadPool]: SQL status: OK in 0.2800000011920929 seconds
[0m00:13:14.123918 [debug] [ThreadPool]: Databricks adapter: conn: 2747302462608: get_thread_connection: sess: 23c0eba9-a1cd-49ff-8203-6673074e6cc5, name: list_hive_metastore_snapshots, idle: 0.315201997756958s, acqrelcnt: 1, lang: None, thrd: (23652, 17356), cmpt: ``, lut: 1707844393.8087168
[0m00:13:14.123918 [debug] [ThreadPool]: Databricks adapter: conn: 2747302462608: idle check connection: sess: 23c0eba9-a1cd-49ff-8203-6673074e6cc5, name: list_hive_metastore_snapshots, idle: 0.315201997756958s, acqrelcnt: 1, lang: None, thrd: (23652, 17356), cmpt: ``, lut: 1707844393.8087168
[0m00:13:14.123918 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:13:14.123918 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m00:13:14.264622 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m00:13:14.264622 [debug] [ThreadPool]: Databricks adapter: conn: 2747302462608: get_thread_connection: sess: 23c0eba9-a1cd-49ff-8203-6673074e6cc5, name: list_hive_metastore_snapshots, idle: 0.4559059143066406s, acqrelcnt: 1, lang: None, thrd: (23652, 17356), cmpt: ``, lut: 1707844393.8087168
[0m00:13:14.280095 [debug] [ThreadPool]: Databricks adapter: conn: 2747302462608: idle check connection: sess: 23c0eba9-a1cd-49ff-8203-6673074e6cc5, name: list_hive_metastore_snapshots, idle: 0.47137904167175293s, acqrelcnt: 1, lang: None, thrd: (23652, 17356), cmpt: ``, lut: 1707844393.8087168
[0m00:13:14.280095 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:13:14.280095 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m00:13:14.423439 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m00:13:14.423439 [debug] [ThreadPool]: Databricks adapter: conn: 2747302462608: _release sess: 23c0eba9-a1cd-49ff-8203-6673074e6cc5, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (23652, 17356), cmpt: ``, lut: 1707844394.4234395
[0m00:13:14.423439 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd9f14f78-6a1c-4302-bcce-beee15c5a7fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027FA7ED9DD0>]}
[0m00:13:14.437147 [debug] [MainThread]: Databricks adapter: conn: 2747299546960: get_thread_connection: sess: None, name: master, idle: 1.8984425067901611s, acqrelcnt: 1, lang: None, thrd: (23652, 8684), cmpt: ``, lut: 1707844392.5387053
[0m00:13:14.438156 [debug] [MainThread]: Databricks adapter: conn: 2747299546960: idle check connection: sess: None, name: master, idle: 1.899451494216919s, acqrelcnt: 1, lang: None, thrd: (23652, 8684), cmpt: ``, lut: 1707844392.5387053
[0m00:13:14.438156 [debug] [MainThread]: Databricks adapter: conn: 2747299546960: get_thread_connection: sess: None, name: master, idle: 1.899451494216919s, acqrelcnt: 1, lang: None, thrd: (23652, 8684), cmpt: ``, lut: 1707844392.5387053
[0m00:13:14.439182 [debug] [MainThread]: Databricks adapter: conn: 2747299546960: idle check connection: sess: None, name: master, idle: 1.899451494216919s, acqrelcnt: 1, lang: None, thrd: (23652, 8684), cmpt: ``, lut: 1707844392.5387053
[0m00:13:14.439182 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:13:14.439182 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:13:14.440211 [debug] [MainThread]: Databricks adapter: conn: 2747299546960: _release sess: None, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (23652, 8684), cmpt: ``, lut: 1707844394.4402118
[0m00:13:14.440211 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:13:14.441234 [info ] [MainThread]: 
[0m00:13:14.446232 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:13:14.446232 [info ] [Thread-1 (]: 1 of 8 START test not_null_dim_product_product_name ............................ [RUN]
[0m00:13:14.448017 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: Creating DatabricksDBTConnection sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0s, acqrelcnt: 0, lang: None, thrd: (23652, 20688), cmpt: ``, lut: None
[0m00:13:14.448835 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5'
[0m00:13:14.448835 [debug] [Thread-1 (]: Databricks adapter: On thread (23652, 20688): None using default compute resource.
[0m00:13:14.448835 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: _acquire sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844394.4488351
[0m00:13:14.449860 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:13:14.468116 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:13:14.469036 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (compile): 00:13:14.449860 => 00:13:14.469036
[0m00:13:14.469997 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:13:14.490136 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:13:14.490136 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.04130125045776367s, acqrelcnt: 1, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844394.4488351
[0m00:13:14.490136 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.04130125045776367s, acqrelcnt: 1, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844394.4488351
[0m00:13:14.490136 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.04130125045776367s, acqrelcnt: 1, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844394.4488351
[0m00:13:14.490136 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.04130125045776367s, acqrelcnt: 1, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844394.4488351
[0m00:13:14.490136 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:13:14.490136 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:13:14.490136 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_name
from `hive_metastore`.`saleslt`.`dim_product`
where product_name is null



      
    ) dbt_internal_test
[0m00:13:14.490136 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:13:14.726143 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: session opened sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844394.7261436
[0m00:13:15.019212 [debug] [Thread-1 (]: SQL status: OK in 0.5299999713897705 seconds
[0m00:13:15.019212 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (execute): 00:13:14.471084 => 00:13:15.019212
[0m00:13:15.019212 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: _release sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844395.0192125
[0m00:13:15.019212 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: _release sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844395.0192125
[0m00:13:15.019212 [info ] [Thread-1 (]: 1 of 8 PASS not_null_dim_product_product_name .................................. [[32mPASS[0m in 0.57s]
[0m00:13:15.019212 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:13:15.019212 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:13:15.019212 [info ] [Thread-1 (]: 2 of 8 START test not_null_dim_product_product_sk .............................. [RUN]
[0m00:13:15.019212 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: idle check connection: sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844395.0192125
[0m00:13:15.019212 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, now test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417)
[0m00:13:15.019212 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844395.0192125
[0m00:13:15.019212 [debug] [Thread-1 (]: Databricks adapter: On thread (23652, 20688): None using default compute resource.
[0m00:13:15.019212 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: _acquire sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844395.0192125
[0m00:13:15.019212 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:13:15.019212 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:13:15.034856 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (compile): 00:13:15.019212 => 00:13:15.034856
[0m00:13:15.035742 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:13:15.039960 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:13:15.040826 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: get_thread_connection: sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.02161407470703125s, acqrelcnt: 1, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844395.0192125
[0m00:13:15.040826 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: idle check connection: sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.02161407470703125s, acqrelcnt: 1, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844395.0192125
[0m00:13:15.040826 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:13:15.040826 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_sk
from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is null



      
    ) dbt_internal_test
[0m00:13:15.295824 [debug] [Thread-1 (]: SQL status: OK in 0.25 seconds
[0m00:13:15.295824 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (execute): 00:13:15.035742 => 00:13:15.295824
[0m00:13:15.295824 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: _release sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844395.2958243
[0m00:13:15.295824 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: _release sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844395.2958243
[0m00:13:15.295824 [info ] [Thread-1 (]: 2 of 8 PASS not_null_dim_product_product_sk .................................... [[32mPASS[0m in 0.28s]
[0m00:13:15.295824 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:13:15.295824 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:13:15.295824 [info ] [Thread-1 (]: 3 of 8 START test not_null_dim_product_sellstartdate ........................... [RUN]
[0m00:13:15.295824 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: idle check connection: sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844395.2958243
[0m00:13:15.295824 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, now test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546)
[0m00:13:15.295824 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844395.2958243
[0m00:13:15.295824 [debug] [Thread-1 (]: Databricks adapter: On thread (23652, 20688): None using default compute resource.
[0m00:13:15.295824 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: _acquire sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844395.2958243
[0m00:13:15.295824 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:13:15.315327 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:13:15.317051 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (compile): 00:13:15.295824 => 00:13:15.316343
[0m00:13:15.317165 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:13:15.317165 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:13:15.321517 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: get_thread_connection: sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.025693416595458984s, acqrelcnt: 1, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844395.2958243
[0m00:13:15.322528 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: idle check connection: sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.026704072952270508s, acqrelcnt: 1, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844395.2958243
[0m00:13:15.322528 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:13:15.323527 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select sellstartdate
from `hive_metastore`.`saleslt`.`dim_product`
where sellstartdate is null



      
    ) dbt_internal_test
[0m00:13:15.551981 [debug] [Thread-1 (]: SQL status: OK in 0.23000000417232513 seconds
[0m00:13:15.567606 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (execute): 00:13:15.317165 => 00:13:15.567606
[0m00:13:15.567606 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: _release sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844395.5676067
[0m00:13:15.567606 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: _release sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844395.5676067
[0m00:13:15.567606 [info ] [Thread-1 (]: 3 of 8 PASS not_null_dim_product_sellstartdate ................................. [[32mPASS[0m in 0.27s]
[0m00:13:15.567606 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:13:15.567606 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:13:15.567606 [info ] [Thread-1 (]: 4 of 8 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m00:13:15.567606 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: idle check connection: sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844395.5676067
[0m00:13:15.567606 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, now test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710)
[0m00:13:15.567606 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: reusing connection test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844395.5676067
[0m00:13:15.567606 [debug] [Thread-1 (]: Databricks adapter: On thread (23652, 20688): None using default compute resource.
[0m00:13:15.577813 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: _acquire sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.010207414627075195s, acqrelcnt: 1, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844395.5676067
[0m00:13:15.578483 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:13:15.586312 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:13:15.588335 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 00:13:15.579151 => 00:13:15.588203
[0m00:13:15.588335 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:13:15.591646 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:13:15.591646 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: get_thread_connection: sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.024039506912231445s, acqrelcnt: 1, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844395.5676067
[0m00:13:15.591646 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: idle check connection: sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.024039506912231445s, acqrelcnt: 1, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844395.5676067
[0m00:13:15.591646 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:13:15.591646 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m00:13:15.894167 [debug] [Thread-1 (]: SQL status: OK in 0.30000001192092896 seconds
[0m00:13:15.894167 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 00:13:15.589676 => 00:13:15.894167
[0m00:13:15.894167 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: _release sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844395.8941672
[0m00:13:15.894167 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: _release sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844395.8941672
[0m00:13:15.894167 [error] [Thread-1 (]: 4 of 8 FAIL 1 not_null_my_first_dbt_model_id ................................... [[31mFAIL 1[0m in 0.33s]
[0m00:13:15.894167 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:13:15.894167 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:13:15.894167 [info ] [Thread-1 (]: 5 of 8 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m00:13:15.909682 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: idle check connection: sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.015515327453613281s, acqrelcnt: 0, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844395.8941672
[0m00:13:15.909682 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, now test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778)
[0m00:13:15.909682 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: reusing connection test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.015515327453613281s, acqrelcnt: 0, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844395.8941672
[0m00:13:15.909682 [debug] [Thread-1 (]: Databricks adapter: On thread (23652, 20688): None using default compute resource.
[0m00:13:15.909682 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: _acquire sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.015515327453613281s, acqrelcnt: 1, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844395.8941672
[0m00:13:15.909682 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:13:15.909682 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:13:15.909682 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (compile): 00:13:15.909682 => 00:13:15.909682
[0m00:13:15.909682 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:13:15.909682 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:13:15.909682 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: get_thread_connection: sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.015515327453613281s, acqrelcnt: 1, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844395.8941672
[0m00:13:15.909682 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: idle check connection: sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.015515327453613281s, acqrelcnt: 1, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844395.8941672
[0m00:13:15.909682 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:13:15.909682 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m00:13:16.195977 [debug] [Thread-1 (]: SQL status: OK in 0.28999999165534973 seconds
[0m00:13:16.195977 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (execute): 00:13:15.909682 => 00:13:16.195977
[0m00:13:16.195977 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: _release sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844396.1959772
[0m00:13:16.195977 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: _release sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844396.1959772
[0m00:13:16.195977 [info ] [Thread-1 (]: 5 of 8 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.30s]
[0m00:13:16.211439 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:13:16.211439 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:13:16.211439 [info ] [Thread-1 (]: 6 of 8 START test unique_dim_product_product_sk ................................ [RUN]
[0m00:13:16.211439 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: idle check connection: sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.015462398529052734s, acqrelcnt: 0, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844396.1959772
[0m00:13:16.211439 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, now test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c)
[0m00:13:16.211439 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: reusing connection test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.015462398529052734s, acqrelcnt: 0, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844396.1959772
[0m00:13:16.211439 [debug] [Thread-1 (]: Databricks adapter: On thread (23652, 20688): None using default compute resource.
[0m00:13:16.211439 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: _acquire sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.015462398529052734s, acqrelcnt: 1, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844396.1959772
[0m00:13:16.211439 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:13:16.211439 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:13:16.211439 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (compile): 00:13:16.211439 => 00:13:16.211439
[0m00:13:16.211439 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:13:16.230090 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:13:16.231090 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: get_thread_connection: sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.03511333465576172s, acqrelcnt: 1, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844396.1959772
[0m00:13:16.231090 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: idle check connection: sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.03511333465576172s, acqrelcnt: 1, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844396.1959772
[0m00:13:16.232274 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:13:16.232685 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    product_sk as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is not null
group by product_sk
having count(*) > 1



      
    ) dbt_internal_test
[0m00:13:16.604323 [debug] [Thread-1 (]: SQL status: OK in 0.36000001430511475 seconds
[0m00:13:16.604323 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (execute): 00:13:16.211439 => 00:13:16.604323
[0m00:13:16.604323 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: _release sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844396.6043236
[0m00:13:16.604323 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: _release sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844396.6043236
[0m00:13:16.604323 [info ] [Thread-1 (]: 6 of 8 PASS unique_dim_product_product_sk ...................................... [[32mPASS[0m in 0.39s]
[0m00:13:16.604323 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:13:16.604323 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:13:16.604323 [info ] [Thread-1 (]: 7 of 8 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m00:13:16.604323 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: idle check connection: sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844396.6043236
[0m00:13:16.604323 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, now test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321)
[0m00:13:16.604323 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: reusing connection test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844396.6043236
[0m00:13:16.604323 [debug] [Thread-1 (]: Databricks adapter: On thread (23652, 20688): None using default compute resource.
[0m00:13:16.604323 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: _acquire sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844396.6043236
[0m00:13:16.604323 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:13:16.604323 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:13:16.619937 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (compile): 00:13:16.604323 => 00:13:16.619937
[0m00:13:16.621224 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:13:16.625267 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:13:16.626172 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: get_thread_connection: sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.02184915542602539s, acqrelcnt: 1, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844396.6043236
[0m00:13:16.626172 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: idle check connection: sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.02184915542602539s, acqrelcnt: 1, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844396.6043236
[0m00:13:16.627184 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:13:16.627184 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m00:13:16.960040 [debug] [Thread-1 (]: SQL status: OK in 0.33000001311302185 seconds
[0m00:13:16.960040 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (execute): 00:13:16.621224 => 00:13:16.960040
[0m00:13:16.975505 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: _release sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844396.9755056
[0m00:13:16.975505 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: _release sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844396.9755056
[0m00:13:16.975505 [info ] [Thread-1 (]: 7 of 8 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.37s]
[0m00:13:16.975505 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:13:16.975505 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:13:16.975505 [info ] [Thread-1 (]: 8 of 8 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m00:13:16.975505 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: idle check connection: sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844396.9755056
[0m00:13:16.975505 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, now test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493)
[0m00:13:16.975505 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: reusing connection test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844396.9755056
[0m00:13:16.975505 [debug] [Thread-1 (]: Databricks adapter: On thread (23652, 20688): None using default compute resource.
[0m00:13:16.975505 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: _acquire sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844396.9755056
[0m00:13:16.975505 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:13:16.975505 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:13:16.975505 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (compile): 00:13:16.975505 => 00:13:16.975505
[0m00:13:16.975505 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:13:16.975505 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:13:16.991144 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: get_thread_connection: sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.01563858985900879s, acqrelcnt: 1, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844396.9755056
[0m00:13:16.992121 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: idle check connection: sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.01563858985900879s, acqrelcnt: 1, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844396.9755056
[0m00:13:16.992121 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:13:16.993045 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m00:13:17.340516 [debug] [Thread-1 (]: SQL status: OK in 0.3499999940395355 seconds
[0m00:13:17.340516 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (execute): 00:13:16.975505 => 00:13:17.340516
[0m00:13:17.356026 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: _release sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844397.356027
[0m00:13:17.356026 [debug] [Thread-1 (]: Databricks adapter: conn: 2747298767376: _release sess: 9962c650-9b54-4970-96de-b1859217d2b1, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (23652, 20688), cmpt: ``, lut: 1707844397.356027
[0m00:13:17.356026 [info ] [Thread-1 (]: 8 of 8 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.38s]
[0m00:13:17.356026 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:13:17.356026 [debug] [MainThread]: Databricks adapter: conn: 2747299546960: idle check connection: sess: None, name: master, idle: 2.9158151149749756s, acqrelcnt: 0, lang: None, thrd: (23652, 8684), cmpt: ``, lut: 1707844394.4402118
[0m00:13:17.356026 [debug] [MainThread]: Databricks adapter: conn: 2747299546960: reusing connection master sess: None, name: master, idle: 2.9158151149749756s, acqrelcnt: 0, lang: None, thrd: (23652, 8684), cmpt: ``, lut: 1707844394.4402118
[0m00:13:17.356026 [debug] [MainThread]: Databricks adapter: Thread (23652, 8684) using default compute resource.
[0m00:13:17.356026 [debug] [MainThread]: Databricks adapter: conn: 2747299546960: _acquire sess: None, name: master, idle: 2.9158151149749756s, acqrelcnt: 1, lang: None, thrd: (23652, 8684), cmpt: ``, lut: 1707844394.4402118
[0m00:13:17.356026 [debug] [MainThread]: Databricks adapter: conn: 2747299546960: get_thread_connection: sess: None, name: master, idle: 2.9158151149749756s, acqrelcnt: 1, lang: None, thrd: (23652, 8684), cmpt: ``, lut: 1707844394.4402118
[0m00:13:17.356026 [debug] [MainThread]: Databricks adapter: conn: 2747299546960: idle check connection: sess: None, name: master, idle: 2.9158151149749756s, acqrelcnt: 1, lang: None, thrd: (23652, 8684), cmpt: ``, lut: 1707844394.4402118
[0m00:13:17.356026 [debug] [MainThread]: On master: ROLLBACK
[0m00:13:17.356026 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:13:17.569853 [debug] [MainThread]: Databricks adapter: conn: 2747299546960: session opened sess: 6a8743b5-caf3-49e4-8cc2-cfe0e07c5668, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23652, 8684), cmpt: ``, lut: 1707844397.5698538
[0m00:13:17.569853 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:13:17.569853 [debug] [MainThread]: Databricks adapter: conn: 2747299546960: get_thread_connection: sess: 6a8743b5-caf3-49e4-8cc2-cfe0e07c5668, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23652, 8684), cmpt: ``, lut: 1707844397.5698538
[0m00:13:17.569853 [debug] [MainThread]: Databricks adapter: conn: 2747299546960: idle check connection: sess: 6a8743b5-caf3-49e4-8cc2-cfe0e07c5668, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (23652, 8684), cmpt: ``, lut: 1707844397.5698538
[0m00:13:17.569853 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:13:17.569853 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:13:17.569853 [debug] [MainThread]: Databricks adapter: conn: 2747299546960: _release sess: 6a8743b5-caf3-49e4-8cc2-cfe0e07c5668, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (23652, 8684), cmpt: ``, lut: 1707844397.5698538
[0m00:13:17.569853 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:13:17.569853 [debug] [MainThread]: On master: ROLLBACK
[0m00:13:17.569853 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:13:17.569853 [debug] [MainThread]: On master: Close
[0m00:13:17.670917 [debug] [MainThread]: Connection 'list_hive_metastore_snapshots' was properly closed.
[0m00:13:17.670917 [debug] [MainThread]: On list_hive_metastore_snapshots: ROLLBACK
[0m00:13:17.670917 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:13:17.670917 [debug] [MainThread]: On list_hive_metastore_snapshots: Close
[0m00:13:17.749024 [debug] [MainThread]: Connection 'test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493' was properly closed.
[0m00:13:17.749024 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: ROLLBACK
[0m00:13:17.749024 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:13:17.749024 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m00:13:17.827182 [info ] [MainThread]: 
[0m00:13:17.827182 [info ] [MainThread]: Finished running 8 tests in 0 hours 0 minutes and 5.29 seconds (5.29s).
[0m00:13:17.834792 [debug] [MainThread]: Command end result
[0m00:13:17.850415 [info ] [MainThread]: 
[0m00:13:17.850815 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:13:17.852322 [info ] [MainThread]: 
[0m00:13:17.853332 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models\example\schema.yml)[0m
[0m00:13:17.854377 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m00:13:17.856312 [info ] [MainThread]: 
[0m00:13:17.857622 [info ] [MainThread]:   compiled Code at target\compiled\medallion_dbt_spark\models\example\schema.yml\not_null_my_first_dbt_model_id.sql
[0m00:13:17.858982 [info ] [MainThread]: 
[0m00:13:17.859985 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=1 SKIP=0 TOTAL=8
[0m00:13:17.861985 [debug] [MainThread]: Command `dbt test` failed at 00:13:17.861985 after 6.97 seconds
[0m00:13:17.862986 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027F956CC250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027F955A5290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027F8E456E50>]}
[0m00:13:17.863983 [debug] [MainThread]: Flushing usage events
[0m00:13:48.134946 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D589125B50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D589466550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D589087F90>]}


============================== 00:13:48.134946 | e178c319-cc55-4e76-bdd3-0a7f7299e44d ==============================
[0m00:13:48.134946 [info ] [MainThread]: Running with dbt=1.7.7
[0m00:13:48.134946 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt test', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m00:13:49.280949 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e178c319-cc55-4e76-bdd3-0a7f7299e44d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D59B736390>]}
[0m00:13:49.365906 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e178c319-cc55-4e76-bdd3-0a7f7299e44d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D59B7E1C50>]}
[0m00:13:49.365906 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m00:13:49.384823 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m00:13:49.459615 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m00:13:49.459615 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m00:13:49.476040 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e178c319-cc55-4e76-bdd3-0a7f7299e44d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D59B9F5E50>]}
[0m00:13:49.497270 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e178c319-cc55-4e76-bdd3-0a7f7299e44d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D59B9CB950>]}
[0m00:13:49.498270 [info ] [MainThread]: Found 5 models, 7 snapshots, 8 tests, 9 sources, 0 exposures, 0 metrics, 535 macros, 0 groups, 0 semantic models
[0m00:13:49.498270 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e178c319-cc55-4e76-bdd3-0a7f7299e44d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D59B86F410>]}
[0m00:13:49.500353 [info ] [MainThread]: 
[0m00:13:49.502272 [debug] [MainThread]: Databricks adapter: conn: 2016950400848: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (21296, 24268), cmpt: ``, lut: None
[0m00:13:49.502272 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:13:49.502272 [debug] [MainThread]: Databricks adapter: Thread (21296, 24268) using default compute resource.
[0m00:13:49.503271 [debug] [MainThread]: Databricks adapter: conn: 2016950400848: _acquire sess: None, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (21296, 24268), cmpt: ``, lut: 1707844429.5032716
[0m00:13:49.504759 [debug] [ThreadPool]: Databricks adapter: conn: 2016950389136: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_snapshots, idle: 0s, acqrelcnt: 0, lang: None, thrd: (21296, 14348), cmpt: ``, lut: None
[0m00:13:49.505769 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_snapshots'
[0m00:13:49.505769 [debug] [ThreadPool]: Databricks adapter: Thread (21296, 14348) using default compute resource.
[0m00:13:49.505769 [debug] [ThreadPool]: Databricks adapter: conn: 2016950389136: _acquire sess: None, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (21296, 14348), cmpt: ``, lut: 1707844429.5057695
[0m00:13:49.511075 [debug] [ThreadPool]: Databricks adapter: conn: 2016950389136: get_thread_connection: sess: None, name: list_hive_metastore_snapshots, idle: 0.00530552864074707s, acqrelcnt: 1, lang: None, thrd: (21296, 14348), cmpt: ``, lut: 1707844429.5057695
[0m00:13:49.511980 [debug] [ThreadPool]: Databricks adapter: conn: 2016950389136: idle check connection: sess: None, name: list_hive_metastore_snapshots, idle: 0.00530552864074707s, acqrelcnt: 1, lang: None, thrd: (21296, 14348), cmpt: ``, lut: 1707844429.5057695
[0m00:13:49.511980 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:13:49.511980 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m00:13:49.512976 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:13:49.756297 [debug] [ThreadPool]: Databricks adapter: conn: 2016950389136: session opened sess: 65d17738-aa83-40dc-a8f6-92bd7527cd1f, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (21296, 14348), cmpt: ``, lut: 1707844429.7562976
[0m00:13:49.865674 [debug] [ThreadPool]: SQL status: OK in 0.3499999940395355 seconds
[0m00:13:49.881170 [debug] [ThreadPool]: Databricks adapter: conn: 2016950389136: get_thread_connection: sess: 65d17738-aa83-40dc-a8f6-92bd7527cd1f, name: list_hive_metastore_snapshots, idle: 0.12487268447875977s, acqrelcnt: 1, lang: None, thrd: (21296, 14348), cmpt: ``, lut: 1707844429.7562976
[0m00:13:49.881170 [debug] [ThreadPool]: Databricks adapter: conn: 2016950389136: idle check connection: sess: 65d17738-aa83-40dc-a8f6-92bd7527cd1f, name: list_hive_metastore_snapshots, idle: 0.12487268447875977s, acqrelcnt: 1, lang: None, thrd: (21296, 14348), cmpt: ``, lut: 1707844429.7562976
[0m00:13:49.881170 [debug] [ThreadPool]: Databricks adapter: conn: 2016950389136: get_thread_connection: sess: 65d17738-aa83-40dc-a8f6-92bd7527cd1f, name: list_hive_metastore_snapshots, idle: 0.12487268447875977s, acqrelcnt: 1, lang: None, thrd: (21296, 14348), cmpt: ``, lut: 1707844429.7562976
[0m00:13:49.881170 [debug] [ThreadPool]: Databricks adapter: conn: 2016950389136: idle check connection: sess: 65d17738-aa83-40dc-a8f6-92bd7527cd1f, name: list_hive_metastore_snapshots, idle: 0.12487268447875977s, acqrelcnt: 1, lang: None, thrd: (21296, 14348), cmpt: ``, lut: 1707844429.7562976
[0m00:13:49.881170 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:13:49.881170 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:13:49.881170 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m00:13:50.045071 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m00:13:50.060632 [debug] [ThreadPool]: Databricks adapter: conn: 2016950389136: get_thread_connection: sess: 65d17738-aa83-40dc-a8f6-92bd7527cd1f, name: list_hive_metastore_snapshots, idle: 0.3043346405029297s, acqrelcnt: 1, lang: None, thrd: (21296, 14348), cmpt: ``, lut: 1707844429.7562976
[0m00:13:50.060632 [debug] [ThreadPool]: Databricks adapter: conn: 2016950389136: idle check connection: sess: 65d17738-aa83-40dc-a8f6-92bd7527cd1f, name: list_hive_metastore_snapshots, idle: 0.3043346405029297s, acqrelcnt: 1, lang: None, thrd: (21296, 14348), cmpt: ``, lut: 1707844429.7562976
[0m00:13:50.060632 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:13:50.060632 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m00:13:50.201613 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m00:13:50.201613 [debug] [ThreadPool]: Databricks adapter: conn: 2016950389136: _release sess: 65d17738-aa83-40dc-a8f6-92bd7527cd1f, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (21296, 14348), cmpt: ``, lut: 1707844430.2016134
[0m00:13:50.201613 [debug] [ThreadPool]: Databricks adapter: conn: 2016950389136: idle check connection: sess: 65d17738-aa83-40dc-a8f6-92bd7527cd1f, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (21296, 14348), cmpt: ``, lut: 1707844430.2016134
[0m00:13:50.201613 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now list_hive_metastore_saleslt)
[0m00:13:50.201613 [debug] [ThreadPool]: Databricks adapter: conn: 2016950389136: reusing connection list_hive_metastore_snapshots sess: 65d17738-aa83-40dc-a8f6-92bd7527cd1f, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (21296, 14348), cmpt: ``, lut: 1707844430.2016134
[0m00:13:50.201613 [debug] [ThreadPool]: Databricks adapter: Thread (21296, 14348) using default compute resource.
[0m00:13:50.201613 [debug] [ThreadPool]: Databricks adapter: conn: 2016950389136: _acquire sess: 65d17738-aa83-40dc-a8f6-92bd7527cd1f, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (21296, 14348), cmpt: ``, lut: 1707844430.2016134
[0m00:13:50.201613 [debug] [ThreadPool]: Databricks adapter: conn: 2016950389136: get_thread_connection: sess: 65d17738-aa83-40dc-a8f6-92bd7527cd1f, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (21296, 14348), cmpt: ``, lut: 1707844430.2016134
[0m00:13:50.201613 [debug] [ThreadPool]: Databricks adapter: conn: 2016950389136: idle check connection: sess: 65d17738-aa83-40dc-a8f6-92bd7527cd1f, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (21296, 14348), cmpt: ``, lut: 1707844430.2016134
[0m00:13:50.201613 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:13:50.217172 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m00:13:50.326542 [debug] [ThreadPool]: SQL status: OK in 0.10999999940395355 seconds
[0m00:13:50.326542 [debug] [ThreadPool]: Databricks adapter: conn: 2016950389136: get_thread_connection: sess: 65d17738-aa83-40dc-a8f6-92bd7527cd1f, name: list_hive_metastore_saleslt, idle: 0.12492871284484863s, acqrelcnt: 1, lang: None, thrd: (21296, 14348), cmpt: ``, lut: 1707844430.2016134
[0m00:13:50.326542 [debug] [ThreadPool]: Databricks adapter: conn: 2016950389136: idle check connection: sess: 65d17738-aa83-40dc-a8f6-92bd7527cd1f, name: list_hive_metastore_saleslt, idle: 0.12492871284484863s, acqrelcnt: 1, lang: None, thrd: (21296, 14348), cmpt: ``, lut: 1707844430.2016134
[0m00:13:50.326542 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:13:50.326542 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m00:13:50.460537 [debug] [ThreadPool]: SQL status: OK in 0.12999999523162842 seconds
[0m00:13:50.476158 [debug] [ThreadPool]: Databricks adapter: conn: 2016950389136: get_thread_connection: sess: 65d17738-aa83-40dc-a8f6-92bd7527cd1f, name: list_hive_metastore_saleslt, idle: 0.27454519271850586s, acqrelcnt: 1, lang: None, thrd: (21296, 14348), cmpt: ``, lut: 1707844430.2016134
[0m00:13:50.476158 [debug] [ThreadPool]: Databricks adapter: conn: 2016950389136: idle check connection: sess: 65d17738-aa83-40dc-a8f6-92bd7527cd1f, name: list_hive_metastore_saleslt, idle: 0.27454519271850586s, acqrelcnt: 1, lang: None, thrd: (21296, 14348), cmpt: ``, lut: 1707844430.2016134
[0m00:13:50.476158 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:13:50.476158 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m00:13:50.616944 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m00:13:50.616944 [debug] [ThreadPool]: Databricks adapter: conn: 2016950389136: _release sess: 65d17738-aa83-40dc-a8f6-92bd7527cd1f, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (21296, 14348), cmpt: ``, lut: 1707844430.616944
[0m00:13:50.616944 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e178c319-cc55-4e76-bdd3-0a7f7299e44d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D59B945550>]}
[0m00:13:50.616944 [debug] [MainThread]: Databricks adapter: conn: 2016950400848: get_thread_connection: sess: None, name: master, idle: 1.1136724948883057s, acqrelcnt: 1, lang: None, thrd: (21296, 24268), cmpt: ``, lut: 1707844429.5032716
[0m00:13:50.616944 [debug] [MainThread]: Databricks adapter: conn: 2016950400848: idle check connection: sess: None, name: master, idle: 1.1136724948883057s, acqrelcnt: 1, lang: None, thrd: (21296, 24268), cmpt: ``, lut: 1707844429.5032716
[0m00:13:50.616944 [debug] [MainThread]: Databricks adapter: conn: 2016950400848: get_thread_connection: sess: None, name: master, idle: 1.1136724948883057s, acqrelcnt: 1, lang: None, thrd: (21296, 24268), cmpt: ``, lut: 1707844429.5032716
[0m00:13:50.632501 [debug] [MainThread]: Databricks adapter: conn: 2016950400848: idle check connection: sess: None, name: master, idle: 1.1136724948883057s, acqrelcnt: 1, lang: None, thrd: (21296, 24268), cmpt: ``, lut: 1707844429.5032716
[0m00:13:50.632501 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:13:50.632501 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:13:50.632501 [debug] [MainThread]: Databricks adapter: conn: 2016950400848: _release sess: None, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (21296, 24268), cmpt: ``, lut: 1707844430.6325018
[0m00:13:50.632501 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:13:50.632501 [info ] [MainThread]: 
[0m00:13:50.632501 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:13:50.632501 [info ] [Thread-1 (]: 1 of 8 START test not_null_dim_product_product_name ............................ [RUN]
[0m00:13:50.641245 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: Creating DatabricksDBTConnection sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0s, acqrelcnt: 0, lang: None, thrd: (21296, 17252), cmpt: ``, lut: None
[0m00:13:50.641245 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5'
[0m00:13:50.642687 [debug] [Thread-1 (]: Databricks adapter: On thread (21296, 17252): None using default compute resource.
[0m00:13:50.642687 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: _acquire sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844430.6426876
[0m00:13:50.643844 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:13:50.660830 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:13:50.660830 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (compile): 00:13:50.643844 => 00:13:50.660830
[0m00:13:50.660830 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:13:50.677735 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:13:50.677735 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.035047292709350586s, acqrelcnt: 1, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844430.6426876
[0m00:13:50.682245 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.03955817222595215s, acqrelcnt: 1, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844430.6426876
[0m00:13:50.682245 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.03955817222595215s, acqrelcnt: 1, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844430.6426876
[0m00:13:50.682245 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.03955817222595215s, acqrelcnt: 1, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844430.6426876
[0m00:13:50.682245 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:13:50.682245 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:13:50.682245 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_name
from `hive_metastore`.`saleslt`.`dim_product`
where product_name is null



      
    ) dbt_internal_test
[0m00:13:50.682245 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:13:50.901830 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: session opened sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844430.901831
[0m00:13:51.159018 [debug] [Thread-1 (]: SQL status: OK in 0.47999998927116394 seconds
[0m00:13:51.174489 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (execute): 00:13:50.660830 => 00:13:51.174489
[0m00:13:51.174489 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: _release sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844431.174489
[0m00:13:51.174489 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: _release sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844431.174489
[0m00:13:51.174489 [info ] [Thread-1 (]: 1 of 8 PASS not_null_dim_product_product_name .................................. [[32mPASS[0m in 0.54s]
[0m00:13:51.174489 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:13:51.174489 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:13:51.174489 [info ] [Thread-1 (]: 2 of 8 START test not_null_dim_product_product_sk .............................. [RUN]
[0m00:13:51.174489 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: idle check connection: sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844431.174489
[0m00:13:51.174489 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, now test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417)
[0m00:13:51.174489 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844431.174489
[0m00:13:51.174489 [debug] [Thread-1 (]: Databricks adapter: On thread (21296, 17252): None using default compute resource.
[0m00:13:51.174489 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: _acquire sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844431.174489
[0m00:13:51.174489 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:13:51.174489 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:13:51.174489 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (compile): 00:13:51.174489 => 00:13:51.174489
[0m00:13:51.190106 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:13:51.192949 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:13:51.194728 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: get_thread_connection: sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.019829988479614258s, acqrelcnt: 1, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844431.174489
[0m00:13:51.194728 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: idle check connection: sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.020239591598510742s, acqrelcnt: 1, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844431.174489
[0m00:13:51.194728 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:13:51.194728 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_sk
from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is null



      
    ) dbt_internal_test
[0m00:13:51.423019 [debug] [Thread-1 (]: SQL status: OK in 0.23000000417232513 seconds
[0m00:13:51.423019 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (execute): 00:13:51.190947 => 00:13:51.423019
[0m00:13:51.423019 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: _release sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844431.42302
[0m00:13:51.423019 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: _release sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844431.42302
[0m00:13:51.423019 [info ] [Thread-1 (]: 2 of 8 PASS not_null_dim_product_product_sk .................................... [[32mPASS[0m in 0.25s]
[0m00:13:51.423019 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:13:51.423019 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:13:51.423019 [info ] [Thread-1 (]: 3 of 8 START test not_null_dim_product_sellstartdate ........................... [RUN]
[0m00:13:51.423019 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: idle check connection: sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844431.42302
[0m00:13:51.423019 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, now test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546)
[0m00:13:51.438543 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.015523672103881836s, acqrelcnt: 0, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844431.42302
[0m00:13:51.438543 [debug] [Thread-1 (]: Databricks adapter: On thread (21296, 17252): None using default compute resource.
[0m00:13:51.438543 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: _acquire sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.015523672103881836s, acqrelcnt: 1, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844431.42302
[0m00:13:51.438543 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:13:51.438543 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:13:51.438543 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (compile): 00:13:51.438543 => 00:13:51.438543
[0m00:13:51.438543 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:13:51.438543 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:13:51.438543 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: get_thread_connection: sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.015523672103881836s, acqrelcnt: 1, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844431.42302
[0m00:13:51.438543 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: idle check connection: sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.015523672103881836s, acqrelcnt: 1, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844431.42302
[0m00:13:51.438543 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:13:51.438543 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select sellstartdate
from `hive_metastore`.`saleslt`.`dim_product`
where sellstartdate is null



      
    ) dbt_internal_test
[0m00:13:51.670578 [debug] [Thread-1 (]: SQL status: OK in 0.23000000417232513 seconds
[0m00:13:51.670578 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (execute): 00:13:51.438543 => 00:13:51.670578
[0m00:13:51.670578 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: _release sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844431.6705782
[0m00:13:51.670578 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: _release sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844431.6705782
[0m00:13:51.686069 [info ] [Thread-1 (]: 3 of 8 PASS not_null_dim_product_sellstartdate ................................. [[32mPASS[0m in 0.25s]
[0m00:13:51.686069 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:13:51.686069 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:13:51.686069 [info ] [Thread-1 (]: 4 of 8 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m00:13:51.686069 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: idle check connection: sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.01549077033996582s, acqrelcnt: 0, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844431.6705782
[0m00:13:51.686069 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, now test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710)
[0m00:13:51.686069 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: reusing connection test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.01549077033996582s, acqrelcnt: 0, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844431.6705782
[0m00:13:51.686069 [debug] [Thread-1 (]: Databricks adapter: On thread (21296, 17252): None using default compute resource.
[0m00:13:51.686069 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: _acquire sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.01549077033996582s, acqrelcnt: 1, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844431.6705782
[0m00:13:51.686069 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:13:51.686069 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:13:51.686069 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 00:13:51.686069 => 00:13:51.686069
[0m00:13:51.686069 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:13:51.686069 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:13:51.686069 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: get_thread_connection: sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.01549077033996582s, acqrelcnt: 1, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844431.6705782
[0m00:13:51.686069 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: idle check connection: sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.01549077033996582s, acqrelcnt: 1, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844431.6705782
[0m00:13:51.686069 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:13:51.701694 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m00:13:51.977904 [debug] [Thread-1 (]: SQL status: OK in 0.2800000011920929 seconds
[0m00:13:51.977904 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 00:13:51.686069 => 00:13:51.977904
[0m00:13:51.993397 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: _release sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844431.9933975
[0m00:13:51.993397 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: _release sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844431.9933975
[0m00:13:51.993397 [error] [Thread-1 (]: 4 of 8 FAIL 1 not_null_my_first_dbt_model_id ................................... [[31mFAIL 1[0m in 0.31s]
[0m00:13:51.993397 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:13:51.993397 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:13:51.993397 [info ] [Thread-1 (]: 5 of 8 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m00:13:51.993397 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: idle check connection: sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844431.9933975
[0m00:13:51.993397 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, now test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778)
[0m00:13:51.993397 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: reusing connection test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844431.9933975
[0m00:13:51.993397 [debug] [Thread-1 (]: Databricks adapter: On thread (21296, 17252): None using default compute resource.
[0m00:13:51.993397 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: _acquire sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844431.9933975
[0m00:13:51.993397 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:13:51.993397 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:13:51.993397 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (compile): 00:13:51.993397 => 00:13:51.993397
[0m00:13:51.993397 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:13:52.011080 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:13:52.011717 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: get_thread_connection: sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.018320322036743164s, acqrelcnt: 1, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844431.9933975
[0m00:13:52.012925 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: idle check connection: sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.01952815055847168s, acqrelcnt: 1, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844431.9933975
[0m00:13:52.012925 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:13:52.013929 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m00:13:52.258918 [debug] [Thread-1 (]: SQL status: OK in 0.23999999463558197 seconds
[0m00:13:52.258918 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (execute): 00:13:51.993397 => 00:13:52.258918
[0m00:13:52.258918 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: _release sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844432.2589183
[0m00:13:52.258918 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: _release sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844432.2589183
[0m00:13:52.258918 [info ] [Thread-1 (]: 5 of 8 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.27s]
[0m00:13:52.258918 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:13:52.258918 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:13:52.258918 [info ] [Thread-1 (]: 6 of 8 START test unique_dim_product_product_sk ................................ [RUN]
[0m00:13:52.258918 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: idle check connection: sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844432.2589183
[0m00:13:52.258918 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, now test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c)
[0m00:13:52.258918 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: reusing connection test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844432.2589183
[0m00:13:52.258918 [debug] [Thread-1 (]: Databricks adapter: On thread (21296, 17252): None using default compute resource.
[0m00:13:52.258918 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: _acquire sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844432.2589183
[0m00:13:52.274408 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:13:52.282174 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:13:52.282174 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (compile): 00:13:52.274408 => 00:13:52.282174
[0m00:13:52.282174 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:13:52.282174 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:13:52.282174 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: get_thread_connection: sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.02325582504272461s, acqrelcnt: 1, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844432.2589183
[0m00:13:52.297147 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: idle check connection: sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.036490678787231445s, acqrelcnt: 1, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844432.2589183
[0m00:13:52.306171 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:13:52.306171 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    product_sk as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is not null
group by product_sk
having count(*) > 1



      
    ) dbt_internal_test
[0m00:13:52.679192 [debug] [Thread-1 (]: SQL status: OK in 0.3700000047683716 seconds
[0m00:13:52.679192 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (execute): 00:13:52.282174 => 00:13:52.679192
[0m00:13:52.679192 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: _release sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844432.679192
[0m00:13:52.679192 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: _release sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844432.679192
[0m00:13:52.679192 [info ] [Thread-1 (]: 6 of 8 PASS unique_dim_product_product_sk ...................................... [[32mPASS[0m in 0.42s]
[0m00:13:52.679192 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:13:52.679192 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:13:52.679192 [info ] [Thread-1 (]: 7 of 8 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m00:13:52.679192 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: idle check connection: sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844432.679192
[0m00:13:52.694696 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, now test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321)
[0m00:13:52.694696 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: reusing connection test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.015504121780395508s, acqrelcnt: 0, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844432.679192
[0m00:13:52.694696 [debug] [Thread-1 (]: Databricks adapter: On thread (21296, 17252): None using default compute resource.
[0m00:13:52.694696 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: _acquire sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.015504121780395508s, acqrelcnt: 1, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844432.679192
[0m00:13:52.694696 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:13:52.694696 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:13:52.694696 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (compile): 00:13:52.694696 => 00:13:52.694696
[0m00:13:52.694696 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:13:52.694696 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:13:52.694696 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: get_thread_connection: sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.015504121780395508s, acqrelcnt: 1, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844432.679192
[0m00:13:52.694696 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: idle check connection: sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.015504121780395508s, acqrelcnt: 1, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844432.679192
[0m00:13:52.694696 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:13:52.694696 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m00:13:53.035776 [debug] [Thread-1 (]: SQL status: OK in 0.3400000035762787 seconds
[0m00:13:53.051248 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (execute): 00:13:52.694696 => 00:13:53.051248
[0m00:13:53.051248 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: _release sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844433.0512483
[0m00:13:53.051248 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: _release sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844433.0512483
[0m00:13:53.051248 [info ] [Thread-1 (]: 7 of 8 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.37s]
[0m00:13:53.051248 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:13:53.051248 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:13:53.051248 [info ] [Thread-1 (]: 8 of 8 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m00:13:53.051248 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: idle check connection: sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844433.0512483
[0m00:13:53.051248 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, now test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493)
[0m00:13:53.051248 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: reusing connection test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844433.0512483
[0m00:13:53.051248 [debug] [Thread-1 (]: Databricks adapter: On thread (21296, 17252): None using default compute resource.
[0m00:13:53.051248 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: _acquire sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844433.0512483
[0m00:13:53.051248 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:13:53.051248 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:13:53.051248 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (compile): 00:13:53.051248 => 00:13:53.051248
[0m00:13:53.066874 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:13:53.070941 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:13:53.071934 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: get_thread_connection: sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.019692659378051758s, acqrelcnt: 1, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844433.0512483
[0m00:13:53.072609 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: idle check connection: sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.021361589431762695s, acqrelcnt: 1, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844433.0512483
[0m00:13:53.073869 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:13:53.073869 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m00:13:53.434649 [debug] [Thread-1 (]: SQL status: OK in 0.36000001430511475 seconds
[0m00:13:53.434649 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (execute): 00:13:53.066874 => 00:13:53.434649
[0m00:13:53.434649 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: _release sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844433.4346497
[0m00:13:53.434649 [debug] [Thread-1 (]: Databricks adapter: conn: 2016951584528: _release sess: 2a2ddc5b-7e3b-4b30-877b-b32c1207a71d, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21296, 17252), cmpt: ``, lut: 1707844433.4346497
[0m00:13:53.434649 [info ] [Thread-1 (]: 8 of 8 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.38s]
[0m00:13:53.434649 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:13:53.450124 [debug] [MainThread]: Databricks adapter: conn: 2016950400848: idle check connection: sess: None, name: master, idle: 2.817622661590576s, acqrelcnt: 0, lang: None, thrd: (21296, 24268), cmpt: ``, lut: 1707844430.6325018
[0m00:13:53.450124 [debug] [MainThread]: Databricks adapter: conn: 2016950400848: reusing connection master sess: None, name: master, idle: 2.817622661590576s, acqrelcnt: 0, lang: None, thrd: (21296, 24268), cmpt: ``, lut: 1707844430.6325018
[0m00:13:53.450124 [debug] [MainThread]: Databricks adapter: Thread (21296, 24268) using default compute resource.
[0m00:13:53.450124 [debug] [MainThread]: Databricks adapter: conn: 2016950400848: _acquire sess: None, name: master, idle: 2.817622661590576s, acqrelcnt: 1, lang: None, thrd: (21296, 24268), cmpt: ``, lut: 1707844430.6325018
[0m00:13:53.450124 [debug] [MainThread]: Databricks adapter: conn: 2016950400848: get_thread_connection: sess: None, name: master, idle: 2.817622661590576s, acqrelcnt: 1, lang: None, thrd: (21296, 24268), cmpt: ``, lut: 1707844430.6325018
[0m00:13:53.450124 [debug] [MainThread]: Databricks adapter: conn: 2016950400848: idle check connection: sess: None, name: master, idle: 2.817622661590576s, acqrelcnt: 1, lang: None, thrd: (21296, 24268), cmpt: ``, lut: 1707844430.6325018
[0m00:13:53.450124 [debug] [MainThread]: On master: ROLLBACK
[0m00:13:53.450124 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:13:53.708944 [debug] [MainThread]: Databricks adapter: conn: 2016950400848: session opened sess: d547fd6f-e570-4447-a4bf-e2cf278e4d24, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (21296, 24268), cmpt: ``, lut: 1707844433.7089446
[0m00:13:53.708944 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:13:53.708944 [debug] [MainThread]: Databricks adapter: conn: 2016950400848: get_thread_connection: sess: d547fd6f-e570-4447-a4bf-e2cf278e4d24, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (21296, 24268), cmpt: ``, lut: 1707844433.7089446
[0m00:13:53.708944 [debug] [MainThread]: Databricks adapter: conn: 2016950400848: idle check connection: sess: d547fd6f-e570-4447-a4bf-e2cf278e4d24, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (21296, 24268), cmpt: ``, lut: 1707844433.7089446
[0m00:13:53.708944 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:13:53.708944 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:13:53.724543 [debug] [MainThread]: Databricks adapter: conn: 2016950400848: _release sess: d547fd6f-e570-4447-a4bf-e2cf278e4d24, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (21296, 24268), cmpt: ``, lut: 1707844433.724543
[0m00:13:53.724543 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:13:53.724543 [debug] [MainThread]: On master: ROLLBACK
[0m00:13:53.724543 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:13:53.724543 [debug] [MainThread]: On master: Close
[0m00:13:53.823231 [debug] [MainThread]: Connection 'list_hive_metastore_saleslt' was properly closed.
[0m00:13:53.823231 [debug] [MainThread]: On list_hive_metastore_saleslt: ROLLBACK
[0m00:13:53.823231 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:13:53.823231 [debug] [MainThread]: On list_hive_metastore_saleslt: Close
[0m00:13:53.897596 [debug] [MainThread]: Connection 'test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493' was properly closed.
[0m00:13:53.897596 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: ROLLBACK
[0m00:13:53.897596 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:13:53.897596 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m00:13:53.975755 [info ] [MainThread]: 
[0m00:13:53.975755 [info ] [MainThread]: Finished running 8 tests in 0 hours 0 minutes and 4.47 seconds (4.47s).
[0m00:13:53.991265 [debug] [MainThread]: Command end result
[0m00:13:53.991265 [info ] [MainThread]: 
[0m00:13:53.991265 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:13:54.006792 [info ] [MainThread]: 
[0m00:13:54.007543 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models\example\schema.yml)[0m
[0m00:13:54.008575 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m00:13:54.009573 [info ] [MainThread]: 
[0m00:13:54.010571 [info ] [MainThread]:   compiled Code at target\compiled\medallion_dbt_spark\models\example\schema.yml\not_null_my_first_dbt_model_id.sql
[0m00:13:54.011825 [info ] [MainThread]: 
[0m00:13:54.011825 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=1 SKIP=0 TOTAL=8
[0m00:13:54.015083 [debug] [MainThread]: Command `dbt test` failed at 00:13:54.013978 after 5.92 seconds
[0m00:13:54.015987 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D588F930D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D589402750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D5893780D0>]}
[0m00:13:54.015987 [debug] [MainThread]: Flushing usage events
[0m00:15:38.446936 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001441328FB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014412FA7410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014412FA6FD0>]}


============================== 00:15:38.446936 | 8ac9e03c-55e0-4715-809c-52b38c7ff051 ==============================
[0m00:15:38.446936 [info ] [MainThread]: Running with dbt=1.7.7
[0m00:15:38.446936 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt test', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m00:15:39.899781 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8ac9e03c-55e0-4715-809c-52b38c7ff051', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000144256C25D0>]}
[0m00:15:39.980662 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8ac9e03c-55e0-4715-809c-52b38c7ff051', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014412FD8F10>]}
[0m00:15:39.980662 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m00:15:39.996287 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m00:15:40.099643 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:15:40.100643 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://models\example\my_first_dbt_model.sql
[0m00:15:40.331789 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8ac9e03c-55e0-4715-809c-52b38c7ff051', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014425CB1110>]}
[0m00:15:40.348659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8ac9e03c-55e0-4715-809c-52b38c7ff051', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014425CFEA50>]}
[0m00:15:40.348659 [info ] [MainThread]: Found 5 models, 7 snapshots, 8 tests, 9 sources, 0 exposures, 0 metrics, 535 macros, 0 groups, 0 semantic models
[0m00:15:40.357051 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8ac9e03c-55e0-4715-809c-52b38c7ff051', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000144259F6690>]}
[0m00:15:40.357051 [info ] [MainThread]: 
[0m00:15:40.357051 [debug] [MainThread]: Databricks adapter: conn: 1392203324048: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (19608, 12848), cmpt: ``, lut: None
[0m00:15:40.357051 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:15:40.357051 [debug] [MainThread]: Databricks adapter: Thread (19608, 12848) using default compute resource.
[0m00:15:40.357051 [debug] [MainThread]: Databricks adapter: conn: 1392203324048: _acquire sess: None, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (19608, 12848), cmpt: ``, lut: 1707844540.3570516
[0m00:15:40.365486 [debug] [ThreadPool]: Databricks adapter: conn: 1392200002384: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_snapshots, idle: 0s, acqrelcnt: 0, lang: None, thrd: (19608, 17120), cmpt: ``, lut: None
[0m00:15:40.365486 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_snapshots'
[0m00:15:40.366527 [debug] [ThreadPool]: Databricks adapter: Thread (19608, 17120) using default compute resource.
[0m00:15:40.366527 [debug] [ThreadPool]: Databricks adapter: conn: 1392200002384: _acquire sess: None, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (19608, 17120), cmpt: ``, lut: 1707844540.3665273
[0m00:15:40.375138 [debug] [ThreadPool]: Databricks adapter: conn: 1392200002384: get_thread_connection: sess: None, name: list_hive_metastore_snapshots, idle: 0.007611513137817383s, acqrelcnt: 1, lang: None, thrd: (19608, 17120), cmpt: ``, lut: 1707844540.3665273
[0m00:15:40.375138 [debug] [ThreadPool]: Databricks adapter: conn: 1392200002384: idle check connection: sess: None, name: list_hive_metastore_snapshots, idle: 0.008611440658569336s, acqrelcnt: 1, lang: None, thrd: (19608, 17120), cmpt: ``, lut: 1707844540.3665273
[0m00:15:40.376681 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:15:40.377786 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m00:15:40.378469 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:15:41.003481 [debug] [ThreadPool]: Databricks adapter: conn: 1392200002384: session opened sess: e8235aea-b7fc-4fd6-bc84-74c4aa83dde1, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (19608, 17120), cmpt: ``, lut: 1707844541.0034819
[0m00:15:41.433567 [debug] [ThreadPool]: SQL status: OK in 1.059999942779541 seconds
[0m00:15:41.536834 [debug] [ThreadPool]: Databricks adapter: conn: 1392200002384: get_thread_connection: sess: e8235aea-b7fc-4fd6-bc84-74c4aa83dde1, name: list_hive_metastore_snapshots, idle: 0.5333526134490967s, acqrelcnt: 1, lang: None, thrd: (19608, 17120), cmpt: ``, lut: 1707844541.0034819
[0m00:15:41.537824 [debug] [ThreadPool]: Databricks adapter: conn: 1392200002384: idle check connection: sess: e8235aea-b7fc-4fd6-bc84-74c4aa83dde1, name: list_hive_metastore_snapshots, idle: 0.5343427658081055s, acqrelcnt: 1, lang: None, thrd: (19608, 17120), cmpt: ``, lut: 1707844541.0034819
[0m00:15:41.538749 [debug] [ThreadPool]: Databricks adapter: conn: 1392200002384: get_thread_connection: sess: e8235aea-b7fc-4fd6-bc84-74c4aa83dde1, name: list_hive_metastore_snapshots, idle: 0.5352678298950195s, acqrelcnt: 1, lang: None, thrd: (19608, 17120), cmpt: ``, lut: 1707844541.0034819
[0m00:15:41.538749 [debug] [ThreadPool]: Databricks adapter: conn: 1392200002384: idle check connection: sess: e8235aea-b7fc-4fd6-bc84-74c4aa83dde1, name: list_hive_metastore_snapshots, idle: 0.5352678298950195s, acqrelcnt: 1, lang: None, thrd: (19608, 17120), cmpt: ``, lut: 1707844541.0034819
[0m00:15:41.538749 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:15:41.538749 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:15:41.540189 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m00:15:41.762401 [debug] [ThreadPool]: SQL status: OK in 0.2199999988079071 seconds
[0m00:15:41.770300 [debug] [ThreadPool]: Databricks adapter: conn: 1392200002384: get_thread_connection: sess: e8235aea-b7fc-4fd6-bc84-74c4aa83dde1, name: list_hive_metastore_snapshots, idle: 0.7658452987670898s, acqrelcnt: 1, lang: None, thrd: (19608, 17120), cmpt: ``, lut: 1707844541.0034819
[0m00:15:41.770300 [debug] [ThreadPool]: Databricks adapter: conn: 1392200002384: idle check connection: sess: e8235aea-b7fc-4fd6-bc84-74c4aa83dde1, name: list_hive_metastore_snapshots, idle: 0.7668187618255615s, acqrelcnt: 1, lang: None, thrd: (19608, 17120), cmpt: ``, lut: 1707844541.0034819
[0m00:15:41.771215 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:15:41.771215 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m00:15:41.917007 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m00:15:41.932631 [debug] [ThreadPool]: Databricks adapter: conn: 1392200002384: _release sess: e8235aea-b7fc-4fd6-bc84-74c4aa83dde1, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (19608, 17120), cmpt: ``, lut: 1707844541.9326315
[0m00:15:41.932631 [debug] [ThreadPool]: Databricks adapter: conn: 1392200002384: idle check connection: sess: e8235aea-b7fc-4fd6-bc84-74c4aa83dde1, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (19608, 17120), cmpt: ``, lut: 1707844541.9326315
[0m00:15:41.932631 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now list_hive_metastore_saleslt)
[0m00:15:41.932631 [debug] [ThreadPool]: Databricks adapter: conn: 1392200002384: reusing connection list_hive_metastore_snapshots sess: e8235aea-b7fc-4fd6-bc84-74c4aa83dde1, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (19608, 17120), cmpt: ``, lut: 1707844541.9326315
[0m00:15:41.932631 [debug] [ThreadPool]: Databricks adapter: Thread (19608, 17120) using default compute resource.
[0m00:15:41.932631 [debug] [ThreadPool]: Databricks adapter: conn: 1392200002384: _acquire sess: e8235aea-b7fc-4fd6-bc84-74c4aa83dde1, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (19608, 17120), cmpt: ``, lut: 1707844541.9326315
[0m00:15:41.932631 [debug] [ThreadPool]: Databricks adapter: conn: 1392200002384: get_thread_connection: sess: e8235aea-b7fc-4fd6-bc84-74c4aa83dde1, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (19608, 17120), cmpt: ``, lut: 1707844541.9326315
[0m00:15:41.932631 [debug] [ThreadPool]: Databricks adapter: conn: 1392200002384: idle check connection: sess: e8235aea-b7fc-4fd6-bc84-74c4aa83dde1, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (19608, 17120), cmpt: ``, lut: 1707844541.9326315
[0m00:15:41.932631 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:15:41.932631 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m00:15:42.042088 [debug] [ThreadPool]: SQL status: OK in 0.10999999940395355 seconds
[0m00:15:42.042088 [debug] [ThreadPool]: Databricks adapter: conn: 1392200002384: get_thread_connection: sess: e8235aea-b7fc-4fd6-bc84-74c4aa83dde1, name: list_hive_metastore_saleslt, idle: 0.10945653915405273s, acqrelcnt: 1, lang: None, thrd: (19608, 17120), cmpt: ``, lut: 1707844541.9326315
[0m00:15:42.042088 [debug] [ThreadPool]: Databricks adapter: conn: 1392200002384: idle check connection: sess: e8235aea-b7fc-4fd6-bc84-74c4aa83dde1, name: list_hive_metastore_saleslt, idle: 0.10945653915405273s, acqrelcnt: 1, lang: None, thrd: (19608, 17120), cmpt: ``, lut: 1707844541.9326315
[0m00:15:42.042088 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:15:42.042088 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m00:15:42.177720 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m00:15:42.193298 [debug] [ThreadPool]: Databricks adapter: conn: 1392200002384: get_thread_connection: sess: e8235aea-b7fc-4fd6-bc84-74c4aa83dde1, name: list_hive_metastore_saleslt, idle: 0.2606668472290039s, acqrelcnt: 1, lang: None, thrd: (19608, 17120), cmpt: ``, lut: 1707844541.9326315
[0m00:15:42.193298 [debug] [ThreadPool]: Databricks adapter: conn: 1392200002384: idle check connection: sess: e8235aea-b7fc-4fd6-bc84-74c4aa83dde1, name: list_hive_metastore_saleslt, idle: 0.2606668472290039s, acqrelcnt: 1, lang: None, thrd: (19608, 17120), cmpt: ``, lut: 1707844541.9326315
[0m00:15:42.193298 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:15:42.193298 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m00:15:42.427586 [debug] [ThreadPool]: SQL status: OK in 0.23000000417232513 seconds
[0m00:15:42.432938 [debug] [ThreadPool]: Databricks adapter: conn: 1392200002384: _release sess: e8235aea-b7fc-4fd6-bc84-74c4aa83dde1, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (19608, 17120), cmpt: ``, lut: 1707844542.4329383
[0m00:15:42.436833 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8ac9e03c-55e0-4715-809c-52b38c7ff051', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001442587F190>]}
[0m00:15:42.436833 [debug] [MainThread]: Databricks adapter: conn: 1392203324048: get_thread_connection: sess: None, name: master, idle: 2.0797817707061768s, acqrelcnt: 1, lang: None, thrd: (19608, 12848), cmpt: ``, lut: 1707844540.3570516
[0m00:15:42.436833 [debug] [MainThread]: Databricks adapter: conn: 1392203324048: idle check connection: sess: None, name: master, idle: 2.0797817707061768s, acqrelcnt: 1, lang: None, thrd: (19608, 12848), cmpt: ``, lut: 1707844540.3570516
[0m00:15:42.436833 [debug] [MainThread]: Databricks adapter: conn: 1392203324048: get_thread_connection: sess: None, name: master, idle: 2.0797817707061768s, acqrelcnt: 1, lang: None, thrd: (19608, 12848), cmpt: ``, lut: 1707844540.3570516
[0m00:15:42.443341 [debug] [MainThread]: Databricks adapter: conn: 1392203324048: idle check connection: sess: None, name: master, idle: 2.0797817707061768s, acqrelcnt: 1, lang: None, thrd: (19608, 12848), cmpt: ``, lut: 1707844540.3570516
[0m00:15:42.443341 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:15:42.443341 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:15:42.443341 [debug] [MainThread]: Databricks adapter: conn: 1392203324048: _release sess: None, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (19608, 12848), cmpt: ``, lut: 1707844542.4433415
[0m00:15:42.443341 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:15:42.443341 [info ] [MainThread]: 
[0m00:15:42.443341 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:15:42.443341 [info ] [Thread-1 (]: 1 of 8 START test not_null_dim_product_product_name ............................ [RUN]
[0m00:15:42.453167 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: Creating DatabricksDBTConnection sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0s, acqrelcnt: 0, lang: None, thrd: (19608, 15600), cmpt: ``, lut: None
[0m00:15:42.453167 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5'
[0m00:15:42.454448 [debug] [Thread-1 (]: Databricks adapter: On thread (19608, 15600): None using default compute resource.
[0m00:15:42.454448 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: _acquire sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844542.4544482
[0m00:15:42.455555 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:15:42.471361 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:15:42.471361 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (compile): 00:15:42.455555 => 00:15:42.471361
[0m00:15:42.471361 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:15:42.485996 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:15:42.485996 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.031548261642456055s, acqrelcnt: 1, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844542.4544482
[0m00:15:42.485996 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.031548261642456055s, acqrelcnt: 1, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844542.4544482
[0m00:15:42.502141 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.031548261642456055s, acqrelcnt: 1, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844542.4544482
[0m00:15:42.502141 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.047693490982055664s, acqrelcnt: 1, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844542.4544482
[0m00:15:42.502141 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:15:42.502141 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:15:42.502141 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_name
from `hive_metastore`.`saleslt`.`dim_product`
where product_name is null



      
    ) dbt_internal_test
[0m00:15:42.502141 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:15:42.727721 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: session opened sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844542.7277215
[0m00:15:43.055813 [debug] [Thread-1 (]: SQL status: OK in 0.550000011920929 seconds
[0m00:15:43.060004 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (execute): 00:15:42.471361 => 00:15:43.060004
[0m00:15:43.060004 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: _release sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844543.060004
[0m00:15:43.060004 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: _release sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844543.060004
[0m00:15:43.060004 [info ] [Thread-1 (]: 1 of 8 PASS not_null_dim_product_product_name .................................. [[32mPASS[0m in 0.62s]
[0m00:15:43.070460 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:15:43.071470 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:15:43.072061 [info ] [Thread-1 (]: 2 of 8 START test not_null_dim_product_product_sk .............................. [RUN]
[0m00:15:43.074650 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: idle check connection: sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.014646768569946289s, acqrelcnt: 0, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844543.060004
[0m00:15:43.074650 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, now test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417)
[0m00:15:43.076313 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.016309499740600586s, acqrelcnt: 0, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844543.060004
[0m00:15:43.077043 [debug] [Thread-1 (]: Databricks adapter: On thread (19608, 15600): None using default compute resource.
[0m00:15:43.077839 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: _acquire sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.017835378646850586s, acqrelcnt: 1, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844543.060004
[0m00:15:43.078572 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:15:43.079940 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:15:43.088511 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (compile): 00:15:43.079265 => 00:15:43.087453
[0m00:15:43.089521 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:15:43.095519 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:15:43.097693 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: get_thread_connection: sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.03651571273803711s, acqrelcnt: 1, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844543.060004
[0m00:15:43.097693 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: idle check connection: sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.037689208984375s, acqrelcnt: 1, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844543.060004
[0m00:15:43.098701 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:15:43.099701 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_sk
from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is null



      
    ) dbt_internal_test
[0m00:15:43.344592 [debug] [Thread-1 (]: SQL status: OK in 0.23999999463558197 seconds
[0m00:15:43.360172 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (execute): 00:15:43.089521 => 00:15:43.360172
[0m00:15:43.360172 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: _release sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844543.3601723
[0m00:15:43.360172 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: _release sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844543.3601723
[0m00:15:43.360172 [info ] [Thread-1 (]: 2 of 8 PASS not_null_dim_product_product_sk .................................... [[32mPASS[0m in 0.29s]
[0m00:15:43.360172 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:15:43.360172 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:15:43.360172 [info ] [Thread-1 (]: 3 of 8 START test not_null_dim_product_sellstartdate ........................... [RUN]
[0m00:15:43.360172 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: idle check connection: sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844543.3601723
[0m00:15:43.360172 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, now test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546)
[0m00:15:43.360172 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844543.3601723
[0m00:15:43.360172 [debug] [Thread-1 (]: Databricks adapter: On thread (19608, 15600): None using default compute resource.
[0m00:15:43.360172 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: _acquire sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844543.3601723
[0m00:15:43.360172 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:15:43.379760 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:15:43.381392 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (compile): 00:15:43.360172 => 00:15:43.380736
[0m00:15:43.381392 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:15:43.385622 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:15:43.387820 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: get_thread_connection: sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.026474714279174805s, acqrelcnt: 1, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844543.3601723
[0m00:15:43.388832 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: idle check connection: sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.027647733688354492s, acqrelcnt: 1, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844543.3601723
[0m00:15:43.388832 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:15:43.389827 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select sellstartdate
from `hive_metastore`.`saleslt`.`dim_product`
where sellstartdate is null



      
    ) dbt_internal_test
[0m00:15:43.617335 [debug] [Thread-1 (]: SQL status: OK in 0.23000000417232513 seconds
[0m00:15:43.617335 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (execute): 00:15:43.382381 => 00:15:43.617335
[0m00:15:43.617335 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: _release sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844543.6173353
[0m00:15:43.633017 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: _release sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844543.6173353
[0m00:15:43.633017 [info ] [Thread-1 (]: 3 of 8 PASS not_null_dim_product_sellstartdate ................................. [[32mPASS[0m in 0.27s]
[0m00:15:43.633017 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:15:43.633017 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:15:43.633017 [info ] [Thread-1 (]: 4 of 8 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m00:15:43.633017 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: idle check connection: sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.015682220458984375s, acqrelcnt: 0, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844543.6173353
[0m00:15:43.633017 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, now test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710)
[0m00:15:43.633017 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: reusing connection test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.015682220458984375s, acqrelcnt: 0, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844543.6173353
[0m00:15:43.633017 [debug] [Thread-1 (]: Databricks adapter: On thread (19608, 15600): None using default compute resource.
[0m00:15:43.639699 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: _acquire sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.015682220458984375s, acqrelcnt: 1, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844543.6173353
[0m00:15:43.639969 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:15:43.645843 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:15:43.645843 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 00:15:43.639969 => 00:15:43.645843
[0m00:15:43.645843 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:15:43.651570 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:15:43.653643 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: get_thread_connection: sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.035245656967163086s, acqrelcnt: 1, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844543.6173353
[0m00:15:43.653643 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: idle check connection: sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.03630805015563965s, acqrelcnt: 1, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844543.6173353
[0m00:15:43.654816 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:15:43.655825 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m00:15:43.936727 [debug] [Thread-1 (]: SQL status: OK in 0.2800000011920929 seconds
[0m00:15:43.936727 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 00:15:43.645843 => 00:15:43.936727
[0m00:15:43.936727 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: _release sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844543.9367278
[0m00:15:43.936727 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: _release sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844543.9367278
[0m00:15:43.936727 [error] [Thread-1 (]: 4 of 8 FAIL 1 not_null_my_first_dbt_model_id ................................... [[31mFAIL 1[0m in 0.30s]
[0m00:15:43.936727 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:15:43.936727 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:15:43.952225 [info ] [Thread-1 (]: 5 of 8 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m00:15:43.952225 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: idle check connection: sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.015497684478759766s, acqrelcnt: 0, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844543.9367278
[0m00:15:43.952225 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, now test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778)
[0m00:15:43.952225 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: reusing connection test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.015497684478759766s, acqrelcnt: 0, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844543.9367278
[0m00:15:43.952225 [debug] [Thread-1 (]: Databricks adapter: On thread (19608, 15600): None using default compute resource.
[0m00:15:43.952225 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: _acquire sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.015497684478759766s, acqrelcnt: 1, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844543.9367278
[0m00:15:43.952225 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:15:43.952225 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:15:43.952225 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (compile): 00:15:43.952225 => 00:15:43.952225
[0m00:15:43.952225 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:15:43.952225 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:15:43.952225 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: get_thread_connection: sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.015497684478759766s, acqrelcnt: 1, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844543.9367278
[0m00:15:43.952225 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: idle check connection: sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.015497684478759766s, acqrelcnt: 1, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844543.9367278
[0m00:15:43.952225 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:15:43.952225 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m00:15:44.235241 [debug] [Thread-1 (]: SQL status: OK in 0.2800000011920929 seconds
[0m00:15:44.235241 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (execute): 00:15:43.952225 => 00:15:44.235241
[0m00:15:44.235241 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: _release sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844544.2352417
[0m00:15:44.235241 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: _release sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844544.2352417
[0m00:15:44.235241 [info ] [Thread-1 (]: 5 of 8 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.28s]
[0m00:15:44.250690 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:15:44.250690 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:15:44.250690 [info ] [Thread-1 (]: 6 of 8 START test unique_dim_product_product_sk ................................ [RUN]
[0m00:15:44.250690 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: idle check connection: sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.015449047088623047s, acqrelcnt: 0, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844544.2352417
[0m00:15:44.250690 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, now test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c)
[0m00:15:44.250690 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: reusing connection test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.015449047088623047s, acqrelcnt: 0, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844544.2352417
[0m00:15:44.250690 [debug] [Thread-1 (]: Databricks adapter: On thread (19608, 15600): None using default compute resource.
[0m00:15:44.250690 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: _acquire sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.015449047088623047s, acqrelcnt: 1, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844544.2352417
[0m00:15:44.250690 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:15:44.250690 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:15:44.250690 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (compile): 00:15:44.250690 => 00:15:44.250690
[0m00:15:44.250690 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:15:44.250690 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:15:44.250690 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: get_thread_connection: sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.015449047088623047s, acqrelcnt: 1, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844544.2352417
[0m00:15:44.266342 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: idle check connection: sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.015449047088623047s, acqrelcnt: 1, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844544.2352417
[0m00:15:44.266342 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:15:44.267464 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    product_sk as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is not null
group by product_sk
having count(*) > 1



      
    ) dbt_internal_test
[0m00:15:44.621325 [debug] [Thread-1 (]: SQL status: OK in 0.3499999940395355 seconds
[0m00:15:44.621325 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (execute): 00:15:44.250690 => 00:15:44.621325
[0m00:15:44.636859 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: _release sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844544.6213257
[0m00:15:44.636859 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: _release sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844544.63686
[0m00:15:44.636859 [info ] [Thread-1 (]: 6 of 8 PASS unique_dim_product_product_sk ...................................... [[32mPASS[0m in 0.39s]
[0m00:15:44.636859 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:15:44.636859 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:15:44.636859 [info ] [Thread-1 (]: 7 of 8 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m00:15:44.636859 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: idle check connection: sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844544.63686
[0m00:15:44.636859 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, now test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321)
[0m00:15:44.636859 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: reusing connection test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844544.63686
[0m00:15:44.636859 [debug] [Thread-1 (]: Databricks adapter: On thread (19608, 15600): None using default compute resource.
[0m00:15:44.636859 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: _acquire sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844544.63686
[0m00:15:44.636859 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:15:44.636859 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:15:44.636859 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (compile): 00:15:44.636859 => 00:15:44.636859
[0m00:15:44.636859 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:15:44.653645 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:15:44.654761 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: get_thread_connection: sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.01779627799987793s, acqrelcnt: 1, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844544.63686
[0m00:15:44.654761 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: idle check connection: sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.01790165901184082s, acqrelcnt: 1, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844544.63686
[0m00:15:44.655810 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:15:44.655810 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m00:15:44.956351 [debug] [Thread-1 (]: SQL status: OK in 0.30000001192092896 seconds
[0m00:15:44.971820 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (execute): 00:15:44.636859 => 00:15:44.971820
[0m00:15:44.971820 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: _release sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844544.97182
[0m00:15:44.971820 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: _release sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844544.97182
[0m00:15:44.971820 [info ] [Thread-1 (]: 7 of 8 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.33s]
[0m00:15:44.971820 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:15:44.971820 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:15:44.971820 [info ] [Thread-1 (]: 8 of 8 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m00:15:44.971820 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: idle check connection: sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844544.97182
[0m00:15:44.971820 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, now test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493)
[0m00:15:44.971820 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: reusing connection test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844544.97182
[0m00:15:44.971820 [debug] [Thread-1 (]: Databricks adapter: On thread (19608, 15600): None using default compute resource.
[0m00:15:44.971820 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: _acquire sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844544.97182
[0m00:15:44.971820 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:15:44.971820 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:15:44.971820 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (compile): 00:15:44.971820 => 00:15:44.971820
[0m00:15:44.971820 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:15:44.990484 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:15:44.991787 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: get_thread_connection: sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.01966714859008789s, acqrelcnt: 1, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844544.97182
[0m00:15:44.992736 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: idle check connection: sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.01996755599975586s, acqrelcnt: 1, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844544.97182
[0m00:15:44.993388 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:15:44.993388 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m00:15:45.349474 [debug] [Thread-1 (]: SQL status: OK in 0.36000001430511475 seconds
[0m00:15:45.349474 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (execute): 00:15:44.971820 => 00:15:45.349474
[0m00:15:45.349474 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: _release sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844545.349475
[0m00:15:45.364928 [debug] [Thread-1 (]: Databricks adapter: conn: 1392202192592: _release sess: c7c69430-650a-4293-a2b6-2bfb7a4682e1, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (19608, 15600), cmpt: ``, lut: 1707844545.3649285
[0m00:15:45.364928 [info ] [Thread-1 (]: 8 of 8 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.39s]
[0m00:15:45.364928 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:15:45.364928 [debug] [MainThread]: Databricks adapter: conn: 1392203324048: idle check connection: sess: None, name: master, idle: 2.9215869903564453s, acqrelcnt: 0, lang: None, thrd: (19608, 12848), cmpt: ``, lut: 1707844542.4433415
[0m00:15:45.364928 [debug] [MainThread]: Databricks adapter: conn: 1392203324048: reusing connection master sess: None, name: master, idle: 2.9215869903564453s, acqrelcnt: 0, lang: None, thrd: (19608, 12848), cmpt: ``, lut: 1707844542.4433415
[0m00:15:45.364928 [debug] [MainThread]: Databricks adapter: Thread (19608, 12848) using default compute resource.
[0m00:15:45.364928 [debug] [MainThread]: Databricks adapter: conn: 1392203324048: _acquire sess: None, name: master, idle: 2.9215869903564453s, acqrelcnt: 1, lang: None, thrd: (19608, 12848), cmpt: ``, lut: 1707844542.4433415
[0m00:15:45.364928 [debug] [MainThread]: Databricks adapter: conn: 1392203324048: get_thread_connection: sess: None, name: master, idle: 2.9215869903564453s, acqrelcnt: 1, lang: None, thrd: (19608, 12848), cmpt: ``, lut: 1707844542.4433415
[0m00:15:45.364928 [debug] [MainThread]: Databricks adapter: conn: 1392203324048: idle check connection: sess: None, name: master, idle: 2.9215869903564453s, acqrelcnt: 1, lang: None, thrd: (19608, 12848), cmpt: ``, lut: 1707844542.4433415
[0m00:15:45.364928 [debug] [MainThread]: On master: ROLLBACK
[0m00:15:45.364928 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:15:45.629467 [debug] [MainThread]: Databricks adapter: conn: 1392203324048: session opened sess: c9174762-07cd-4ec6-abae-375794e66ddf, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (19608, 12848), cmpt: ``, lut: 1707844545.6294675
[0m00:15:45.629467 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:15:45.629467 [debug] [MainThread]: Databricks adapter: conn: 1392203324048: get_thread_connection: sess: c9174762-07cd-4ec6-abae-375794e66ddf, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (19608, 12848), cmpt: ``, lut: 1707844545.6294675
[0m00:15:45.629467 [debug] [MainThread]: Databricks adapter: conn: 1392203324048: idle check connection: sess: c9174762-07cd-4ec6-abae-375794e66ddf, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (19608, 12848), cmpt: ``, lut: 1707844545.6294675
[0m00:15:45.629467 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:15:45.629467 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:15:45.629467 [debug] [MainThread]: Databricks adapter: conn: 1392203324048: _release sess: c9174762-07cd-4ec6-abae-375794e66ddf, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (19608, 12848), cmpt: ``, lut: 1707844545.6294675
[0m00:15:45.629467 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:15:45.629467 [debug] [MainThread]: On master: ROLLBACK
[0m00:15:45.629467 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:15:45.629467 [debug] [MainThread]: On master: Close
[0m00:15:45.723248 [debug] [MainThread]: Connection 'list_hive_metastore_saleslt' was properly closed.
[0m00:15:45.723248 [debug] [MainThread]: On list_hive_metastore_saleslt: ROLLBACK
[0m00:15:45.723248 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:15:45.723248 [debug] [MainThread]: On list_hive_metastore_saleslt: Close
[0m00:15:45.804262 [debug] [MainThread]: Connection 'test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493' was properly closed.
[0m00:15:45.804262 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: ROLLBACK
[0m00:15:45.804262 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:15:45.804262 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m00:15:45.884822 [info ] [MainThread]: 
[0m00:15:45.884822 [info ] [MainThread]: Finished running 8 tests in 0 hours 0 minutes and 5.53 seconds (5.53s).
[0m00:15:45.884822 [debug] [MainThread]: Command end result
[0m00:15:45.903994 [info ] [MainThread]: 
[0m00:15:45.905174 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:15:45.905174 [info ] [MainThread]: 
[0m00:15:45.905174 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models\example\schema.yml)[0m
[0m00:15:45.905174 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m00:15:45.905174 [info ] [MainThread]: 
[0m00:15:45.910774 [info ] [MainThread]:   compiled Code at target\compiled\medallion_dbt_spark\models\example\schema.yml\not_null_my_first_dbt_model_id.sql
[0m00:15:45.911783 [info ] [MainThread]: 
[0m00:15:45.912780 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=1 SKIP=0 TOTAL=8
[0m00:15:45.914857 [debug] [MainThread]: Command `dbt test` failed at 00:15:45.914857 after 7.51 seconds
[0m00:15:45.914857 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001441331A210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014412FAF690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000144128D0E50>]}
[0m00:15:45.915946 [debug] [MainThread]: Flushing usage events
[0m00:18:21.243716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D59D75AAD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D59D9AA810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D59D6C4B90>]}


============================== 00:18:21.243716 | c215719c-a35c-4941-95bc-8b40026c4544 ==============================
[0m00:18:21.243716 [info ] [MainThread]: Running with dbt=1.7.7
[0m00:18:21.243716 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt test --store-failures', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m00:18:22.485940 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c215719c-a35c-4941-95bc-8b40026c4544', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D5AFE37590>]}
[0m00:18:22.565338 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c215719c-a35c-4941-95bc-8b40026c4544', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D5AFD9D1D0>]}
[0m00:18:22.566253 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m00:18:22.576178 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m00:18:22.663850 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m00:18:22.663850 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m00:18:22.683795 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c215719c-a35c-4941-95bc-8b40026c4544', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D5AFE3F710>]}
[0m00:18:22.698919 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c215719c-a35c-4941-95bc-8b40026c4544', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D5AFFB7B10>]}
[0m00:18:22.699919 [info ] [MainThread]: Found 5 models, 7 snapshots, 8 tests, 9 sources, 0 exposures, 0 metrics, 535 macros, 0 groups, 0 semantic models
[0m00:18:22.701570 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c215719c-a35c-4941-95bc-8b40026c4544', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D5AFF4E810>]}
[0m00:18:22.703651 [info ] [MainThread]: 
[0m00:18:22.704464 [debug] [MainThread]: Databricks adapter: conn: 3116803034256: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (21500, 23068), cmpt: ``, lut: None
[0m00:18:22.705473 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:18:22.705473 [debug] [MainThread]: Databricks adapter: Thread (21500, 23068) using default compute resource.
[0m00:18:22.705473 [debug] [MainThread]: Databricks adapter: conn: 3116803034256: _acquire sess: None, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (21500, 23068), cmpt: ``, lut: 1707844702.7054732
[0m00:18:22.707473 [debug] [ThreadPool]: Databricks adapter: conn: 3116803771408: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (21500, 4992), cmpt: ``, lut: None
[0m00:18:22.708477 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m00:18:22.709472 [debug] [ThreadPool]: Databricks adapter: Thread (21500, 4992) using default compute resource.
[0m00:18:22.709907 [debug] [ThreadPool]: Databricks adapter: conn: 3116803771408: _acquire sess: None, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (21500, 4992), cmpt: ``, lut: 1707844702.7099075
[0m00:18:22.709907 [debug] [ThreadPool]: Databricks adapter: conn: 3116803771408: get_thread_connection: sess: None, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (21500, 4992), cmpt: ``, lut: 1707844702.7099075
[0m00:18:22.710944 [debug] [ThreadPool]: Databricks adapter: conn: 3116803771408: idle check connection: sess: None, name: list_hive_metastore, idle: 0.0010371208190917969s, acqrelcnt: 1, lang: None, thrd: (21500, 4992), cmpt: ``, lut: 1707844702.7099075
[0m00:18:22.710944 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m00:18:22.711965 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m00:18:22.711965 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:18:23.139060 [debug] [ThreadPool]: Databricks adapter: conn: 3116803771408: session opened sess: 1bdd7a50-fca9-4bff-ae87-8f5fe4e0bb17, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (21500, 4992), cmpt: ``, lut: 1707844703.1390607
[0m00:18:23.521575 [debug] [ThreadPool]: SQL status: OK in 0.8100000023841858 seconds
[0m00:18:23.521575 [debug] [ThreadPool]: Databricks adapter: conn: 3116803771408: _release sess: 1bdd7a50-fca9-4bff-ae87-8f5fe4e0bb17, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (21500, 4992), cmpt: ``, lut: 1707844703.5215755
[0m00:18:23.521575 [debug] [ThreadPool]: Databricks adapter: conn: 3116803771408: idle check connection: sess: 1bdd7a50-fca9-4bff-ae87-8f5fe4e0bb17, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (21500, 4992), cmpt: ``, lut: 1707844703.5215755
[0m00:18:23.521575 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_saleslt_dbt_test__audit)
[0m00:18:23.521575 [debug] [ThreadPool]: Databricks adapter: conn: 3116803771408: reusing connection list_hive_metastore sess: 1bdd7a50-fca9-4bff-ae87-8f5fe4e0bb17, name: create_hive_metastore_saleslt_dbt_test__audit, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (21500, 4992), cmpt: ``, lut: 1707844703.5215755
[0m00:18:23.521575 [debug] [ThreadPool]: Databricks adapter: Thread (21500, 4992) using default compute resource.
[0m00:18:23.521575 [debug] [ThreadPool]: Databricks adapter: conn: 3116803771408: _acquire sess: 1bdd7a50-fca9-4bff-ae87-8f5fe4e0bb17, name: create_hive_metastore_saleslt_dbt_test__audit, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (21500, 4992), cmpt: ``, lut: 1707844703.5215755
[0m00:18:23.521575 [debug] [ThreadPool]: Databricks adapter: conn: 3116803771408: idle check connection: sess: 1bdd7a50-fca9-4bff-ae87-8f5fe4e0bb17, name: create_hive_metastore_saleslt_dbt_test__audit, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (21500, 4992), cmpt: ``, lut: 1707844703.5215755
[0m00:18:23.521575 [debug] [ThreadPool]: Databricks adapter: Thread (21500, 4992) using default compute resource.
[0m00:18:23.521575 [debug] [ThreadPool]: Databricks adapter: conn: 3116803771408: _acquire sess: 1bdd7a50-fca9-4bff-ae87-8f5fe4e0bb17, name: create_hive_metastore_saleslt_dbt_test__audit, idle: 0.0s, acqrelcnt: 2, lang: None, thrd: (21500, 4992), cmpt: ``, lut: 1707844703.5215755
[0m00:18:23.537223 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "saleslt_dbt_test__audit"
"
[0m00:18:23.537223 [debug] [ThreadPool]: Databricks adapter: conn: 3116803771408: get_thread_connection: sess: 1bdd7a50-fca9-4bff-ae87-8f5fe4e0bb17, name: create_hive_metastore_saleslt_dbt_test__audit, idle: 0.01564812660217285s, acqrelcnt: 2, lang: None, thrd: (21500, 4992), cmpt: ``, lut: 1707844703.5215755
[0m00:18:23.537223 [debug] [ThreadPool]: Databricks adapter: conn: 3116803771408: idle check connection: sess: 1bdd7a50-fca9-4bff-ae87-8f5fe4e0bb17, name: create_hive_metastore_saleslt_dbt_test__audit, idle: 0.01564812660217285s, acqrelcnt: 2, lang: None, thrd: (21500, 4992), cmpt: ``, lut: 1707844703.5215755
[0m00:18:23.537223 [debug] [ThreadPool]: Databricks adapter: conn: 3116803771408: get_thread_connection: sess: 1bdd7a50-fca9-4bff-ae87-8f5fe4e0bb17, name: create_hive_metastore_saleslt_dbt_test__audit, idle: 0.01564812660217285s, acqrelcnt: 2, lang: None, thrd: (21500, 4992), cmpt: ``, lut: 1707844703.5215755
[0m00:18:23.537223 [debug] [ThreadPool]: Databricks adapter: conn: 3116803771408: idle check connection: sess: 1bdd7a50-fca9-4bff-ae87-8f5fe4e0bb17, name: create_hive_metastore_saleslt_dbt_test__audit, idle: 0.01564812660217285s, acqrelcnt: 2, lang: None, thrd: (21500, 4992), cmpt: ``, lut: 1707844703.5215755
[0m00:18:23.537223 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:18:23.537223 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_saleslt_dbt_test__audit"
[0m00:18:23.537223 [debug] [ThreadPool]: On create_hive_metastore_saleslt_dbt_test__audit: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "create_hive_metastore_saleslt_dbt_test__audit"} */
create schema if not exists `hive_metastore`.`saleslt_dbt_test__audit`
  
[0m00:18:23.896649 [debug] [ThreadPool]: SQL status: OK in 0.36000001430511475 seconds
[0m00:18:23.896649 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m00:18:23.896649 [debug] [ThreadPool]: Databricks adapter: conn: 3116803771408: _release sess: 1bdd7a50-fca9-4bff-ae87-8f5fe4e0bb17, name: create_hive_metastore_saleslt_dbt_test__audit, idle: 0.3750743865966797s, acqrelcnt: 1, lang: None, thrd: (21500, 4992), cmpt: ``, lut: 1707844703.5215755
[0m00:18:23.896649 [debug] [ThreadPool]: Databricks adapter: conn: 3116803771408: _release sess: 1bdd7a50-fca9-4bff-ae87-8f5fe4e0bb17, name: create_hive_metastore_saleslt_dbt_test__audit, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (21500, 4992), cmpt: ``, lut: 1707844703.8966498
[0m00:18:23.896649 [debug] [ThreadPool]: Databricks adapter: conn: 3116804560080: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_saleslt_dbt_test__audit, idle: 0s, acqrelcnt: 0, lang: None, thrd: (21500, 3252), cmpt: ``, lut: None
[0m00:18:23.896649 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_saleslt_dbt_test__audit'
[0m00:18:23.896649 [debug] [ThreadPool]: Databricks adapter: Thread (21500, 3252) using default compute resource.
[0m00:18:23.896649 [debug] [ThreadPool]: Databricks adapter: conn: 3116804560080: _acquire sess: None, name: list_hive_metastore_saleslt_dbt_test__audit, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (21500, 3252), cmpt: ``, lut: 1707844703.8966498
[0m00:18:23.912100 [debug] [ThreadPool]: Databricks adapter: conn: 3116804560080: get_thread_connection: sess: None, name: list_hive_metastore_saleslt_dbt_test__audit, idle: 0.01545095443725586s, acqrelcnt: 1, lang: None, thrd: (21500, 3252), cmpt: ``, lut: 1707844703.8966498
[0m00:18:23.912100 [debug] [ThreadPool]: Databricks adapter: conn: 3116804560080: idle check connection: sess: None, name: list_hive_metastore_saleslt_dbt_test__audit, idle: 0.01545095443725586s, acqrelcnt: 1, lang: None, thrd: (21500, 3252), cmpt: ``, lut: 1707844703.8966498
[0m00:18:23.912100 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt_dbt_test__audit"
[0m00:18:23.912100 [debug] [ThreadPool]: On list_hive_metastore_saleslt_dbt_test__audit: GetTables(database=hive_metastore, schema=saleslt_dbt_test__audit, identifier=None)
[0m00:18:23.912100 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:18:24.243704 [debug] [ThreadPool]: Databricks adapter: conn: 3116804560080: session opened sess: c77b2f69-0d57-4d8e-a500-15c2ffa54d7c, name: list_hive_metastore_saleslt_dbt_test__audit, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (21500, 3252), cmpt: ``, lut: 1707844704.2281454
[0m00:18:24.384350 [debug] [ThreadPool]: SQL status: OK in 0.4699999988079071 seconds
[0m00:18:24.384350 [debug] [ThreadPool]: Databricks adapter: conn: 3116804560080: _release sess: c77b2f69-0d57-4d8e-a500-15c2ffa54d7c, name: list_hive_metastore_saleslt_dbt_test__audit, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (21500, 3252), cmpt: ``, lut: 1707844704.3843503
[0m00:18:24.384350 [debug] [ThreadPool]: Databricks adapter: conn: 3116804560080: idle check connection: sess: c77b2f69-0d57-4d8e-a500-15c2ffa54d7c, name: list_hive_metastore_saleslt_dbt_test__audit, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (21500, 3252), cmpt: ``, lut: 1707844704.3843503
[0m00:18:24.384350 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt_dbt_test__audit, now list_hive_metastore_snapshots)
[0m00:18:24.384350 [debug] [ThreadPool]: Databricks adapter: conn: 3116804560080: reusing connection list_hive_metastore_saleslt_dbt_test__audit sess: c77b2f69-0d57-4d8e-a500-15c2ffa54d7c, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (21500, 3252), cmpt: ``, lut: 1707844704.3843503
[0m00:18:24.384350 [debug] [ThreadPool]: Databricks adapter: Thread (21500, 3252) using default compute resource.
[0m00:18:24.384350 [debug] [ThreadPool]: Databricks adapter: conn: 3116804560080: _acquire sess: c77b2f69-0d57-4d8e-a500-15c2ffa54d7c, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (21500, 3252), cmpt: ``, lut: 1707844704.3843503
[0m00:18:24.384350 [debug] [ThreadPool]: Databricks adapter: conn: 3116804560080: get_thread_connection: sess: c77b2f69-0d57-4d8e-a500-15c2ffa54d7c, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (21500, 3252), cmpt: ``, lut: 1707844704.3843503
[0m00:18:24.384350 [debug] [ThreadPool]: Databricks adapter: conn: 3116804560080: idle check connection: sess: c77b2f69-0d57-4d8e-a500-15c2ffa54d7c, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (21500, 3252), cmpt: ``, lut: 1707844704.3843503
[0m00:18:24.384350 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:18:24.384350 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m00:18:24.510905 [debug] [ThreadPool]: SQL status: OK in 0.12999999523162842 seconds
[0m00:18:24.510905 [debug] [ThreadPool]: Databricks adapter: conn: 3116804560080: get_thread_connection: sess: c77b2f69-0d57-4d8e-a500-15c2ffa54d7c, name: list_hive_metastore_snapshots, idle: 0.1265547275543213s, acqrelcnt: 1, lang: None, thrd: (21500, 3252), cmpt: ``, lut: 1707844704.3843503
[0m00:18:24.510905 [debug] [ThreadPool]: Databricks adapter: conn: 3116804560080: idle check connection: sess: c77b2f69-0d57-4d8e-a500-15c2ffa54d7c, name: list_hive_metastore_snapshots, idle: 0.1265547275543213s, acqrelcnt: 1, lang: None, thrd: (21500, 3252), cmpt: ``, lut: 1707844704.3843503
[0m00:18:24.510905 [debug] [ThreadPool]: Databricks adapter: conn: 3116804560080: get_thread_connection: sess: c77b2f69-0d57-4d8e-a500-15c2ffa54d7c, name: list_hive_metastore_snapshots, idle: 0.1265547275543213s, acqrelcnt: 1, lang: None, thrd: (21500, 3252), cmpt: ``, lut: 1707844704.3843503
[0m00:18:24.510905 [debug] [ThreadPool]: Databricks adapter: conn: 3116804560080: idle check connection: sess: c77b2f69-0d57-4d8e-a500-15c2ffa54d7c, name: list_hive_metastore_snapshots, idle: 0.1265547275543213s, acqrelcnt: 1, lang: None, thrd: (21500, 3252), cmpt: ``, lut: 1707844704.3843503
[0m00:18:24.510905 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:18:24.510905 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:18:24.510905 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m00:18:24.658149 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m00:18:24.658149 [debug] [ThreadPool]: Databricks adapter: conn: 3116804560080: get_thread_connection: sess: c77b2f69-0d57-4d8e-a500-15c2ffa54d7c, name: list_hive_metastore_snapshots, idle: 0.27379941940307617s, acqrelcnt: 1, lang: None, thrd: (21500, 3252), cmpt: ``, lut: 1707844704.3843503
[0m00:18:24.658149 [debug] [ThreadPool]: Databricks adapter: conn: 3116804560080: idle check connection: sess: c77b2f69-0d57-4d8e-a500-15c2ffa54d7c, name: list_hive_metastore_snapshots, idle: 0.27379941940307617s, acqrelcnt: 1, lang: None, thrd: (21500, 3252), cmpt: ``, lut: 1707844704.3843503
[0m00:18:24.673635 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:18:24.673635 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m00:18:24.830025 [debug] [ThreadPool]: SQL status: OK in 0.1599999964237213 seconds
[0m00:18:24.830025 [debug] [ThreadPool]: Databricks adapter: conn: 3116804560080: _release sess: c77b2f69-0d57-4d8e-a500-15c2ffa54d7c, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (21500, 3252), cmpt: ``, lut: 1707844704.8300257
[0m00:18:24.830025 [debug] [ThreadPool]: Databricks adapter: conn: 3116804560080: idle check connection: sess: c77b2f69-0d57-4d8e-a500-15c2ffa54d7c, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (21500, 3252), cmpt: ``, lut: 1707844704.8300257
[0m00:18:24.830025 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now list_hive_metastore_saleslt)
[0m00:18:24.830025 [debug] [ThreadPool]: Databricks adapter: conn: 3116804560080: reusing connection list_hive_metastore_snapshots sess: c77b2f69-0d57-4d8e-a500-15c2ffa54d7c, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (21500, 3252), cmpt: ``, lut: 1707844704.8300257
[0m00:18:24.830025 [debug] [ThreadPool]: Databricks adapter: Thread (21500, 3252) using default compute resource.
[0m00:18:24.830025 [debug] [ThreadPool]: Databricks adapter: conn: 3116804560080: _acquire sess: c77b2f69-0d57-4d8e-a500-15c2ffa54d7c, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (21500, 3252), cmpt: ``, lut: 1707844704.8300257
[0m00:18:24.830025 [debug] [ThreadPool]: Databricks adapter: conn: 3116804560080: get_thread_connection: sess: c77b2f69-0d57-4d8e-a500-15c2ffa54d7c, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (21500, 3252), cmpt: ``, lut: 1707844704.8300257
[0m00:18:24.830025 [debug] [ThreadPool]: Databricks adapter: conn: 3116804560080: idle check connection: sess: c77b2f69-0d57-4d8e-a500-15c2ffa54d7c, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (21500, 3252), cmpt: ``, lut: 1707844704.8300257
[0m00:18:24.845511 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:18:24.846141 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m00:18:24.959938 [debug] [ThreadPool]: SQL status: OK in 0.10999999940395355 seconds
[0m00:18:24.959938 [debug] [ThreadPool]: Databricks adapter: conn: 3116804560080: get_thread_connection: sess: c77b2f69-0d57-4d8e-a500-15c2ffa54d7c, name: list_hive_metastore_saleslt, idle: 0.1299128532409668s, acqrelcnt: 1, lang: None, thrd: (21500, 3252), cmpt: ``, lut: 1707844704.8300257
[0m00:18:24.959938 [debug] [ThreadPool]: Databricks adapter: conn: 3116804560080: idle check connection: sess: c77b2f69-0d57-4d8e-a500-15c2ffa54d7c, name: list_hive_metastore_saleslt, idle: 0.1299128532409668s, acqrelcnt: 1, lang: None, thrd: (21500, 3252), cmpt: ``, lut: 1707844704.8300257
[0m00:18:24.959938 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:18:24.959938 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m00:18:25.091415 [debug] [ThreadPool]: SQL status: OK in 0.12999999523162842 seconds
[0m00:18:25.106978 [debug] [ThreadPool]: Databricks adapter: conn: 3116804560080: get_thread_connection: sess: c77b2f69-0d57-4d8e-a500-15c2ffa54d7c, name: list_hive_metastore_saleslt, idle: 0.27695322036743164s, acqrelcnt: 1, lang: None, thrd: (21500, 3252), cmpt: ``, lut: 1707844704.8300257
[0m00:18:25.106978 [debug] [ThreadPool]: Databricks adapter: conn: 3116804560080: idle check connection: sess: c77b2f69-0d57-4d8e-a500-15c2ffa54d7c, name: list_hive_metastore_saleslt, idle: 0.27695322036743164s, acqrelcnt: 1, lang: None, thrd: (21500, 3252), cmpt: ``, lut: 1707844704.8300257
[0m00:18:25.106978 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:18:25.106978 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m00:18:25.263301 [debug] [ThreadPool]: SQL status: OK in 0.1599999964237213 seconds
[0m00:18:25.263301 [debug] [ThreadPool]: Databricks adapter: conn: 3116804560080: _release sess: c77b2f69-0d57-4d8e-a500-15c2ffa54d7c, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (21500, 3252), cmpt: ``, lut: 1707844705.2633011
[0m00:18:25.263301 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c215719c-a35c-4941-95bc-8b40026c4544', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D5AFDF11D0>]}
[0m00:18:25.263301 [debug] [MainThread]: Databricks adapter: conn: 3116803034256: get_thread_connection: sess: None, name: master, idle: 2.557827949523926s, acqrelcnt: 1, lang: None, thrd: (21500, 23068), cmpt: ``, lut: 1707844702.7054732
[0m00:18:25.263301 [debug] [MainThread]: Databricks adapter: conn: 3116803034256: idle check connection: sess: None, name: master, idle: 2.557827949523926s, acqrelcnt: 1, lang: None, thrd: (21500, 23068), cmpt: ``, lut: 1707844702.7054732
[0m00:18:25.263301 [debug] [MainThread]: Databricks adapter: conn: 3116803034256: get_thread_connection: sess: None, name: master, idle: 2.557827949523926s, acqrelcnt: 1, lang: None, thrd: (21500, 23068), cmpt: ``, lut: 1707844702.7054732
[0m00:18:25.263301 [debug] [MainThread]: Databricks adapter: conn: 3116803034256: idle check connection: sess: None, name: master, idle: 2.557827949523926s, acqrelcnt: 1, lang: None, thrd: (21500, 23068), cmpt: ``, lut: 1707844702.7054732
[0m00:18:25.263301 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:18:25.278809 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:18:25.278809 [debug] [MainThread]: Databricks adapter: conn: 3116803034256: _release sess: None, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (21500, 23068), cmpt: ``, lut: 1707844705.278809
[0m00:18:25.278809 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:18:25.278809 [info ] [MainThread]: 
[0m00:18:25.278809 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:18:25.278809 [info ] [Thread-1 (]: 1 of 8 START test not_null_dim_product_product_name ............................ [RUN]
[0m00:18:25.288487 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: Creating DatabricksDBTConnection sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0s, acqrelcnt: 0, lang: None, thrd: (21500, 10576), cmpt: ``, lut: None
[0m00:18:25.288487 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5'
[0m00:18:25.289562 [debug] [Thread-1 (]: Databricks adapter: On thread (21500, 10576): None using default compute resource.
[0m00:18:25.289562 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: _acquire sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844705.2895627
[0m00:18:25.290615 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:18:25.301661 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:18:25.301661 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (compile): 00:18:25.290615 => 00:18:25.301661
[0m00:18:25.301661 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:18:25.315223 [debug] [Thread-1 (]: Applying CREATE to: `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_dim_product_product_name`
[0m00:18:25.361521 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.07195854187011719s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844705.2895627
[0m00:18:25.363331 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.07295656204223633s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844705.2895627
[0m00:18:25.363331 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.07376909255981445s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844705.2895627
[0m00:18:25.363331 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.07376909255981445s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844705.2895627
[0m00:18:25.364342 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:18:25.364342 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:18:25.365396 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"} */

        
  
    
        create or replace table `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_dim_product_product_name`
      
      
    using delta
      
      
      
      
      
      
      
      as
      
    
    



select *
from `hive_metastore`.`saleslt`.`dim_product`
where product_name is null



  
    
[0m00:18:25.365396 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:18:25.593996 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: session opened sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844705.5939968
[0m00:18:28.148204 [debug] [Thread-1 (]: SQL status: OK in 2.7799999713897705 seconds
[0m00:18:28.148204 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:18:28.148204 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:18:28.163680 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: get_thread_connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 2.5542080402374268s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844705.5939968
[0m00:18:28.163680 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: idle check connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 2.569683313369751s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844705.5939968
[0m00:18:28.163680 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:18:28.163680 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
        select *
        from `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_dim_product_product_name`
    
      
    ) dbt_internal_test
[0m00:18:28.559605 [debug] [Thread-1 (]: SQL status: OK in 0.4000000059604645 seconds
[0m00:18:28.575179 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (execute): 00:18:25.301661 => 00:18:28.575179
[0m00:18:28.575179 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: _release sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844708.5751796
[0m00:18:28.575179 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: _release sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844708.5751796
[0m00:18:28.575179 [info ] [Thread-1 (]: 1 of 8 PASS not_null_dim_product_product_name .................................. [[32mPASS[0m in 3.30s]
[0m00:18:28.575179 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:18:28.575179 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:18:28.575179 [info ] [Thread-1 (]: 2 of 8 START test not_null_dim_product_product_sk .............................. [RUN]
[0m00:18:28.575179 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: idle check connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844708.5751796
[0m00:18:28.575179 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, now test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417)
[0m00:18:28.575179 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844708.5751796
[0m00:18:28.575179 [debug] [Thread-1 (]: Databricks adapter: On thread (21500, 10576): None using default compute resource.
[0m00:18:28.575179 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: _acquire sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844708.5751796
[0m00:18:28.575179 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:18:28.575179 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:18:28.575179 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (compile): 00:18:28.575179 => 00:18:28.575179
[0m00:18:28.575179 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:18:28.590720 [debug] [Thread-1 (]: Applying CREATE to: `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_dim_product_product_sk`
[0m00:18:28.595859 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: get_thread_connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.02068018913269043s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844708.5751796
[0m00:18:28.596824 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: idle check connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.021645069122314453s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844708.5751796
[0m00:18:28.597756 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:18:28.597756 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"} */

        
  
    
        create or replace table `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_dim_product_product_sk`
      
      
    using delta
      
      
      
      
      
      
      
      as
      
    
    



select *
from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is null



  
    
[0m00:18:31.003882 [debug] [Thread-1 (]: SQL status: OK in 2.4100000858306885 seconds
[0m00:18:31.003882 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:18:31.003882 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:18:31.019390 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: get_thread_connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 2.4287030696868896s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844708.5751796
[0m00:18:31.019390 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: idle check connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 2.4442102909088135s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844708.5751796
[0m00:18:31.019390 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:18:31.019390 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
        select *
        from `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_dim_product_product_sk`
    
      
    ) dbt_internal_test
[0m00:18:31.318807 [debug] [Thread-1 (]: SQL status: OK in 0.30000001192092896 seconds
[0m00:18:31.318807 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (execute): 00:18:28.575179 => 00:18:31.318807
[0m00:18:31.318807 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: _release sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844711.3188074
[0m00:18:31.318807 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: _release sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844711.3188074
[0m00:18:31.318807 [info ] [Thread-1 (]: 2 of 8 PASS not_null_dim_product_product_sk .................................... [[32mPASS[0m in 2.74s]
[0m00:18:31.318807 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:18:31.318807 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:18:31.318807 [info ] [Thread-1 (]: 3 of 8 START test not_null_dim_product_sellstartdate ........................... [RUN]
[0m00:18:31.318807 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: idle check connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844711.3188074
[0m00:18:31.318807 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, now test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546)
[0m00:18:31.318807 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844711.3188074
[0m00:18:31.318807 [debug] [Thread-1 (]: Databricks adapter: On thread (21500, 10576): None using default compute resource.
[0m00:18:31.318807 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: _acquire sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844711.3188074
[0m00:18:31.318807 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:18:31.336775 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:18:31.336908 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (compile): 00:18:31.318807 => 00:18:31.336908
[0m00:18:31.336908 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:18:31.341211 [debug] [Thread-1 (]: Applying CREATE to: `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_dim_product_sellstartdate`
[0m00:18:31.341211 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: get_thread_connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.022403955459594727s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844711.3188074
[0m00:18:31.341211 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: idle check connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.022403955459594727s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844711.3188074
[0m00:18:31.341211 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:18:31.341211 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"} */

        
  
    
        create or replace table `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_dim_product_sellstartdate`
      
      
    using delta
      
      
      
      
      
      
      
      as
      
    
    



select *
from `hive_metastore`.`saleslt`.`dim_product`
where sellstartdate is null



  
    
[0m00:18:33.113068 [debug] [Thread-1 (]: SQL status: OK in 1.7699999809265137 seconds
[0m00:18:33.128552 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:18:33.128552 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:18:33.128552 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: get_thread_connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 1.8097448348999023s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844711.3188074
[0m00:18:33.128552 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: idle check connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 1.8097448348999023s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844711.3188074
[0m00:18:33.128552 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:18:33.128552 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
        select *
        from `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_dim_product_sellstartdate`
    
      
    ) dbt_internal_test
[0m00:18:33.378362 [debug] [Thread-1 (]: SQL status: OK in 0.25 seconds
[0m00:18:33.378362 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (execute): 00:18:31.336908 => 00:18:33.378362
[0m00:18:33.378362 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: _release sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844713.3783627
[0m00:18:33.378362 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: _release sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844713.3783627
[0m00:18:33.378362 [info ] [Thread-1 (]: 3 of 8 PASS not_null_dim_product_sellstartdate ................................. [[32mPASS[0m in 2.06s]
[0m00:18:33.378362 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:18:33.378362 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:18:33.378362 [info ] [Thread-1 (]: 4 of 8 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m00:18:33.393836 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: idle check connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.015474081039428711s, acqrelcnt: 0, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844713.3783627
[0m00:18:33.393836 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, now test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710)
[0m00:18:33.393836 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: reusing connection test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.015474081039428711s, acqrelcnt: 0, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844713.3783627
[0m00:18:33.393836 [debug] [Thread-1 (]: Databricks adapter: On thread (21500, 10576): None using default compute resource.
[0m00:18:33.393836 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: _acquire sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.015474081039428711s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844713.3783627
[0m00:18:33.393836 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:18:33.393836 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:18:33.393836 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 00:18:33.393836 => 00:18:33.393836
[0m00:18:33.393836 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:18:33.393836 [debug] [Thread-1 (]: Applying CREATE to: `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_my_first_dbt_model_id`
[0m00:18:33.393836 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: get_thread_connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.015474081039428711s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844713.3783627
[0m00:18:33.393836 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: idle check connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.015474081039428711s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844713.3783627
[0m00:18:33.409463 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:18:33.409463 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"} */

        
  
    
        create or replace table `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_my_first_dbt_model_id`
      
      
    using delta
      
      
      
      
      
      
      
      as
      
    
    



select *
from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is null



  
    
[0m00:18:35.625147 [debug] [Thread-1 (]: SQL status: OK in 2.2100000381469727 seconds
[0m00:18:35.625147 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:18:35.625147 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:18:35.625147 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: get_thread_connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 2.2467844486236572s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844713.3783627
[0m00:18:35.625147 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: idle check connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 2.2467844486236572s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844713.3783627
[0m00:18:35.625147 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:18:35.625147 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
        select *
        from `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_my_first_dbt_model_id`
    
      
    ) dbt_internal_test
[0m00:18:36.013871 [debug] [Thread-1 (]: SQL status: OK in 0.38999998569488525 seconds
[0m00:18:36.013871 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 00:18:33.393836 => 00:18:36.013871
[0m00:18:36.013871 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: _release sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844716.0138717
[0m00:18:36.013871 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: _release sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844716.0138717
[0m00:18:36.013871 [error] [Thread-1 (]: 4 of 8 FAIL 1 not_null_my_first_dbt_model_id ................................... [[31mFAIL 1[0m in 2.62s]
[0m00:18:36.013871 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:18:36.013871 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:18:36.013871 [info ] [Thread-1 (]: 5 of 8 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m00:18:36.013871 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: idle check connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844716.0138717
[0m00:18:36.013871 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, now test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778)
[0m00:18:36.013871 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: reusing connection test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844716.0138717
[0m00:18:36.013871 [debug] [Thread-1 (]: Databricks adapter: On thread (21500, 10576): None using default compute resource.
[0m00:18:36.013871 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: _acquire sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844716.0138717
[0m00:18:36.013871 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:18:36.033655 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:18:36.034988 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (compile): 00:18:36.013871 => 00:18:36.034639
[0m00:18:36.035962 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:18:36.038905 [debug] [Thread-1 (]: Applying CREATE to: `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_my_second_dbt_model_id`
[0m00:18:36.042628 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: get_thread_connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.02875685691833496s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844716.0138717
[0m00:18:36.043628 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: idle check connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.02875685691833496s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844716.0138717
[0m00:18:36.043628 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:18:36.043628 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"} */

        
  
    
        create or replace table `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_my_second_dbt_model_id`
      
      
    using delta
      
      
      
      
      
      
      
      as
      
    
    



select *
from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is null



  
    
[0m00:18:38.112954 [debug] [Thread-1 (]: SQL status: OK in 2.069999933242798 seconds
[0m00:18:38.112954 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:18:38.112954 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:18:38.112954 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: get_thread_connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 2.0990824699401855s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844716.0138717
[0m00:18:38.112954 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: idle check connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 2.0990824699401855s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844716.0138717
[0m00:18:38.112954 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:18:38.112954 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
        select *
        from `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_my_second_dbt_model_id`
    
      
    ) dbt_internal_test
[0m00:18:38.421262 [debug] [Thread-1 (]: SQL status: OK in 0.3100000023841858 seconds
[0m00:18:38.421262 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (execute): 00:18:36.036310 => 00:18:38.421262
[0m00:18:38.421262 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: _release sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844718.4212623
[0m00:18:38.421262 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: _release sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844718.4212623
[0m00:18:38.421262 [info ] [Thread-1 (]: 5 of 8 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 2.41s]
[0m00:18:38.421262 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:18:38.421262 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:18:38.421262 [info ] [Thread-1 (]: 6 of 8 START test unique_dim_product_product_sk ................................ [RUN]
[0m00:18:38.421262 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: idle check connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844718.4212623
[0m00:18:38.421262 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, now test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c)
[0m00:18:38.421262 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: reusing connection test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844718.4212623
[0m00:18:38.421262 [debug] [Thread-1 (]: Databricks adapter: On thread (21500, 10576): None using default compute resource.
[0m00:18:38.421262 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: _acquire sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844718.4212623
[0m00:18:38.421262 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:18:38.445182 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:18:38.446160 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (compile): 00:18:38.436797 => 00:18:38.446160
[0m00:18:38.447303 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:18:38.450308 [debug] [Thread-1 (]: Applying CREATE to: `hive_metastore`.`saleslt_dbt_test__audit`.`unique_dim_product_product_sk`
[0m00:18:38.452578 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: get_thread_connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.031316280364990234s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844718.4212623
[0m00:18:38.452578 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: idle check connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.031316280364990234s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844718.4212623
[0m00:18:38.452578 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:18:38.452578 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"} */

        
  
    
        create or replace table `hive_metastore`.`saleslt_dbt_test__audit`.`unique_dim_product_product_sk`
      
      
    using delta
      
      
      
      
      
      
      
      as
      
    
    

select
    product_sk as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is not null
group by product_sk
having count(*) > 1



  
    
[0m00:18:40.412159 [debug] [Thread-1 (]: SQL status: OK in 1.9600000381469727 seconds
[0m00:18:40.412159 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:18:40.412159 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:18:40.412159 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: get_thread_connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 1.9908971786499023s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844718.4212623
[0m00:18:40.412159 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: idle check connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 1.9908971786499023s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844718.4212623
[0m00:18:40.412159 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:18:40.412159 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
        select *
        from `hive_metastore`.`saleslt_dbt_test__audit`.`unique_dim_product_product_sk`
    
      
    ) dbt_internal_test
[0m00:18:40.637509 [debug] [Thread-1 (]: SQL status: OK in 0.23000000417232513 seconds
[0m00:18:40.637509 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (execute): 00:18:38.447303 => 00:18:40.637509
[0m00:18:40.637509 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: _release sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844720.6375096
[0m00:18:40.637509 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: _release sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844720.6375096
[0m00:18:40.637509 [info ] [Thread-1 (]: 6 of 8 PASS unique_dim_product_product_sk ...................................... [[32mPASS[0m in 2.22s]
[0m00:18:40.637509 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:18:40.637509 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:18:40.637509 [info ] [Thread-1 (]: 7 of 8 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m00:18:40.637509 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: idle check connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844720.6375096
[0m00:18:40.637509 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, now test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321)
[0m00:18:40.637509 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: reusing connection test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844720.6375096
[0m00:18:40.637509 [debug] [Thread-1 (]: Databricks adapter: On thread (21500, 10576): None using default compute resource.
[0m00:18:40.653123 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: _acquire sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.015614032745361328s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844720.6375096
[0m00:18:40.653990 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:18:40.658808 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:18:40.658808 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (compile): 00:18:40.653990 => 00:18:40.658808
[0m00:18:40.658808 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:18:40.663345 [debug] [Thread-1 (]: Applying CREATE to: `hive_metastore`.`saleslt_dbt_test__audit`.`unique_my_first_dbt_model_id`
[0m00:18:40.665852 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: get_thread_connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.028342485427856445s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844720.6375096
[0m00:18:40.665852 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: idle check connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.028342485427856445s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844720.6375096
[0m00:18:40.665852 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:18:40.665852 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"} */

        
  
    
        create or replace table `hive_metastore`.`saleslt_dbt_test__audit`.`unique_my_first_dbt_model_id`
      
      
    using delta
      
      
      
      
      
      
      
      as
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



  
    
[0m00:18:42.819183 [debug] [Thread-1 (]: SQL status: OK in 2.1500000953674316 seconds
[0m00:18:42.819183 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:18:42.819183 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:18:42.819183 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: get_thread_connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 2.181674003601074s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844720.6375096
[0m00:18:42.819183 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: idle check connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 2.181674003601074s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844720.6375096
[0m00:18:42.829748 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:18:42.829748 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
        select *
        from `hive_metastore`.`saleslt_dbt_test__audit`.`unique_my_first_dbt_model_id`
    
      
    ) dbt_internal_test
[0m00:18:43.112583 [debug] [Thread-1 (]: SQL status: OK in 0.2800000011920929 seconds
[0m00:18:43.115557 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (execute): 00:18:40.658808 => 00:18:43.115557
[0m00:18:43.116747 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: _release sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844723.1165545
[0m00:18:43.116747 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: _release sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844723.116747
[0m00:18:43.116747 [info ] [Thread-1 (]: 7 of 8 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 2.48s]
[0m00:18:43.116747 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:18:43.116747 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:18:43.116747 [info ] [Thread-1 (]: 8 of 8 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m00:18:43.116747 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: idle check connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844723.116747
[0m00:18:43.116747 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, now test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493)
[0m00:18:43.116747 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: reusing connection test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844723.116747
[0m00:18:43.116747 [debug] [Thread-1 (]: Databricks adapter: On thread (21500, 10576): None using default compute resource.
[0m00:18:43.116747 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: _acquire sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844723.116747
[0m00:18:43.116747 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:18:43.116747 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:18:43.116747 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (compile): 00:18:43.116747 => 00:18:43.116747
[0m00:18:43.116747 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:18:43.132485 [debug] [Thread-1 (]: Applying CREATE to: `hive_metastore`.`saleslt_dbt_test__audit`.`unique_my_second_dbt_model_id`
[0m00:18:43.136927 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: get_thread_connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.01967644691467285s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844723.116747
[0m00:18:43.137225 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: idle check connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.020478248596191406s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844723.116747
[0m00:18:43.138250 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:18:43.138250 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"} */

        
  
    
        create or replace table `hive_metastore`.`saleslt_dbt_test__audit`.`unique_my_second_dbt_model_id`
      
      
    using delta
      
      
      
      
      
      
      
      as
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



  
    
[0m00:18:45.146857 [debug] [Thread-1 (]: SQL status: OK in 2.009999990463257 seconds
[0m00:18:45.148822 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:18:45.149804 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:18:45.149804 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: get_thread_connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 2.033057928085327s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844723.116747
[0m00:18:45.150774 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: idle check connection: sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 2.0340278148651123s, acqrelcnt: 1, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844723.116747
[0m00:18:45.151531 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:18:45.151599 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
        select *
        from `hive_metastore`.`saleslt_dbt_test__audit`.`unique_my_second_dbt_model_id`
    
      
    ) dbt_internal_test
[0m00:18:45.383756 [debug] [Thread-1 (]: SQL status: OK in 0.23000000417232513 seconds
[0m00:18:45.383756 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (execute): 00:18:43.116747 => 00:18:45.383756
[0m00:18:45.383756 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: _release sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844725.383756
[0m00:18:45.383756 [debug] [Thread-1 (]: Databricks adapter: conn: 3116806193488: _release sess: 45c5f794-c08d-4ea8-8a06-3d9ed8085eee, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (21500, 10576), cmpt: ``, lut: 1707844725.383756
[0m00:18:45.383756 [info ] [Thread-1 (]: 8 of 8 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 2.27s]
[0m00:18:45.383756 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:18:45.383756 [debug] [MainThread]: Databricks adapter: conn: 3116803034256: idle check connection: sess: None, name: master, idle: 20.104946851730347s, acqrelcnt: 0, lang: None, thrd: (21500, 23068), cmpt: ``, lut: 1707844705.278809
[0m00:18:45.383756 [debug] [MainThread]: Databricks adapter: conn: 3116803034256: reusing connection master sess: None, name: master, idle: 20.104946851730347s, acqrelcnt: 0, lang: None, thrd: (21500, 23068), cmpt: ``, lut: 1707844705.278809
[0m00:18:45.383756 [debug] [MainThread]: Databricks adapter: Thread (21500, 23068) using default compute resource.
[0m00:18:45.383756 [debug] [MainThread]: Databricks adapter: conn: 3116803034256: _acquire sess: None, name: master, idle: 20.104946851730347s, acqrelcnt: 1, lang: None, thrd: (21500, 23068), cmpt: ``, lut: 1707844705.278809
[0m00:18:45.399295 [debug] [MainThread]: Databricks adapter: conn: 3116803034256: get_thread_connection: sess: None, name: master, idle: 20.12048649787903s, acqrelcnt: 1, lang: None, thrd: (21500, 23068), cmpt: ``, lut: 1707844705.278809
[0m00:18:45.399295 [debug] [MainThread]: Databricks adapter: conn: 3116803034256: idle check connection: sess: None, name: master, idle: 20.12048649787903s, acqrelcnt: 1, lang: None, thrd: (21500, 23068), cmpt: ``, lut: 1707844705.278809
[0m00:18:45.400389 [debug] [MainThread]: On master: ROLLBACK
[0m00:18:45.401453 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:18:45.630013 [debug] [MainThread]: Databricks adapter: conn: 3116803034256: session opened sess: f7f1aa97-cf72-430e-aac8-76252ba40497, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (21500, 23068), cmpt: ``, lut: 1707844725.6300132
[0m00:18:45.632846 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:18:45.632846 [debug] [MainThread]: Databricks adapter: conn: 3116803034256: get_thread_connection: sess: f7f1aa97-cf72-430e-aac8-76252ba40497, name: master, idle: 0.0028333663940429688s, acqrelcnt: 1, lang: None, thrd: (21500, 23068), cmpt: ``, lut: 1707844725.6300132
[0m00:18:45.632846 [debug] [MainThread]: Databricks adapter: conn: 3116803034256: idle check connection: sess: f7f1aa97-cf72-430e-aac8-76252ba40497, name: master, idle: 0.0028333663940429688s, acqrelcnt: 1, lang: None, thrd: (21500, 23068), cmpt: ``, lut: 1707844725.6300132
[0m00:18:45.632846 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:18:45.632846 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:18:45.632846 [debug] [MainThread]: Databricks adapter: conn: 3116803034256: _release sess: f7f1aa97-cf72-430e-aac8-76252ba40497, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (21500, 23068), cmpt: ``, lut: 1707844725.6328466
[0m00:18:45.632846 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:18:45.632846 [debug] [MainThread]: On master: ROLLBACK
[0m00:18:45.632846 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:18:45.632846 [debug] [MainThread]: On master: Close
[0m00:18:45.717296 [debug] [MainThread]: Connection 'create_hive_metastore_saleslt_dbt_test__audit' was properly closed.
[0m00:18:45.717296 [debug] [MainThread]: On create_hive_metastore_saleslt_dbt_test__audit: ROLLBACK
[0m00:18:45.717296 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:18:45.717296 [debug] [MainThread]: On create_hive_metastore_saleslt_dbt_test__audit: Close
[0m00:18:45.795419 [debug] [MainThread]: Connection 'list_hive_metastore_saleslt' was properly closed.
[0m00:18:45.795419 [debug] [MainThread]: On list_hive_metastore_saleslt: ROLLBACK
[0m00:18:45.795419 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:18:45.795419 [debug] [MainThread]: On list_hive_metastore_saleslt: Close
[0m00:18:45.895758 [debug] [MainThread]: Connection 'test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493' was properly closed.
[0m00:18:45.895758 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: ROLLBACK
[0m00:18:45.895758 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:18:45.895758 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m00:18:45.976251 [info ] [MainThread]: 
[0m00:18:45.976251 [info ] [MainThread]: Finished running 8 tests in 0 hours 0 minutes and 23.27 seconds (23.27s).
[0m00:18:45.983743 [debug] [MainThread]: Command end result
[0m00:18:45.983743 [info ] [MainThread]: 
[0m00:18:45.999425 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:18:46.000671 [info ] [MainThread]: 
[0m00:18:46.002638 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models\example\schema.yml)[0m
[0m00:18:46.003645 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m00:18:46.004779 [info ] [MainThread]: 
[0m00:18:46.004779 [info ] [MainThread]:   compiled Code at target\compiled\medallion_dbt_spark\models\example\schema.yml\not_null_my_first_dbt_model_id.sql
[0m00:18:46.004779 [info ] [MainThread]: 
[0m00:18:46.004779 [info ] [MainThread]:   See test failures:
  -----------------------------------------------------------------------------------------
  select * from `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_my_first_dbt_model_id`
  -----------------------------------------------------------------------------------------
[0m00:18:46.009976 [info ] [MainThread]: 
[0m00:18:46.010986 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=1 SKIP=0 TOTAL=8
[0m00:18:46.011987 [debug] [MainThread]: Command `dbt test` failed at 00:18:46.011987 after 24.81 seconds
[0m00:18:46.013005 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D59D6CE110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D5968C6250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D5968C5FD0>]}
[0m00:18:46.013983 [debug] [MainThread]: Flushing usage events
[0m00:20:49.880474 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4AA7A2510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4A7328090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4AAD58ED0>]}


============================== 00:20:49.880474 | 44be457a-c66f-4f95-acec-3858a5929f5e ==============================
[0m00:20:49.880474 [info ] [MainThread]: Running with dbt=1.7.7
[0m00:20:49.880474 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt test', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m00:20:50.997618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '44be457a-c66f-4f95-acec-3858a5929f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4BCE29B10>]}
[0m00:20:51.060138 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '44be457a-c66f-4f95-acec-3858a5929f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4A9907790>]}
[0m00:20:51.060138 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m00:20:51.075742 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m00:20:51.164981 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m00:20:51.166025 [debug] [MainThread]: Partial parsing: updated file: medallion_dbt_spark://models\marts\sales\dim_sales.yml
[0m00:20:51.181725 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '44be457a-c66f-4f95-acec-3858a5929f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4BCED7650>]}
[0m00:20:51.197363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '44be457a-c66f-4f95-acec-3858a5929f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4BCFBB0D0>]}
[0m00:20:51.197363 [info ] [MainThread]: Found 5 models, 7 snapshots, 8 tests, 9 sources, 0 exposures, 0 metrics, 535 macros, 0 groups, 0 semantic models
[0m00:20:51.197363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '44be457a-c66f-4f95-acec-3858a5929f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4BD0A1250>]}
[0m00:20:51.197363 [info ] [MainThread]: 
[0m00:20:51.197363 [debug] [MainThread]: Databricks adapter: conn: 2081935724368: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (12228, 25972), cmpt: ``, lut: None
[0m00:20:51.197363 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:20:51.197363 [debug] [MainThread]: Databricks adapter: Thread (12228, 25972) using default compute resource.
[0m00:20:51.197363 [debug] [MainThread]: Databricks adapter: conn: 2081935724368: _acquire sess: None, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (12228, 25972), cmpt: ``, lut: 1707844851.1973636
[0m00:20:51.212935 [debug] [ThreadPool]: Databricks adapter: conn: 2081934773456: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_snapshots, idle: 0s, acqrelcnt: 0, lang: None, thrd: (12228, 23316), cmpt: ``, lut: None
[0m00:20:51.212935 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_snapshots'
[0m00:20:51.212935 [debug] [ThreadPool]: Databricks adapter: Thread (12228, 23316) using default compute resource.
[0m00:20:51.212935 [debug] [ThreadPool]: Databricks adapter: conn: 2081934773456: _acquire sess: None, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (12228, 23316), cmpt: ``, lut: 1707844851.212935
[0m00:20:51.212935 [debug] [ThreadPool]: Databricks adapter: conn: 2081934773456: get_thread_connection: sess: None, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (12228, 23316), cmpt: ``, lut: 1707844851.212935
[0m00:20:51.212935 [debug] [ThreadPool]: Databricks adapter: conn: 2081934773456: idle check connection: sess: None, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (12228, 23316), cmpt: ``, lut: 1707844851.212935
[0m00:20:51.212935 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:20:51.212935 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m00:20:51.212935 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:20:51.595734 [debug] [ThreadPool]: Databricks adapter: conn: 2081934773456: session opened sess: 2adeddf0-7bc3-421e-b008-076f343a246f, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (12228, 23316), cmpt: ``, lut: 1707844851.5957344
[0m00:20:52.001144 [debug] [ThreadPool]: SQL status: OK in 0.7900000214576721 seconds
[0m00:20:52.022283 [debug] [ThreadPool]: Databricks adapter: conn: 2081934773456: get_thread_connection: sess: 2adeddf0-7bc3-421e-b008-076f343a246f, name: list_hive_metastore_snapshots, idle: 0.42654943466186523s, acqrelcnt: 1, lang: None, thrd: (12228, 23316), cmpt: ``, lut: 1707844851.5957344
[0m00:20:52.023286 [debug] [ThreadPool]: Databricks adapter: conn: 2081934773456: idle check connection: sess: 2adeddf0-7bc3-421e-b008-076f343a246f, name: list_hive_metastore_snapshots, idle: 0.4275522232055664s, acqrelcnt: 1, lang: None, thrd: (12228, 23316), cmpt: ``, lut: 1707844851.5957344
[0m00:20:52.023286 [debug] [ThreadPool]: Databricks adapter: conn: 2081934773456: get_thread_connection: sess: 2adeddf0-7bc3-421e-b008-076f343a246f, name: list_hive_metastore_snapshots, idle: 0.4275522232055664s, acqrelcnt: 1, lang: None, thrd: (12228, 23316), cmpt: ``, lut: 1707844851.5957344
[0m00:20:52.024229 [debug] [ThreadPool]: Databricks adapter: conn: 2081934773456: idle check connection: sess: 2adeddf0-7bc3-421e-b008-076f343a246f, name: list_hive_metastore_snapshots, idle: 0.428494930267334s, acqrelcnt: 1, lang: None, thrd: (12228, 23316), cmpt: ``, lut: 1707844851.5957344
[0m00:20:52.024229 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:20:52.025218 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:20:52.025218 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m00:20:52.236309 [debug] [ThreadPool]: SQL status: OK in 0.20999999344348907 seconds
[0m00:20:52.251843 [debug] [ThreadPool]: Databricks adapter: conn: 2081934773456: get_thread_connection: sess: 2adeddf0-7bc3-421e-b008-076f343a246f, name: list_hive_metastore_snapshots, idle: 0.6561093330383301s, acqrelcnt: 1, lang: None, thrd: (12228, 23316), cmpt: ``, lut: 1707844851.5957344
[0m00:20:52.251843 [debug] [ThreadPool]: Databricks adapter: conn: 2081934773456: idle check connection: sess: 2adeddf0-7bc3-421e-b008-076f343a246f, name: list_hive_metastore_snapshots, idle: 0.6561093330383301s, acqrelcnt: 1, lang: None, thrd: (12228, 23316), cmpt: ``, lut: 1707844851.5957344
[0m00:20:52.251843 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:20:52.251843 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m00:20:52.400418 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m00:20:52.400418 [debug] [ThreadPool]: Databricks adapter: conn: 2081934773456: _release sess: 2adeddf0-7bc3-421e-b008-076f343a246f, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (12228, 23316), cmpt: ``, lut: 1707844852.4004188
[0m00:20:52.400418 [debug] [ThreadPool]: Databricks adapter: conn: 2081934773456: idle check connection: sess: 2adeddf0-7bc3-421e-b008-076f343a246f, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (12228, 23316), cmpt: ``, lut: 1707844852.4004188
[0m00:20:52.400418 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now list_hive_metastore_saleslt)
[0m00:20:52.400418 [debug] [ThreadPool]: Databricks adapter: conn: 2081934773456: reusing connection list_hive_metastore_snapshots sess: 2adeddf0-7bc3-421e-b008-076f343a246f, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (12228, 23316), cmpt: ``, lut: 1707844852.4004188
[0m00:20:52.400418 [debug] [ThreadPool]: Databricks adapter: Thread (12228, 23316) using default compute resource.
[0m00:20:52.400418 [debug] [ThreadPool]: Databricks adapter: conn: 2081934773456: _acquire sess: 2adeddf0-7bc3-421e-b008-076f343a246f, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (12228, 23316), cmpt: ``, lut: 1707844852.4004188
[0m00:20:52.415909 [debug] [ThreadPool]: Databricks adapter: conn: 2081934773456: get_thread_connection: sess: 2adeddf0-7bc3-421e-b008-076f343a246f, name: list_hive_metastore_saleslt, idle: 0.015490293502807617s, acqrelcnt: 1, lang: None, thrd: (12228, 23316), cmpt: ``, lut: 1707844852.4004188
[0m00:20:52.415909 [debug] [ThreadPool]: Databricks adapter: conn: 2081934773456: idle check connection: sess: 2adeddf0-7bc3-421e-b008-076f343a246f, name: list_hive_metastore_saleslt, idle: 0.015490293502807617s, acqrelcnt: 1, lang: None, thrd: (12228, 23316), cmpt: ``, lut: 1707844852.4004188
[0m00:20:52.415909 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:20:52.415909 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m00:20:52.509788 [debug] [ThreadPool]: SQL status: OK in 0.09000000357627869 seconds
[0m00:20:52.526571 [debug] [ThreadPool]: Databricks adapter: conn: 2081934773456: get_thread_connection: sess: 2adeddf0-7bc3-421e-b008-076f343a246f, name: list_hive_metastore_saleslt, idle: 0.12615251541137695s, acqrelcnt: 1, lang: None, thrd: (12228, 23316), cmpt: ``, lut: 1707844852.4004188
[0m00:20:52.526571 [debug] [ThreadPool]: Databricks adapter: conn: 2081934773456: idle check connection: sess: 2adeddf0-7bc3-421e-b008-076f343a246f, name: list_hive_metastore_saleslt, idle: 0.12615251541137695s, acqrelcnt: 1, lang: None, thrd: (12228, 23316), cmpt: ``, lut: 1707844852.4004188
[0m00:20:52.526571 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:20:52.526571 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m00:20:52.659855 [debug] [ThreadPool]: SQL status: OK in 0.12999999523162842 seconds
[0m00:20:52.659855 [debug] [ThreadPool]: Databricks adapter: conn: 2081934773456: get_thread_connection: sess: 2adeddf0-7bc3-421e-b008-076f343a246f, name: list_hive_metastore_saleslt, idle: 0.25943660736083984s, acqrelcnt: 1, lang: None, thrd: (12228, 23316), cmpt: ``, lut: 1707844852.4004188
[0m00:20:52.659855 [debug] [ThreadPool]: Databricks adapter: conn: 2081934773456: idle check connection: sess: 2adeddf0-7bc3-421e-b008-076f343a246f, name: list_hive_metastore_saleslt, idle: 0.25943660736083984s, acqrelcnt: 1, lang: None, thrd: (12228, 23316), cmpt: ``, lut: 1707844852.4004188
[0m00:20:52.659855 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:20:52.659855 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m00:20:52.847391 [debug] [ThreadPool]: SQL status: OK in 0.1899999976158142 seconds
[0m00:20:52.847391 [debug] [ThreadPool]: Databricks adapter: conn: 2081934773456: _release sess: 2adeddf0-7bc3-421e-b008-076f343a246f, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (12228, 23316), cmpt: ``, lut: 1707844852.8473916
[0m00:20:52.862942 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '44be457a-c66f-4f95-acec-3858a5929f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4BCD8B310>]}
[0m00:20:52.862942 [debug] [MainThread]: Databricks adapter: conn: 2081935724368: get_thread_connection: sess: None, name: master, idle: 1.6655786037445068s, acqrelcnt: 1, lang: None, thrd: (12228, 25972), cmpt: ``, lut: 1707844851.1973636
[0m00:20:52.862942 [debug] [MainThread]: Databricks adapter: conn: 2081935724368: idle check connection: sess: None, name: master, idle: 1.6655786037445068s, acqrelcnt: 1, lang: None, thrd: (12228, 25972), cmpt: ``, lut: 1707844851.1973636
[0m00:20:52.862942 [debug] [MainThread]: Databricks adapter: conn: 2081935724368: get_thread_connection: sess: None, name: master, idle: 1.6655786037445068s, acqrelcnt: 1, lang: None, thrd: (12228, 25972), cmpt: ``, lut: 1707844851.1973636
[0m00:20:52.862942 [debug] [MainThread]: Databricks adapter: conn: 2081935724368: idle check connection: sess: None, name: master, idle: 1.6655786037445068s, acqrelcnt: 1, lang: None, thrd: (12228, 25972), cmpt: ``, lut: 1707844851.1973636
[0m00:20:52.862942 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:20:52.862942 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:20:52.862942 [debug] [MainThread]: Databricks adapter: conn: 2081935724368: _release sess: None, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (12228, 25972), cmpt: ``, lut: 1707844852.8629422
[0m00:20:52.862942 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:20:52.862942 [info ] [MainThread]: 
[0m00:20:52.862942 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:20:52.862942 [info ] [Thread-1 (]: 1 of 8 START test not_null_dim_product_product_name ............................ [RUN]
[0m00:20:52.874320 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: Creating DatabricksDBTConnection sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0s, acqrelcnt: 0, lang: None, thrd: (12228, 25144), cmpt: ``, lut: None
[0m00:20:52.874320 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5'
[0m00:20:52.877433 [debug] [Thread-1 (]: Databricks adapter: On thread (12228, 25144): None using default compute resource.
[0m00:20:52.878830 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: _acquire sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844852.878831
[0m00:20:52.879925 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:20:52.901440 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:20:52.903346 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (compile): 00:20:52.880311 => 00:20:52.902436
[0m00:20:52.903346 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:20:52.920507 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:20:52.920507 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.04167652130126953s, acqrelcnt: 1, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844852.878831
[0m00:20:52.920507 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.04167652130126953s, acqrelcnt: 1, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844852.878831
[0m00:20:52.920507 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.04167652130126953s, acqrelcnt: 1, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844852.878831
[0m00:20:52.920507 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.04167652130126953s, acqrelcnt: 1, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844852.878831
[0m00:20:52.920507 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:20:52.920507 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:20:52.920507 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_name
from `hive_metastore`.`saleslt`.`dim_product`
where product_name is null



      
    ) dbt_internal_test
[0m00:20:52.920507 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:20:53.147683 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: session opened sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844853.1476839
[0m00:20:53.463566 [debug] [Thread-1 (]: SQL status: OK in 0.5400000214576721 seconds
[0m00:20:53.463566 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (execute): 00:20:52.904360 => 00:20:53.463566
[0m00:20:53.463566 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: _release sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844853.4635665
[0m00:20:53.463566 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: _release sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844853.4635665
[0m00:20:53.463566 [info ] [Thread-1 (]: 1 of 8 PASS not_null_dim_product_product_name .................................. [[32mPASS[0m in 0.60s]
[0m00:20:53.463566 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:20:53.463566 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:20:53.463566 [info ] [Thread-1 (]: 2 of 8 START test not_null_dim_product_product_sk .............................. [RUN]
[0m00:20:53.463566 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: idle check connection: sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844853.4635665
[0m00:20:53.463566 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, now test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417)
[0m00:20:53.463566 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844853.4635665
[0m00:20:53.463566 [debug] [Thread-1 (]: Databricks adapter: On thread (12228, 25144): None using default compute resource.
[0m00:20:53.463566 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: _acquire sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844853.4635665
[0m00:20:53.463566 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:20:53.481322 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:20:53.483354 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (compile): 00:20:53.463566 => 00:20:53.482445
[0m00:20:53.483354 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:20:53.484938 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:20:53.484938 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: get_thread_connection: sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.021371841430664062s, acqrelcnt: 1, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844853.4635665
[0m00:20:53.484938 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: idle check connection: sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.021371841430664062s, acqrelcnt: 1, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844853.4635665
[0m00:20:53.484938 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:20:53.484938 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select product_sk
from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is null



      
    ) dbt_internal_test
[0m00:20:53.729644 [debug] [Thread-1 (]: SQL status: OK in 0.23999999463558197 seconds
[0m00:20:53.729644 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (execute): 00:20:53.483354 => 00:20:53.729644
[0m00:20:53.729644 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: _release sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844853.7296448
[0m00:20:53.729644 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: _release sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844853.7296448
[0m00:20:53.745129 [info ] [Thread-1 (]: 2 of 8 PASS not_null_dim_product_product_sk .................................... [[32mPASS[0m in 0.27s]
[0m00:20:53.745129 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:20:53.745129 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:20:53.745129 [info ] [Thread-1 (]: 3 of 8 START test not_null_dim_product_sellstartdate ........................... [RUN]
[0m00:20:53.745129 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: idle check connection: sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.015485048294067383s, acqrelcnt: 0, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844853.7296448
[0m00:20:53.745129 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, now test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546)
[0m00:20:53.745129 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.015485048294067383s, acqrelcnt: 0, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844853.7296448
[0m00:20:53.745129 [debug] [Thread-1 (]: Databricks adapter: On thread (12228, 25144): None using default compute resource.
[0m00:20:53.745129 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: _acquire sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.015485048294067383s, acqrelcnt: 1, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844853.7296448
[0m00:20:53.745129 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:20:53.745129 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:20:53.745129 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (compile): 00:20:53.745129 => 00:20:53.745129
[0m00:20:53.745129 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:20:53.745129 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:20:53.745129 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: get_thread_connection: sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.015485048294067383s, acqrelcnt: 1, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844853.7296448
[0m00:20:53.745129 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: idle check connection: sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.015485048294067383s, acqrelcnt: 1, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844853.7296448
[0m00:20:53.760797 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:20:53.760797 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select sellstartdate
from `hive_metastore`.`saleslt`.`dim_product`
where sellstartdate is null



      
    ) dbt_internal_test
[0m00:20:53.977970 [debug] [Thread-1 (]: SQL status: OK in 0.2199999988079071 seconds
[0m00:20:53.977970 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (execute): 00:20:53.745129 => 00:20:53.977970
[0m00:20:53.977970 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: _release sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844853.9779701
[0m00:20:53.977970 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: _release sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844853.9779701
[0m00:20:53.977970 [info ] [Thread-1 (]: 3 of 8 PASS not_null_dim_product_sellstartdate ................................. [[32mPASS[0m in 0.23s]
[0m00:20:53.977970 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:20:53.993430 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:20:53.993430 [info ] [Thread-1 (]: 4 of 8 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m00:20:53.993430 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: idle check connection: sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.015460729598999023s, acqrelcnt: 0, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844853.9779701
[0m00:20:53.993430 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, now test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710)
[0m00:20:53.993430 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: reusing connection test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.015460729598999023s, acqrelcnt: 0, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844853.9779701
[0m00:20:53.993430 [debug] [Thread-1 (]: Databricks adapter: On thread (12228, 25144): None using default compute resource.
[0m00:20:53.993430 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: _acquire sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.015460729598999023s, acqrelcnt: 1, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844853.9779701
[0m00:20:53.993430 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:20:53.993430 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:20:53.993430 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 00:20:53.993430 => 00:20:53.993430
[0m00:20:53.993430 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:20:53.993430 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:20:53.993430 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: get_thread_connection: sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.015460729598999023s, acqrelcnt: 1, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844853.9779701
[0m00:20:53.993430 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: idle check connection: sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.015460729598999023s, acqrelcnt: 1, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844853.9779701
[0m00:20:53.993430 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:20:53.993430 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m00:20:54.277760 [debug] [Thread-1 (]: SQL status: OK in 0.2800000011920929 seconds
[0m00:20:54.293315 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 00:20:53.993430 => 00:20:54.293315
[0m00:20:54.293315 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: _release sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844854.2933156
[0m00:20:54.293315 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: _release sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844854.2933156
[0m00:20:54.293315 [error] [Thread-1 (]: 4 of 8 FAIL 1 not_null_my_first_dbt_model_id ................................... [[31mFAIL 1[0m in 0.30s]
[0m00:20:54.293315 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:20:54.293315 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:20:54.293315 [info ] [Thread-1 (]: 5 of 8 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m00:20:54.293315 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: idle check connection: sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844854.2933156
[0m00:20:54.293315 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, now test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778)
[0m00:20:54.293315 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: reusing connection test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844854.2933156
[0m00:20:54.293315 [debug] [Thread-1 (]: Databricks adapter: On thread (12228, 25144): None using default compute resource.
[0m00:20:54.293315 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: _acquire sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844854.2933156
[0m00:20:54.293315 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:20:54.293315 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:20:54.293315 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (compile): 00:20:54.293315 => 00:20:54.293315
[0m00:20:54.293315 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:20:54.308943 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:20:54.309849 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: get_thread_connection: sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.016533613204956055s, acqrelcnt: 1, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844854.2933156
[0m00:20:54.310786 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: idle check connection: sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0174710750579834s, acqrelcnt: 1, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844854.2933156
[0m00:20:54.310786 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:20:54.311833 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is null



      
    ) dbt_internal_test
[0m00:20:54.578316 [debug] [Thread-1 (]: SQL status: OK in 0.27000001072883606 seconds
[0m00:20:54.578316 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (execute): 00:20:54.293315 => 00:20:54.578316
[0m00:20:54.578316 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: _release sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844854.578316
[0m00:20:54.578316 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: _release sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844854.578316
[0m00:20:54.578316 [info ] [Thread-1 (]: 5 of 8 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.29s]
[0m00:20:54.578316 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:20:54.593808 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:20:54.593808 [info ] [Thread-1 (]: 6 of 8 START test unique_dim_product_product_sk ................................ [RUN]
[0m00:20:54.593808 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: idle check connection: sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.015492677688598633s, acqrelcnt: 0, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844854.578316
[0m00:20:54.593808 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, now test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c)
[0m00:20:54.593808 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: reusing connection test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.015492677688598633s, acqrelcnt: 0, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844854.578316
[0m00:20:54.593808 [debug] [Thread-1 (]: Databricks adapter: On thread (12228, 25144): None using default compute resource.
[0m00:20:54.593808 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: _acquire sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.015492677688598633s, acqrelcnt: 1, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844854.578316
[0m00:20:54.593808 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:20:54.593808 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:20:54.593808 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (compile): 00:20:54.593808 => 00:20:54.593808
[0m00:20:54.593808 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:20:54.593808 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:20:54.593808 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: get_thread_connection: sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.015492677688598633s, acqrelcnt: 1, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844854.578316
[0m00:20:54.593808 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: idle check connection: sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.015492677688598633s, acqrelcnt: 1, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844854.578316
[0m00:20:54.609398 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:20:54.609398 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    product_sk as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is not null
group by product_sk
having count(*) > 1



      
    ) dbt_internal_test
[0m00:20:54.960220 [debug] [Thread-1 (]: SQL status: OK in 0.3499999940395355 seconds
[0m00:20:54.960220 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (execute): 00:20:54.593808 => 00:20:54.960220
[0m00:20:54.960220 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: _release sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844854.9602203
[0m00:20:54.960220 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: _release sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844854.9602203
[0m00:20:54.960220 [info ] [Thread-1 (]: 6 of 8 PASS unique_dim_product_product_sk ...................................... [[32mPASS[0m in 0.37s]
[0m00:20:54.969230 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:20:54.969230 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:20:54.969230 [info ] [Thread-1 (]: 7 of 8 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m00:20:54.969230 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: idle check connection: sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.009009838104248047s, acqrelcnt: 0, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844854.9602203
[0m00:20:54.969230 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, now test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321)
[0m00:20:54.969230 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: reusing connection test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.009009838104248047s, acqrelcnt: 0, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844854.9602203
[0m00:20:54.969230 [debug] [Thread-1 (]: Databricks adapter: On thread (12228, 25144): None using default compute resource.
[0m00:20:54.969230 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: _acquire sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.009009838104248047s, acqrelcnt: 1, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844854.9602203
[0m00:20:54.969230 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:20:54.969230 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:20:54.969230 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (compile): 00:20:54.969230 => 00:20:54.969230
[0m00:20:54.969230 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:20:54.969230 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:20:54.969230 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: get_thread_connection: sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.009009838104248047s, acqrelcnt: 1, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844854.9602203
[0m00:20:54.984861 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: idle check connection: sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.009009838104248047s, acqrelcnt: 1, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844854.9602203
[0m00:20:54.984861 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:20:54.985956 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m00:20:55.303426 [debug] [Thread-1 (]: SQL status: OK in 0.3199999928474426 seconds
[0m00:20:55.303426 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (execute): 00:20:54.969230 => 00:20:55.303426
[0m00:20:55.303426 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: _release sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844855.3034265
[0m00:20:55.303426 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: _release sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844855.3034265
[0m00:20:55.303426 [info ] [Thread-1 (]: 7 of 8 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.33s]
[0m00:20:55.303426 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:20:55.303426 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:20:55.303426 [info ] [Thread-1 (]: 8 of 8 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m00:20:55.316090 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: idle check connection: sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.012663841247558594s, acqrelcnt: 0, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844855.3034265
[0m00:20:55.316090 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, now test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493)
[0m00:20:55.316090 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: reusing connection test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.012663841247558594s, acqrelcnt: 0, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844855.3034265
[0m00:20:55.316090 [debug] [Thread-1 (]: Databricks adapter: On thread (12228, 25144): None using default compute resource.
[0m00:20:55.316090 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: _acquire sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.012663841247558594s, acqrelcnt: 1, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844855.3034265
[0m00:20:55.316090 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:20:55.316090 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:20:55.316090 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (compile): 00:20:55.316090 => 00:20:55.316090
[0m00:20:55.316090 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:20:55.316090 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:20:55.316090 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: get_thread_connection: sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.012663841247558594s, acqrelcnt: 1, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844855.3034265
[0m00:20:55.316090 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: idle check connection: sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.012663841247558594s, acqrelcnt: 1, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844855.3034265
[0m00:20:55.316090 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:20:55.316090 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m00:20:55.641494 [debug] [Thread-1 (]: SQL status: OK in 0.33000001311302185 seconds
[0m00:20:55.657030 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (execute): 00:20:55.316090 => 00:20:55.657030
[0m00:20:55.657030 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: _release sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844855.6570308
[0m00:20:55.657030 [debug] [Thread-1 (]: Databricks adapter: conn: 2081934579600: _release sess: 3443e676-2101-4e44-8d2f-4abb4941b702, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (12228, 25144), cmpt: ``, lut: 1707844855.6570308
[0m00:20:55.657030 [info ] [Thread-1 (]: 8 of 8 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.34s]
[0m00:20:55.657030 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:20:55.657030 [debug] [MainThread]: Databricks adapter: conn: 2081935724368: idle check connection: sess: None, name: master, idle: 2.79408860206604s, acqrelcnt: 0, lang: None, thrd: (12228, 25972), cmpt: ``, lut: 1707844852.8629422
[0m00:20:55.657030 [debug] [MainThread]: Databricks adapter: conn: 2081935724368: reusing connection master sess: None, name: master, idle: 2.79408860206604s, acqrelcnt: 0, lang: None, thrd: (12228, 25972), cmpt: ``, lut: 1707844852.8629422
[0m00:20:55.657030 [debug] [MainThread]: Databricks adapter: Thread (12228, 25972) using default compute resource.
[0m00:20:55.657030 [debug] [MainThread]: Databricks adapter: conn: 2081935724368: _acquire sess: None, name: master, idle: 2.79408860206604s, acqrelcnt: 1, lang: None, thrd: (12228, 25972), cmpt: ``, lut: 1707844852.8629422
[0m00:20:55.657030 [debug] [MainThread]: Databricks adapter: conn: 2081935724368: get_thread_connection: sess: None, name: master, idle: 2.79408860206604s, acqrelcnt: 1, lang: None, thrd: (12228, 25972), cmpt: ``, lut: 1707844852.8629422
[0m00:20:55.657030 [debug] [MainThread]: Databricks adapter: conn: 2081935724368: idle check connection: sess: None, name: master, idle: 2.79408860206604s, acqrelcnt: 1, lang: None, thrd: (12228, 25972), cmpt: ``, lut: 1707844852.8629422
[0m00:20:55.657030 [debug] [MainThread]: On master: ROLLBACK
[0m00:20:55.657030 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:20:55.912198 [debug] [MainThread]: Databricks adapter: conn: 2081935724368: session opened sess: a2c66c14-f8b7-4d1f-a88e-6b8ffac0fd37, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (12228, 25972), cmpt: ``, lut: 1707844855.912199
[0m00:20:55.912198 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:20:55.912198 [debug] [MainThread]: Databricks adapter: conn: 2081935724368: get_thread_connection: sess: a2c66c14-f8b7-4d1f-a88e-6b8ffac0fd37, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (12228, 25972), cmpt: ``, lut: 1707844855.912199
[0m00:20:55.912198 [debug] [MainThread]: Databricks adapter: conn: 2081935724368: idle check connection: sess: a2c66c14-f8b7-4d1f-a88e-6b8ffac0fd37, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (12228, 25972), cmpt: ``, lut: 1707844855.912199
[0m00:20:55.912198 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:20:55.912198 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:20:55.912198 [debug] [MainThread]: Databricks adapter: conn: 2081935724368: _release sess: a2c66c14-f8b7-4d1f-a88e-6b8ffac0fd37, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (12228, 25972), cmpt: ``, lut: 1707844855.912199
[0m00:20:55.927725 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:20:55.927725 [debug] [MainThread]: On master: ROLLBACK
[0m00:20:55.927725 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:20:55.927725 [debug] [MainThread]: On master: Close
[0m00:20:56.021530 [debug] [MainThread]: Connection 'list_hive_metastore_saleslt' was properly closed.
[0m00:20:56.021530 [debug] [MainThread]: On list_hive_metastore_saleslt: ROLLBACK
[0m00:20:56.021530 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:20:56.021530 [debug] [MainThread]: On list_hive_metastore_saleslt: Close
[0m00:20:56.099722 [debug] [MainThread]: Connection 'test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493' was properly closed.
[0m00:20:56.099722 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: ROLLBACK
[0m00:20:56.099722 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:20:56.099722 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m00:20:56.177847 [info ] [MainThread]: 
[0m00:20:56.177847 [info ] [MainThread]: Finished running 8 tests in 0 hours 0 minutes and 4.98 seconds (4.98s).
[0m00:20:56.177847 [debug] [MainThread]: Command end result
[0m00:20:56.204987 [info ] [MainThread]: 
[0m00:20:56.206710 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:20:56.207315 [info ] [MainThread]: 
[0m00:20:56.209364 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models\example\schema.yml)[0m
[0m00:20:56.209883 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m00:20:56.211296 [info ] [MainThread]: 
[0m00:20:56.211296 [info ] [MainThread]:   compiled Code at target\compiled\medallion_dbt_spark\models\example\schema.yml\not_null_my_first_dbt_model_id.sql
[0m00:20:56.211296 [info ] [MainThread]: 
[0m00:20:56.211296 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=1 SKIP=0 TOTAL=8
[0m00:20:56.211296 [debug] [MainThread]: Command `dbt test` failed at 00:20:56.211296 after 6.37 seconds
[0m00:20:56.211296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4AAD2EC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4AA68BFD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E4AA705F90>]}
[0m00:20:56.211296 [debug] [MainThread]: Flushing usage events
[0m00:21:51.442962 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E74ED93CD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E74EAC14D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E74EAC3B10>]}


============================== 00:21:51.442962 | 70dc61f6-9a0a-4f2e-9b42-2e2164b73864 ==============================
[0m00:21:51.442962 [info ] [MainThread]: Running with dbt=1.7.7
[0m00:21:51.458514 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt test --store-failures', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m00:21:52.769035 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '70dc61f6-9a0a-4f2e-9b42-2e2164b73864', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E760E48BD0>]}
[0m00:21:52.852432 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '70dc61f6-9a0a-4f2e-9b42-2e2164b73864', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E760EB65D0>]}
[0m00:21:52.853433 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m00:21:52.863768 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m00:21:52.954269 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m00:21:52.954269 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m00:21:52.961269 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '70dc61f6-9a0a-4f2e-9b42-2e2164b73864', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E761103050>]}
[0m00:21:52.977868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '70dc61f6-9a0a-4f2e-9b42-2e2164b73864', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E760FF7D50>]}
[0m00:21:52.978869 [info ] [MainThread]: Found 5 models, 7 snapshots, 8 tests, 9 sources, 0 exposures, 0 metrics, 535 macros, 0 groups, 0 semantic models
[0m00:21:52.979876 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '70dc61f6-9a0a-4f2e-9b42-2e2164b73864', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E760F92690>]}
[0m00:21:52.982868 [info ] [MainThread]: 
[0m00:21:52.983361 [debug] [MainThread]: Databricks adapter: conn: 2093275862352: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (18648, 6828), cmpt: ``, lut: None
[0m00:21:52.983361 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:21:52.983361 [debug] [MainThread]: Databricks adapter: Thread (18648, 6828) using default compute resource.
[0m00:21:52.983361 [debug] [MainThread]: Databricks adapter: conn: 2093275862352: _acquire sess: None, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (18648, 6828), cmpt: ``, lut: 1707844912.9833612
[0m00:21:52.983361 [debug] [ThreadPool]: Databricks adapter: conn: 2093276416720: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (18648, 24932), cmpt: ``, lut: None
[0m00:21:52.983361 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m00:21:52.983361 [debug] [ThreadPool]: Databricks adapter: Thread (18648, 24932) using default compute resource.
[0m00:21:52.983361 [debug] [ThreadPool]: Databricks adapter: conn: 2093276416720: _acquire sess: None, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (18648, 24932), cmpt: ``, lut: 1707844912.9833612
[0m00:21:52.983361 [debug] [ThreadPool]: Databricks adapter: conn: 2093276416720: get_thread_connection: sess: None, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (18648, 24932), cmpt: ``, lut: 1707844912.9833612
[0m00:21:52.983361 [debug] [ThreadPool]: Databricks adapter: conn: 2093276416720: idle check connection: sess: None, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (18648, 24932), cmpt: ``, lut: 1707844912.9833612
[0m00:21:52.983361 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m00:21:52.983361 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m00:21:52.983361 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:21:53.389223 [debug] [ThreadPool]: Databricks adapter: conn: 2093276416720: session opened sess: dc49e713-45e6-4311-b96a-82351d888209, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (18648, 24932), cmpt: ``, lut: 1707844913.3892236
[0m00:21:53.545457 [debug] [ThreadPool]: SQL status: OK in 0.5600000023841858 seconds
[0m00:21:53.560928 [debug] [ThreadPool]: Databricks adapter: conn: 2093276416720: _release sess: dc49e713-45e6-4311-b96a-82351d888209, name: list_hive_metastore, idle: 0.015471220016479492s, acqrelcnt: 0, lang: None, thrd: (18648, 24932), cmpt: ``, lut: 1707844913.5454574
[0m00:21:53.560928 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_snapshots, idle: 0s, acqrelcnt: 0, lang: None, thrd: (18648, 12708), cmpt: ``, lut: None
[0m00:21:53.560928 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_snapshots'
[0m00:21:53.560928 [debug] [ThreadPool]: Databricks adapter: Thread (18648, 12708) using default compute resource.
[0m00:21:53.560928 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: _acquire sess: None, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (18648, 12708), cmpt: ``, lut: 1707844913.5609286
[0m00:21:53.560928 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: get_thread_connection: sess: None, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (18648, 12708), cmpt: ``, lut: 1707844913.5609286
[0m00:21:53.560928 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: idle check connection: sess: None, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (18648, 12708), cmpt: ``, lut: 1707844913.5609286
[0m00:21:53.560928 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:21:53.560928 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m00:21:53.560928 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:21:53.797416 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: session opened sess: 57bc21d8-40c7-4e35-a47f-c619d02e6d59, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (18648, 12708), cmpt: ``, lut: 1707844913.7974164
[0m00:21:53.913206 [debug] [ThreadPool]: SQL status: OK in 0.3499999940395355 seconds
[0m00:21:53.928864 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: get_thread_connection: sess: 57bc21d8-40c7-4e35-a47f-c619d02e6d59, name: list_hive_metastore_snapshots, idle: 0.13144826889038086s, acqrelcnt: 1, lang: None, thrd: (18648, 12708), cmpt: ``, lut: 1707844913.7974164
[0m00:21:53.928864 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: idle check connection: sess: 57bc21d8-40c7-4e35-a47f-c619d02e6d59, name: list_hive_metastore_snapshots, idle: 0.13144826889038086s, acqrelcnt: 1, lang: None, thrd: (18648, 12708), cmpt: ``, lut: 1707844913.7974164
[0m00:21:53.928864 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: get_thread_connection: sess: 57bc21d8-40c7-4e35-a47f-c619d02e6d59, name: list_hive_metastore_snapshots, idle: 0.13144826889038086s, acqrelcnt: 1, lang: None, thrd: (18648, 12708), cmpt: ``, lut: 1707844913.7974164
[0m00:21:53.928864 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: idle check connection: sess: 57bc21d8-40c7-4e35-a47f-c619d02e6d59, name: list_hive_metastore_snapshots, idle: 0.13144826889038086s, acqrelcnt: 1, lang: None, thrd: (18648, 12708), cmpt: ``, lut: 1707844913.7974164
[0m00:21:53.928864 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:21:53.928864 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:21:53.928864 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m00:21:54.068142 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m00:21:54.083238 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: get_thread_connection: sess: 57bc21d8-40c7-4e35-a47f-c619d02e6d59, name: list_hive_metastore_snapshots, idle: 0.28582191467285156s, acqrelcnt: 1, lang: None, thrd: (18648, 12708), cmpt: ``, lut: 1707844913.7974164
[0m00:21:54.083238 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: idle check connection: sess: 57bc21d8-40c7-4e35-a47f-c619d02e6d59, name: list_hive_metastore_snapshots, idle: 0.28582191467285156s, acqrelcnt: 1, lang: None, thrd: (18648, 12708), cmpt: ``, lut: 1707844913.7974164
[0m00:21:54.083238 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:21:54.083238 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m00:21:54.229256 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m00:21:54.229256 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: _release sess: 57bc21d8-40c7-4e35-a47f-c619d02e6d59, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (18648, 12708), cmpt: ``, lut: 1707844914.2292564
[0m00:21:54.229256 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: idle check connection: sess: 57bc21d8-40c7-4e35-a47f-c619d02e6d59, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (18648, 12708), cmpt: ``, lut: 1707844914.2292564
[0m00:21:54.229256 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now list_hive_metastore_saleslt)
[0m00:21:54.229256 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: reusing connection list_hive_metastore_snapshots sess: 57bc21d8-40c7-4e35-a47f-c619d02e6d59, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (18648, 12708), cmpt: ``, lut: 1707844914.2292564
[0m00:21:54.229256 [debug] [ThreadPool]: Databricks adapter: Thread (18648, 12708) using default compute resource.
[0m00:21:54.244079 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: _acquire sess: 57bc21d8-40c7-4e35-a47f-c619d02e6d59, name: list_hive_metastore_saleslt, idle: 0.014822959899902344s, acqrelcnt: 1, lang: None, thrd: (18648, 12708), cmpt: ``, lut: 1707844914.2292564
[0m00:21:54.246079 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: get_thread_connection: sess: 57bc21d8-40c7-4e35-a47f-c619d02e6d59, name: list_hive_metastore_saleslt, idle: 0.016823530197143555s, acqrelcnt: 1, lang: None, thrd: (18648, 12708), cmpt: ``, lut: 1707844914.2292564
[0m00:21:54.247185 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: idle check connection: sess: 57bc21d8-40c7-4e35-a47f-c619d02e6d59, name: list_hive_metastore_saleslt, idle: 0.01782369613647461s, acqrelcnt: 1, lang: None, thrd: (18648, 12708), cmpt: ``, lut: 1707844914.2292564
[0m00:21:54.247185 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:21:54.247185 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m00:21:54.355958 [debug] [ThreadPool]: SQL status: OK in 0.10999999940395355 seconds
[0m00:21:54.362271 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: get_thread_connection: sess: 57bc21d8-40c7-4e35-a47f-c619d02e6d59, name: list_hive_metastore_saleslt, idle: 0.13261985778808594s, acqrelcnt: 1, lang: None, thrd: (18648, 12708), cmpt: ``, lut: 1707844914.2292564
[0m00:21:54.362271 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: idle check connection: sess: 57bc21d8-40c7-4e35-a47f-c619d02e6d59, name: list_hive_metastore_saleslt, idle: 0.13301467895507812s, acqrelcnt: 1, lang: None, thrd: (18648, 12708), cmpt: ``, lut: 1707844914.2292564
[0m00:21:54.363313 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:21:54.363313 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m00:21:54.489309 [debug] [ThreadPool]: SQL status: OK in 0.12999999523162842 seconds
[0m00:21:54.495666 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: get_thread_connection: sess: 57bc21d8-40c7-4e35-a47f-c619d02e6d59, name: list_hive_metastore_saleslt, idle: 0.26640963554382324s, acqrelcnt: 1, lang: None, thrd: (18648, 12708), cmpt: ``, lut: 1707844914.2292564
[0m00:21:54.496698 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: idle check connection: sess: 57bc21d8-40c7-4e35-a47f-c619d02e6d59, name: list_hive_metastore_saleslt, idle: 0.2674417495727539s, acqrelcnt: 1, lang: None, thrd: (18648, 12708), cmpt: ``, lut: 1707844914.2292564
[0m00:21:54.496698 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:21:54.497664 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m00:21:54.726971 [debug] [ThreadPool]: SQL status: OK in 0.23000000417232513 seconds
[0m00:21:54.731890 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: _release sess: 57bc21d8-40c7-4e35-a47f-c619d02e6d59, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (18648, 12708), cmpt: ``, lut: 1707844914.7309895
[0m00:21:54.733351 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: idle check connection: sess: 57bc21d8-40c7-4e35-a47f-c619d02e6d59, name: list_hive_metastore_saleslt, idle: 0.002362489700317383s, acqrelcnt: 0, lang: None, thrd: (18648, 12708), cmpt: ``, lut: 1707844914.7309895
[0m00:21:54.737193 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_saleslt, now list_hive_metastore_saleslt_dbt_test__audit)
[0m00:21:54.737193 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: reusing connection list_hive_metastore_saleslt sess: 57bc21d8-40c7-4e35-a47f-c619d02e6d59, name: list_hive_metastore_saleslt_dbt_test__audit, idle: 0.006203651428222656s, acqrelcnt: 0, lang: None, thrd: (18648, 12708), cmpt: ``, lut: 1707844914.7309895
[0m00:21:54.738201 [debug] [ThreadPool]: Databricks adapter: Thread (18648, 12708) using default compute resource.
[0m00:21:54.738201 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: _acquire sess: 57bc21d8-40c7-4e35-a47f-c619d02e6d59, name: list_hive_metastore_saleslt_dbt_test__audit, idle: 0.0072116851806640625s, acqrelcnt: 1, lang: None, thrd: (18648, 12708), cmpt: ``, lut: 1707844914.7309895
[0m00:21:54.740162 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: get_thread_connection: sess: 57bc21d8-40c7-4e35-a47f-c619d02e6d59, name: list_hive_metastore_saleslt_dbt_test__audit, idle: 0.009172677993774414s, acqrelcnt: 1, lang: None, thrd: (18648, 12708), cmpt: ``, lut: 1707844914.7309895
[0m00:21:54.741249 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: idle check connection: sess: 57bc21d8-40c7-4e35-a47f-c619d02e6d59, name: list_hive_metastore_saleslt_dbt_test__audit, idle: 0.010260343551635742s, acqrelcnt: 1, lang: None, thrd: (18648, 12708), cmpt: ``, lut: 1707844914.7309895
[0m00:21:54.741249 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt_dbt_test__audit"
[0m00:21:54.742180 [debug] [ThreadPool]: On list_hive_metastore_saleslt_dbt_test__audit: GetTables(database=hive_metastore, schema=saleslt_dbt_test__audit, identifier=None)
[0m00:21:54.853207 [debug] [ThreadPool]: SQL status: OK in 0.10999999940395355 seconds
[0m00:21:54.858733 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: get_thread_connection: sess: 57bc21d8-40c7-4e35-a47f-c619d02e6d59, name: list_hive_metastore_saleslt_dbt_test__audit, idle: 0.12774443626403809s, acqrelcnt: 1, lang: None, thrd: (18648, 12708), cmpt: ``, lut: 1707844914.7309895
[0m00:21:54.859690 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: idle check connection: sess: 57bc21d8-40c7-4e35-a47f-c619d02e6d59, name: list_hive_metastore_saleslt_dbt_test__audit, idle: 0.12774443626403809s, acqrelcnt: 1, lang: None, thrd: (18648, 12708), cmpt: ``, lut: 1707844914.7309895
[0m00:21:54.859690 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt_dbt_test__audit"
[0m00:21:54.859690 [debug] [ThreadPool]: On list_hive_metastore_saleslt_dbt_test__audit: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt_dbt_test__audit"} */

      select current_catalog()
  
[0m00:21:55.024368 [debug] [ThreadPool]: SQL status: OK in 0.1599999964237213 seconds
[0m00:21:55.033358 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: get_thread_connection: sess: 57bc21d8-40c7-4e35-a47f-c619d02e6d59, name: list_hive_metastore_saleslt_dbt_test__audit, idle: 0.3023691177368164s, acqrelcnt: 1, lang: None, thrd: (18648, 12708), cmpt: ``, lut: 1707844914.7309895
[0m00:21:55.033358 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: idle check connection: sess: 57bc21d8-40c7-4e35-a47f-c619d02e6d59, name: list_hive_metastore_saleslt_dbt_test__audit, idle: 0.3023691177368164s, acqrelcnt: 1, lang: None, thrd: (18648, 12708), cmpt: ``, lut: 1707844914.7309895
[0m00:21:55.034296 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt_dbt_test__audit"
[0m00:21:55.034296 [debug] [ThreadPool]: On list_hive_metastore_saleslt_dbt_test__audit: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt_dbt_test__audit"} */
show views in `hive_metastore`.`saleslt_dbt_test__audit`
  
[0m00:21:55.169822 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m00:21:55.169822 [debug] [ThreadPool]: Databricks adapter: conn: 2093276424080: _release sess: 57bc21d8-40c7-4e35-a47f-c619d02e6d59, name: list_hive_metastore_saleslt_dbt_test__audit, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (18648, 12708), cmpt: ``, lut: 1707844915.1698225
[0m00:21:55.169822 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '70dc61f6-9a0a-4f2e-9b42-2e2164b73864', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E760E75590>]}
[0m00:21:55.169822 [debug] [MainThread]: Databricks adapter: conn: 2093275862352: get_thread_connection: sess: None, name: master, idle: 2.1864612102508545s, acqrelcnt: 1, lang: None, thrd: (18648, 6828), cmpt: ``, lut: 1707844912.9833612
[0m00:21:55.169822 [debug] [MainThread]: Databricks adapter: conn: 2093275862352: idle check connection: sess: None, name: master, idle: 2.1864612102508545s, acqrelcnt: 1, lang: None, thrd: (18648, 6828), cmpt: ``, lut: 1707844912.9833612
[0m00:21:55.169822 [debug] [MainThread]: Databricks adapter: conn: 2093275862352: get_thread_connection: sess: None, name: master, idle: 2.1864612102508545s, acqrelcnt: 1, lang: None, thrd: (18648, 6828), cmpt: ``, lut: 1707844912.9833612
[0m00:21:55.169822 [debug] [MainThread]: Databricks adapter: conn: 2093275862352: idle check connection: sess: None, name: master, idle: 2.1864612102508545s, acqrelcnt: 1, lang: None, thrd: (18648, 6828), cmpt: ``, lut: 1707844912.9833612
[0m00:21:55.169822 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:21:55.169822 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:21:55.169822 [debug] [MainThread]: Databricks adapter: conn: 2093275862352: _release sess: None, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (18648, 6828), cmpt: ``, lut: 1707844915.1698225
[0m00:21:55.184830 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:21:55.186262 [info ] [MainThread]: 
[0m00:21:55.186262 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:21:55.186262 [info ] [Thread-1 (]: 1 of 8 START test not_null_dim_product_product_name ............................ [RUN]
[0m00:21:55.193483 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: Creating DatabricksDBTConnection sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0s, acqrelcnt: 0, lang: None, thrd: (18648, 12268), cmpt: ``, lut: None
[0m00:21:55.193483 [debug] [Thread-1 (]: Acquiring new databricks connection 'test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5'
[0m00:21:55.194491 [debug] [Thread-1 (]: Databricks adapter: On thread (18648, 12268): None using default compute resource.
[0m00:21:55.194491 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: _acquire sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844915.1944911
[0m00:21:55.194491 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:21:55.211522 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:21:55.212449 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (compile): 00:21:55.195488 => 00:21:55.212449
[0m00:21:55.212449 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:21:55.230920 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_dim_product_product_name`
[0m00:21:55.236583 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: get_thread_connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.04209256172180176s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844915.1944911
[0m00:21:55.236583 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.04209256172180176s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844915.1944911
[0m00:21:55.236583 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:21:55.236583 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"} */
drop table if exists `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_dim_product_product_name`
[0m00:21:55.236583 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m00:21:55.445119 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: session opened sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844915.4451194
[0m00:21:55.835056 [debug] [Thread-1 (]: SQL status: OK in 0.6000000238418579 seconds
[0m00:21:55.838029 [debug] [Thread-1 (]: Applying CREATE to: `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_dim_product_product_name`
[0m00:21:55.886236 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: get_thread_connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.4411172866821289s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844915.4451194
[0m00:21:55.886236 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.4411172866821289s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844915.4451194
[0m00:21:55.886236 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: get_thread_connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.4411172866821289s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844915.4451194
[0m00:21:55.886236 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.4411172866821289s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844915.4451194
[0m00:21:55.886236 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m00:21:55.886236 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:21:55.886236 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"} */

        
  
    
        create or replace table `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_dim_product_product_name`
      
      
    using delta
      
      
      
      
      
      
      
      as
      
    
    



select *
from `hive_metastore`.`saleslt`.`dim_product`
where product_name is null



  
    
[0m00:21:58.388296 [debug] [Thread-1 (]: SQL status: OK in 2.5 seconds
[0m00:21:58.388296 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:21:58.404048 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:21:58.404048 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: get_thread_connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 2.9589290618896484s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844915.4451194
[0m00:21:58.404048 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 2.9589290618896484s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844915.4451194
[0m00:21:58.404048 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:21:58.404048 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
        select *
        from `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_dim_product_product_name`
    
      
    ) dbt_internal_test
[0m00:21:58.710217 [debug] [Thread-1 (]: SQL status: OK in 0.3100000023841858 seconds
[0m00:21:58.710217 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (execute): 00:21:55.213447 => 00:21:58.710217
[0m00:21:58.710217 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: _release sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844918.710217
[0m00:21:58.725691 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: _release sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844918.7256918
[0m00:21:58.725691 [info ] [Thread-1 (]: 1 of 8 PASS not_null_dim_product_product_name .................................. [[32mPASS[0m in 3.54s]
[0m00:21:58.725691 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:21:58.725691 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:21:58.725691 [info ] [Thread-1 (]: 2 of 8 START test not_null_dim_product_product_sk .............................. [RUN]
[0m00:21:58.725691 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844918.7256918
[0m00:21:58.725691 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, now test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417)
[0m00:21:58.725691 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844918.7256918
[0m00:21:58.725691 [debug] [Thread-1 (]: Databricks adapter: On thread (18648, 12268): None using default compute resource.
[0m00:21:58.725691 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: _acquire sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844918.7256918
[0m00:21:58.725691 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:21:58.725691 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:21:58.725691 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (compile): 00:21:58.725691 => 00:21:58.725691
[0m00:21:58.725691 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:21:58.741408 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_dim_product_product_sk`
[0m00:21:58.742523 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: get_thread_connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.01683211326599121s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844918.7256918
[0m00:21:58.743525 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.01683211326599121s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844918.7256918
[0m00:21:58.743525 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:21:58.744559 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"} */
drop table if exists `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_dim_product_product_sk`
[0m00:21:59.058906 [debug] [Thread-1 (]: SQL status: OK in 0.3100000023841858 seconds
[0m00:21:59.061497 [debug] [Thread-1 (]: Applying CREATE to: `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_dim_product_product_sk`
[0m00:21:59.061497 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: get_thread_connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.33580517768859863s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844918.7256918
[0m00:21:59.061497 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.33580517768859863s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844918.7256918
[0m00:21:59.061497 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:21:59.061497 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"} */

        
  
    
        create or replace table `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_dim_product_product_sk`
      
      
    using delta
      
      
      
      
      
      
      
      as
      
    
    



select *
from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is null



  
    
[0m00:22:00.877169 [debug] [Thread-1 (]: SQL status: OK in 1.8200000524520874 seconds
[0m00:22:00.877169 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:22:00.877169 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:22:00.877169 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: get_thread_connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 2.151477098464966s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844918.7256918
[0m00:22:00.877169 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 2.151477098464966s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844918.7256918
[0m00:22:00.877169 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:22:00.877169 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
        select *
        from `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_dim_product_product_sk`
    
      
    ) dbt_internal_test
[0m00:22:01.111553 [debug] [Thread-1 (]: SQL status: OK in 0.23000000417232513 seconds
[0m00:22:01.111553 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (execute): 00:21:58.725691 => 00:22:01.111553
[0m00:22:01.111553 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: _release sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844921.1115532
[0m00:22:01.111553 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: _release sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844921.1115532
[0m00:22:01.111553 [info ] [Thread-1 (]: 2 of 8 PASS not_null_dim_product_product_sk .................................... [[32mPASS[0m in 2.39s]
[0m00:22:01.111553 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:22:01.111553 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:22:01.111553 [info ] [Thread-1 (]: 3 of 8 START test not_null_dim_product_sellstartdate ........................... [RUN]
[0m00:22:01.111553 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844921.1115532
[0m00:22:01.111553 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, now test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546)
[0m00:22:01.111553 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844921.1115532
[0m00:22:01.111553 [debug] [Thread-1 (]: Databricks adapter: On thread (18648, 12268): None using default compute resource.
[0m00:22:01.111553 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: _acquire sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844921.1115532
[0m00:22:01.127099 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:22:01.131195 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:22:01.133467 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (compile): 00:22:01.127099 => 00:22:01.132426
[0m00:22:01.133467 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:22:01.133467 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_dim_product_sellstartdate`
[0m00:22:01.133467 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: get_thread_connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.02191448211669922s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844921.1115532
[0m00:22:01.133467 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.02191448211669922s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844921.1115532
[0m00:22:01.133467 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:22:01.133467 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"} */
drop table if exists `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_dim_product_sellstartdate`
[0m00:22:01.463362 [debug] [Thread-1 (]: SQL status: OK in 0.3199999928474426 seconds
[0m00:22:01.463362 [debug] [Thread-1 (]: Applying CREATE to: `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_dim_product_sellstartdate`
[0m00:22:01.463362 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: get_thread_connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.3518092632293701s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844921.1115532
[0m00:22:01.463362 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.3518092632293701s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844921.1115532
[0m00:22:01.463362 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:22:01.463362 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"} */

        
  
    
        create or replace table `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_dim_product_sellstartdate`
      
      
    using delta
      
      
      
      
      
      
      
      as
      
    
    



select *
from `hive_metastore`.`saleslt`.`dim_product`
where sellstartdate is null



  
    
[0m00:22:03.510907 [debug] [Thread-1 (]: SQL status: OK in 2.049999952316284 seconds
[0m00:22:03.510907 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:22:03.510907 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:22:03.510907 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: get_thread_connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 2.3993542194366455s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844921.1115532
[0m00:22:03.510907 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 2.3993542194366455s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844921.1115532
[0m00:22:03.510907 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:22:03.510907 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
        select *
        from `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_dim_product_sellstartdate`
    
      
    ) dbt_internal_test
[0m00:22:03.796600 [debug] [Thread-1 (]: SQL status: OK in 0.28999999165534973 seconds
[0m00:22:03.796600 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (execute): 00:22:01.133467 => 00:22:03.796600
[0m00:22:03.796600 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: _release sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844923.7966006
[0m00:22:03.796600 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: _release sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844923.7966006
[0m00:22:03.796600 [info ] [Thread-1 (]: 3 of 8 PASS not_null_dim_product_sellstartdate ................................. [[32mPASS[0m in 2.69s]
[0m00:22:03.796600 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:22:03.796600 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:22:03.796600 [info ] [Thread-1 (]: 4 of 8 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m00:22:03.796600 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844923.7966006
[0m00:22:03.796600 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, now test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710)
[0m00:22:03.796600 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: reusing connection test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844923.7966006
[0m00:22:03.796600 [debug] [Thread-1 (]: Databricks adapter: On thread (18648, 12268): None using default compute resource.
[0m00:22:03.796600 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: _acquire sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844923.7966006
[0m00:22:03.796600 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:22:03.814748 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:22:03.815662 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 00:22:03.796600 => 00:22:03.815662
[0m00:22:03.816660 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:22:03.822483 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_my_first_dbt_model_id`
[0m00:22:03.823502 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: get_thread_connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.026901721954345703s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844923.7966006
[0m00:22:03.823502 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.026901721954345703s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844923.7966006
[0m00:22:03.823502 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:22:03.823502 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"} */
drop table if exists `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_my_first_dbt_model_id`
[0m00:22:04.122671 [debug] [Thread-1 (]: SQL status: OK in 0.30000001192092896 seconds
[0m00:22:04.122671 [debug] [Thread-1 (]: Applying CREATE to: `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_my_first_dbt_model_id`
[0m00:22:04.138148 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: get_thread_connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.34154820442199707s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844923.7966006
[0m00:22:04.138148 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.34154820442199707s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844923.7966006
[0m00:22:04.138148 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:22:04.138148 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"} */

        
  
    
        create or replace table `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_my_first_dbt_model_id`
      
      
    using delta
      
      
      
      
      
      
      
      as
      
    
    



select *
from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is null



  
    
[0m00:22:06.197759 [debug] [Thread-1 (]: SQL status: OK in 2.059999942779541 seconds
[0m00:22:06.197759 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:22:06.197759 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:22:06.197759 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: get_thread_connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 2.4011588096618652s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844923.7966006
[0m00:22:06.197759 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 2.4011588096618652s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844923.7966006
[0m00:22:06.197759 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:22:06.197759 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
        select *
        from `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_my_first_dbt_model_id`
    
      
    ) dbt_internal_test
[0m00:22:06.496800 [debug] [Thread-1 (]: SQL status: OK in 0.30000001192092896 seconds
[0m00:22:06.496800 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 00:22:03.817890 => 00:22:06.496800
[0m00:22:06.496800 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: _release sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844926.4968002
[0m00:22:06.496800 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: _release sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844926.4968002
[0m00:22:06.496800 [error] [Thread-1 (]: 4 of 8 FAIL 1 not_null_my_first_dbt_model_id ................................... [[31mFAIL 1[0m in 2.70s]
[0m00:22:06.496800 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:22:06.496800 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:22:06.496800 [info ] [Thread-1 (]: 5 of 8 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m00:22:06.496800 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844926.4968002
[0m00:22:06.496800 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, now test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778)
[0m00:22:06.496800 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: reusing connection test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844926.4968002
[0m00:22:06.496800 [debug] [Thread-1 (]: Databricks adapter: On thread (18648, 12268): None using default compute resource.
[0m00:22:06.512458 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: _acquire sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.015657901763916016s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844926.4968002
[0m00:22:06.512458 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:22:06.512458 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:22:06.512458 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (compile): 00:22:06.512458 => 00:22:06.512458
[0m00:22:06.512458 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:22:06.512458 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_my_second_dbt_model_id`
[0m00:22:06.512458 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: get_thread_connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.015657901763916016s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844926.4968002
[0m00:22:06.512458 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.015657901763916016s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844926.4968002
[0m00:22:06.512458 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:22:06.512458 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"} */
drop table if exists `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_my_second_dbt_model_id`
[0m00:22:06.805504 [debug] [Thread-1 (]: SQL status: OK in 0.28999999165534973 seconds
[0m00:22:06.821094 [debug] [Thread-1 (]: Applying CREATE to: `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_my_second_dbt_model_id`
[0m00:22:06.821094 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: get_thread_connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.3242936134338379s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844926.4968002
[0m00:22:06.821094 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.3242936134338379s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844926.4968002
[0m00:22:06.821094 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:22:06.821094 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"} */

        
  
    
        create or replace table `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_my_second_dbt_model_id`
      
      
    using delta
      
      
      
      
      
      
      
      as
      
    
    



select *
from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is null



  
    
[0m00:22:08.536059 [debug] [Thread-1 (]: SQL status: OK in 1.7100000381469727 seconds
[0m00:22:08.539047 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:22:08.539996 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:22:08.540965 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: get_thread_connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 2.0441651344299316s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844926.4968002
[0m00:22:08.541963 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 2.0441651344299316s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844926.4968002
[0m00:22:08.541963 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:22:08.542963 [debug] [Thread-1 (]: On test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
        select *
        from `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_my_second_dbt_model_id`
    
      
    ) dbt_internal_test
[0m00:22:08.749177 [debug] [Thread-1 (]: SQL status: OK in 0.20999999344348907 seconds
[0m00:22:08.755376 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (execute): 00:22:06.512458 => 00:22:08.755376
[0m00:22:08.756363 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: _release sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844928.7563632
[0m00:22:08.757281 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: _release sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844928.7572818
[0m00:22:08.758279 [info ] [Thread-1 (]: 5 of 8 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 2.26s]
[0m00:22:08.758799 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:22:08.759891 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:22:08.759891 [info ] [Thread-1 (]: 6 of 8 START test unique_dim_product_product_sk ................................ [RUN]
[0m00:22:08.761005 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0037238597869873047s, acqrelcnt: 0, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844928.7572818
[0m00:22:08.762087 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, now test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c)
[0m00:22:08.762087 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: reusing connection test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.004805564880371094s, acqrelcnt: 0, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844928.7572818
[0m00:22:08.763099 [debug] [Thread-1 (]: Databricks adapter: On thread (18648, 12268): None using default compute resource.
[0m00:22:08.763099 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: _acquire sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0058171749114990234s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844928.7572818
[0m00:22:08.763099 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:22:08.769508 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:22:08.771536 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (compile): 00:22:08.764013 => 00:22:08.770609
[0m00:22:08.771536 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:22:08.772868 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`saleslt_dbt_test__audit`.`unique_dim_product_product_sk`
[0m00:22:08.772868 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: get_thread_connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.01558685302734375s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844928.7572818
[0m00:22:08.772868 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.01558685302734375s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844928.7572818
[0m00:22:08.772868 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:22:08.772868 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"} */
drop table if exists `hive_metastore`.`saleslt_dbt_test__audit`.`unique_dim_product_product_sk`
[0m00:22:09.086519 [debug] [Thread-1 (]: SQL status: OK in 0.3100000023841858 seconds
[0m00:22:09.086519 [debug] [Thread-1 (]: Applying CREATE to: `hive_metastore`.`saleslt_dbt_test__audit`.`unique_dim_product_product_sk`
[0m00:22:09.103096 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: get_thread_connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.34581470489501953s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844928.7572818
[0m00:22:09.103096 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.34581470489501953s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844928.7572818
[0m00:22:09.103096 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:22:09.103096 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"} */

        
  
    
        create or replace table `hive_metastore`.`saleslt_dbt_test__audit`.`unique_dim_product_product_sk`
      
      
    using delta
      
      
      
      
      
      
      
      as
      
    
    

select
    product_sk as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`dim_product`
where product_sk is not null
group by product_sk
having count(*) > 1



  
    
[0m00:22:11.505790 [debug] [Thread-1 (]: SQL status: OK in 2.4000000953674316 seconds
[0m00:22:11.505790 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:22:11.505790 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:22:11.505790 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: get_thread_connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 2.7485084533691406s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844928.7572818
[0m00:22:11.505790 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 2.7485084533691406s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844928.7572818
[0m00:22:11.505790 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:22:11.505790 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
        select *
        from `hive_metastore`.`saleslt_dbt_test__audit`.`unique_dim_product_product_sk`
    
      
    ) dbt_internal_test
[0m00:22:11.802169 [debug] [Thread-1 (]: SQL status: OK in 0.30000001192092896 seconds
[0m00:22:11.817721 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (execute): 00:22:08.771536 => 00:22:11.817721
[0m00:22:11.817721 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: _release sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844931.8177216
[0m00:22:11.817721 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: _release sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844931.8177216
[0m00:22:11.817721 [info ] [Thread-1 (]: 6 of 8 PASS unique_dim_product_product_sk ...................................... [[32mPASS[0m in 3.06s]
[0m00:22:11.817721 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:22:11.817721 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:22:11.817721 [info ] [Thread-1 (]: 7 of 8 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m00:22:11.817721 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844931.8177216
[0m00:22:11.817721 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, now test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321)
[0m00:22:11.817721 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: reusing connection test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844931.8177216
[0m00:22:11.817721 [debug] [Thread-1 (]: Databricks adapter: On thread (18648, 12268): None using default compute resource.
[0m00:22:11.817721 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: _acquire sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844931.8177216
[0m00:22:11.817721 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:22:11.817721 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:22:11.817721 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (compile): 00:22:11.817721 => 00:22:11.817721
[0m00:22:11.817721 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:22:11.837325 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`saleslt_dbt_test__audit`.`unique_my_first_dbt_model_id`
[0m00:22:11.838902 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: get_thread_connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.020991086959838867s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844931.8177216
[0m00:22:11.838902 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.02118062973022461s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844931.8177216
[0m00:22:11.838902 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:22:11.838902 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"} */
drop table if exists `hive_metastore`.`saleslt_dbt_test__audit`.`unique_my_first_dbt_model_id`
[0m00:22:12.128019 [debug] [Thread-1 (]: SQL status: OK in 0.28999999165534973 seconds
[0m00:22:12.128019 [debug] [Thread-1 (]: Applying CREATE to: `hive_metastore`.`saleslt_dbt_test__audit`.`unique_my_first_dbt_model_id`
[0m00:22:12.128019 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: get_thread_connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.31029725074768066s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844931.8177216
[0m00:22:12.143559 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.3258378505706787s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844931.8177216
[0m00:22:12.143559 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:22:12.143559 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"} */

        
  
    
        create or replace table `hive_metastore`.`saleslt_dbt_test__audit`.`unique_my_first_dbt_model_id`
      
      
    using delta
      
      
      
      
      
      
      
      as
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



  
    
[0m00:22:14.102488 [debug] [Thread-1 (]: SQL status: OK in 1.9600000381469727 seconds
[0m00:22:14.102488 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:22:14.102488 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:22:14.102488 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: get_thread_connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 2.284766674041748s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844931.8177216
[0m00:22:14.102488 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 2.284766674041748s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844931.8177216
[0m00:22:14.102488 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:22:14.102488 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
        select *
        from `hive_metastore`.`saleslt_dbt_test__audit`.`unique_my_first_dbt_model_id`
    
      
    ) dbt_internal_test
[0m00:22:14.310732 [debug] [Thread-1 (]: SQL status: OK in 0.20999999344348907 seconds
[0m00:22:14.326390 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (execute): 00:22:11.817721 => 00:22:14.326390
[0m00:22:14.326390 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: _release sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844934.32639
[0m00:22:14.326390 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: _release sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844934.32639
[0m00:22:14.326390 [info ] [Thread-1 (]: 7 of 8 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 2.51s]
[0m00:22:14.326390 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:22:14.326390 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:22:14.326390 [info ] [Thread-1 (]: 8 of 8 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m00:22:14.326390 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844934.32639
[0m00:22:14.326390 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, now test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493)
[0m00:22:14.326390 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: reusing connection test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844934.32639
[0m00:22:14.326390 [debug] [Thread-1 (]: Databricks adapter: On thread (18648, 12268): None using default compute resource.
[0m00:22:14.326390 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: _acquire sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844934.32639
[0m00:22:14.326390 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:22:14.326390 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:22:14.343099 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (compile): 00:22:14.326390 => 00:22:14.342093
[0m00:22:14.344042 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:22:14.347801 [debug] [Thread-1 (]: Applying DROP to: `hive_metastore`.`saleslt_dbt_test__audit`.`unique_my_second_dbt_model_id`
[0m00:22:14.347801 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: get_thread_connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.02141094207763672s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844934.32639
[0m00:22:14.347801 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.02141094207763672s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844934.32639
[0m00:22:14.347801 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:22:14.347801 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"} */
drop table if exists `hive_metastore`.`saleslt_dbt_test__audit`.`unique_my_second_dbt_model_id`
[0m00:22:14.660169 [debug] [Thread-1 (]: SQL status: OK in 0.3100000023841858 seconds
[0m00:22:14.660169 [debug] [Thread-1 (]: Applying CREATE to: `hive_metastore`.`saleslt_dbt_test__audit`.`unique_my_second_dbt_model_id`
[0m00:22:14.660169 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: get_thread_connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.33377909660339355s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844934.32639
[0m00:22:14.660169 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.33377909660339355s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844934.32639
[0m00:22:14.660169 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:22:14.660169 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"} */

        
  
    
        create or replace table `hive_metastore`.`saleslt_dbt_test__audit`.`unique_my_second_dbt_model_id`
      
      
    using delta
      
      
      
      
      
      
      
      as
      
    
    

select
    id as unique_field,
    count(*) as n_records

from `hive_metastore`.`saleslt`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



  
    
[0m00:22:16.415316 [debug] [Thread-1 (]: SQL status: OK in 1.7599999904632568 seconds
[0m00:22:16.415316 [debug] [Thread-1 (]: Spark adapter: NotImplemented: commit
[0m00:22:16.415316 [debug] [Thread-1 (]: Writing runtime sql for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:22:16.415316 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: get_thread_connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 2.0889267921447754s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844934.32639
[0m00:22:16.415316 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: idle check connection: sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 2.0889267921447754s, acqrelcnt: 1, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844934.32639
[0m00:22:16.415316 [debug] [Thread-1 (]: Using databricks connection "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:22:16.415316 [debug] [Thread-1 (]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "node_id": "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
        select *
        from `hive_metastore`.`saleslt_dbt_test__audit`.`unique_my_second_dbt_model_id`
    
      
    ) dbt_internal_test
[0m00:22:16.625961 [debug] [Thread-1 (]: SQL status: OK in 0.20000000298023224 seconds
[0m00:22:16.625961 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (execute): 00:22:14.344042 => 00:22:16.625961
[0m00:22:16.625961 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: _release sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844936.6259615
[0m00:22:16.625961 [debug] [Thread-1 (]: Databricks adapter: conn: 2093278680016: _release sess: 4dbb5f6b-a5bf-47b1-bb9f-f5753993b125, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (18648, 12268), cmpt: ``, lut: 1707844936.6259615
[0m00:22:16.625961 [info ] [Thread-1 (]: 8 of 8 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 2.30s]
[0m00:22:16.625961 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:22:16.625961 [debug] [MainThread]: Databricks adapter: conn: 2093275862352: idle check connection: sess: None, name: master, idle: 21.456139087677002s, acqrelcnt: 0, lang: None, thrd: (18648, 6828), cmpt: ``, lut: 1707844915.1698225
[0m00:22:16.625961 [debug] [MainThread]: Databricks adapter: conn: 2093275862352: reusing connection master sess: None, name: master, idle: 21.456139087677002s, acqrelcnt: 0, lang: None, thrd: (18648, 6828), cmpt: ``, lut: 1707844915.1698225
[0m00:22:16.625961 [debug] [MainThread]: Databricks adapter: Thread (18648, 6828) using default compute resource.
[0m00:22:16.625961 [debug] [MainThread]: Databricks adapter: conn: 2093275862352: _acquire sess: None, name: master, idle: 21.456139087677002s, acqrelcnt: 1, lang: None, thrd: (18648, 6828), cmpt: ``, lut: 1707844915.1698225
[0m00:22:16.625961 [debug] [MainThread]: Databricks adapter: conn: 2093275862352: get_thread_connection: sess: None, name: master, idle: 21.456139087677002s, acqrelcnt: 1, lang: None, thrd: (18648, 6828), cmpt: ``, lut: 1707844915.1698225
[0m00:22:16.641616 [debug] [MainThread]: Databricks adapter: conn: 2093275862352: idle check connection: sess: None, name: master, idle: 21.47179388999939s, acqrelcnt: 1, lang: None, thrd: (18648, 6828), cmpt: ``, lut: 1707844915.1698225
[0m00:22:16.641616 [debug] [MainThread]: On master: ROLLBACK
[0m00:22:16.642545 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:22:16.876700 [debug] [MainThread]: Databricks adapter: conn: 2093275862352: session opened sess: e78cd0ec-bd7c-4a1b-9c0e-27bf76c976d3, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (18648, 6828), cmpt: ``, lut: 1707844936.8767
[0m00:22:16.878626 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:22:16.878626 [debug] [MainThread]: Databricks adapter: conn: 2093275862352: get_thread_connection: sess: e78cd0ec-bd7c-4a1b-9c0e-27bf76c976d3, name: master, idle: 0.0019268989562988281s, acqrelcnt: 1, lang: None, thrd: (18648, 6828), cmpt: ``, lut: 1707844936.8767
[0m00:22:16.878626 [debug] [MainThread]: Databricks adapter: conn: 2093275862352: idle check connection: sess: e78cd0ec-bd7c-4a1b-9c0e-27bf76c976d3, name: master, idle: 0.0019268989562988281s, acqrelcnt: 1, lang: None, thrd: (18648, 6828), cmpt: ``, lut: 1707844936.8767
[0m00:22:16.878626 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m00:22:16.878626 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m00:22:16.878626 [debug] [MainThread]: Databricks adapter: conn: 2093275862352: _release sess: e78cd0ec-bd7c-4a1b-9c0e-27bf76c976d3, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (18648, 6828), cmpt: ``, lut: 1707844936.8786268
[0m00:22:16.878626 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:22:16.878626 [debug] [MainThread]: On master: ROLLBACK
[0m00:22:16.878626 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:22:16.878626 [debug] [MainThread]: On master: Close
[0m00:22:16.961517 [debug] [MainThread]: Connection 'list_hive_metastore' was properly closed.
[0m00:22:16.961517 [debug] [MainThread]: On list_hive_metastore: Close
[0m00:22:17.046050 [debug] [MainThread]: Connection 'list_hive_metastore_saleslt_dbt_test__audit' was properly closed.
[0m00:22:17.046050 [debug] [MainThread]: On list_hive_metastore_saleslt_dbt_test__audit: ROLLBACK
[0m00:22:17.046050 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:22:17.046050 [debug] [MainThread]: On list_hive_metastore_saleslt_dbt_test__audit: Close
[0m00:22:17.127528 [debug] [MainThread]: Connection 'test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493' was properly closed.
[0m00:22:17.127528 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: ROLLBACK
[0m00:22:17.127528 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:22:17.127528 [debug] [MainThread]: On test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m00:22:17.217720 [info ] [MainThread]: 
[0m00:22:17.218733 [info ] [MainThread]: Finished running 8 tests in 0 hours 0 minutes and 24.23 seconds (24.23s).
[0m00:22:17.220812 [debug] [MainThread]: Command end result
[0m00:22:17.233222 [info ] [MainThread]: 
[0m00:22:17.234646 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m00:22:17.234646 [info ] [MainThread]: 
[0m00:22:17.235645 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models\example\schema.yml)[0m
[0m00:22:17.236380 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m00:22:17.237399 [info ] [MainThread]: 
[0m00:22:17.237399 [info ] [MainThread]:   compiled Code at target\compiled\medallion_dbt_spark\models\example\schema.yml\not_null_my_first_dbt_model_id.sql
[0m00:22:17.238398 [info ] [MainThread]: 
[0m00:22:17.239396 [info ] [MainThread]:   See test failures:
  -----------------------------------------------------------------------------------------
  select * from `hive_metastore`.`saleslt_dbt_test__audit`.`not_null_my_first_dbt_model_id`
  -----------------------------------------------------------------------------------------
[0m00:22:17.239396 [info ] [MainThread]: 
[0m00:22:17.239396 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=1 SKIP=0 TOTAL=8
[0m00:22:17.243226 [debug] [MainThread]: Command `dbt test` failed at 00:22:17.243226 after 25.83 seconds
[0m00:22:17.244235 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E74E7AEC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7478F1990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7478F0D90>]}
[0m00:22:17.244235 [debug] [MainThread]: Flushing usage events
[0m00:24:33.595674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002136B4CDD50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002136B1411D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002136B4B20D0>]}


============================== 00:24:33.601046 | e43015e3-2a8a-4ce1-bf7f-d0e3d05c90da ==============================
[0m00:24:33.601046 [info ] [MainThread]: Running with dbt=1.7.7
[0m00:24:33.601950 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt docs generate', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m00:24:35.211401 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e43015e3-2a8a-4ce1-bf7f-d0e3d05c90da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002137D589550>]}
[0m00:24:35.282318 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e43015e3-2a8a-4ce1-bf7f-d0e3d05c90da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002136B4CC4D0>]}
[0m00:24:35.283316 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m00:24:35.294963 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m00:24:35.391395 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m00:24:35.391395 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m00:24:35.397390 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e43015e3-2a8a-4ce1-bf7f-d0e3d05c90da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002137D779AD0>]}
[0m00:24:35.401335 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e43015e3-2a8a-4ce1-bf7f-d0e3d05c90da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002137D819190>]}
[0m00:24:35.401335 [info ] [MainThread]: Found 5 models, 7 snapshots, 8 tests, 9 sources, 0 exposures, 0 metrics, 535 macros, 0 groups, 0 semantic models
[0m00:24:35.403013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e43015e3-2a8a-4ce1-bf7f-d0e3d05c90da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002137D5B5ED0>]}
[0m00:24:35.405018 [info ] [MainThread]: 
[0m00:24:35.407104 [debug] [MainThread]: Databricks adapter: conn: 2282733281168: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (26240, 10484), cmpt: ``, lut: None
[0m00:24:35.407104 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m00:24:35.408021 [debug] [MainThread]: Databricks adapter: Thread (26240, 10484) using default compute resource.
[0m00:24:35.408021 [debug] [MainThread]: Databricks adapter: conn: 2282733281168: _acquire sess: None, name: master, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (26240, 10484), cmpt: ``, lut: 1707845075.4080217
[0m00:24:35.410109 [debug] [ThreadPool]: Databricks adapter: conn: 2282733120080: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_snapshots, idle: 0s, acqrelcnt: 0, lang: None, thrd: (26240, 21616), cmpt: ``, lut: None
[0m00:24:35.410109 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_snapshots'
[0m00:24:35.411024 [debug] [ThreadPool]: Databricks adapter: Thread (26240, 21616) using default compute resource.
[0m00:24:35.411024 [debug] [ThreadPool]: Databricks adapter: conn: 2282733120080: _acquire sess: None, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (26240, 21616), cmpt: ``, lut: 1707845075.4110243
[0m00:24:35.417023 [debug] [ThreadPool]: Databricks adapter: conn: 2282733120080: get_thread_connection: sess: None, name: list_hive_metastore_snapshots, idle: 0.005999088287353516s, acqrelcnt: 1, lang: None, thrd: (26240, 21616), cmpt: ``, lut: 1707845075.4110243
[0m00:24:35.418103 [debug] [ThreadPool]: Databricks adapter: conn: 2282733120080: idle check connection: sess: None, name: list_hive_metastore_snapshots, idle: 0.007078886032104492s, acqrelcnt: 1, lang: None, thrd: (26240, 21616), cmpt: ``, lut: 1707845075.4110243
[0m00:24:35.418103 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:24:35.419027 [debug] [ThreadPool]: On list_hive_metastore_snapshots: GetTables(database=hive_metastore, schema=snapshots, identifier=None)
[0m00:24:35.419027 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:24:35.987568 [debug] [ThreadPool]: Databricks adapter: conn: 2282733120080: session opened sess: 5e452c91-f072-440d-96ee-54e72588f8b3, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (26240, 21616), cmpt: ``, lut: 1707845075.9875689
[0m00:24:36.392663 [debug] [ThreadPool]: SQL status: OK in 0.9700000286102295 seconds
[0m00:24:36.414031 [debug] [ThreadPool]: Databricks adapter: conn: 2282733120080: get_thread_connection: sess: 5e452c91-f072-440d-96ee-54e72588f8b3, name: list_hive_metastore_snapshots, idle: 0.42646217346191406s, acqrelcnt: 1, lang: None, thrd: (26240, 21616), cmpt: ``, lut: 1707845075.9875689
[0m00:24:36.415033 [debug] [ThreadPool]: Databricks adapter: conn: 2282733120080: idle check connection: sess: 5e452c91-f072-440d-96ee-54e72588f8b3, name: list_hive_metastore_snapshots, idle: 0.42746424674987793s, acqrelcnt: 1, lang: None, thrd: (26240, 21616), cmpt: ``, lut: 1707845075.9875689
[0m00:24:36.415033 [debug] [ThreadPool]: Databricks adapter: conn: 2282733120080: get_thread_connection: sess: 5e452c91-f072-440d-96ee-54e72588f8b3, name: list_hive_metastore_snapshots, idle: 0.42746424674987793s, acqrelcnt: 1, lang: None, thrd: (26240, 21616), cmpt: ``, lut: 1707845075.9875689
[0m00:24:36.416033 [debug] [ThreadPool]: Databricks adapter: conn: 2282733120080: idle check connection: sess: 5e452c91-f072-440d-96ee-54e72588f8b3, name: list_hive_metastore_snapshots, idle: 0.4284646511077881s, acqrelcnt: 1, lang: None, thrd: (26240, 21616), cmpt: ``, lut: 1707845075.9875689
[0m00:24:36.416033 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:24:36.416033 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:24:36.417032 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */

      select current_catalog()
  
[0m00:24:36.758629 [debug] [ThreadPool]: SQL status: OK in 0.3400000035762787 seconds
[0m00:24:36.770579 [debug] [ThreadPool]: Databricks adapter: conn: 2282733120080: get_thread_connection: sess: 5e452c91-f072-440d-96ee-54e72588f8b3, name: list_hive_metastore_snapshots, idle: 0.7830102443695068s, acqrelcnt: 1, lang: None, thrd: (26240, 21616), cmpt: ``, lut: 1707845075.9875689
[0m00:24:36.771580 [debug] [ThreadPool]: Databricks adapter: conn: 2282733120080: idle check connection: sess: 5e452c91-f072-440d-96ee-54e72588f8b3, name: list_hive_metastore_snapshots, idle: 0.7840116024017334s, acqrelcnt: 1, lang: None, thrd: (26240, 21616), cmpt: ``, lut: 1707845075.9875689
[0m00:24:36.771580 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_snapshots"
[0m00:24:36.772778 [debug] [ThreadPool]: On list_hive_metastore_snapshots: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_snapshots"} */
show views in `hive_metastore`.`snapshots`
  
[0m00:24:36.923841 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m00:24:36.927920 [debug] [ThreadPool]: Databricks adapter: conn: 2282733120080: _release sess: 5e452c91-f072-440d-96ee-54e72588f8b3, name: list_hive_metastore_snapshots, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (26240, 21616), cmpt: ``, lut: 1707845076.9269304
[0m00:24:36.928868 [debug] [ThreadPool]: Databricks adapter: conn: 2282733120080: idle check connection: sess: 5e452c91-f072-440d-96ee-54e72588f8b3, name: list_hive_metastore_snapshots, idle: 0.0019383430480957031s, acqrelcnt: 0, lang: None, thrd: (26240, 21616), cmpt: ``, lut: 1707845076.9269304
[0m00:24:36.931102 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore_snapshots, now list_hive_metastore_saleslt)
[0m00:24:36.931102 [debug] [ThreadPool]: Databricks adapter: conn: 2282733120080: reusing connection list_hive_metastore_snapshots sess: 5e452c91-f072-440d-96ee-54e72588f8b3, name: list_hive_metastore_saleslt, idle: 0.004172325134277344s, acqrelcnt: 0, lang: None, thrd: (26240, 21616), cmpt: ``, lut: 1707845076.9269304
[0m00:24:36.931102 [debug] [ThreadPool]: Databricks adapter: Thread (26240, 21616) using default compute resource.
[0m00:24:36.932112 [debug] [ThreadPool]: Databricks adapter: conn: 2282733120080: _acquire sess: 5e452c91-f072-440d-96ee-54e72588f8b3, name: list_hive_metastore_saleslt, idle: 0.005182027816772461s, acqrelcnt: 1, lang: None, thrd: (26240, 21616), cmpt: ``, lut: 1707845076.9269304
[0m00:24:36.935102 [debug] [ThreadPool]: Databricks adapter: conn: 2282733120080: get_thread_connection: sess: 5e452c91-f072-440d-96ee-54e72588f8b3, name: list_hive_metastore_saleslt, idle: 0.007172107696533203s, acqrelcnt: 1, lang: None, thrd: (26240, 21616), cmpt: ``, lut: 1707845076.9269304
[0m00:24:36.935102 [debug] [ThreadPool]: Databricks adapter: conn: 2282733120080: idle check connection: sess: 5e452c91-f072-440d-96ee-54e72588f8b3, name: list_hive_metastore_saleslt, idle: 0.008172273635864258s, acqrelcnt: 1, lang: None, thrd: (26240, 21616), cmpt: ``, lut: 1707845076.9269304
[0m00:24:36.936103 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:24:36.936103 [debug] [ThreadPool]: On list_hive_metastore_saleslt: GetTables(database=hive_metastore, schema=saleslt, identifier=None)
[0m00:24:37.051841 [debug] [ThreadPool]: SQL status: OK in 0.10999999940395355 seconds
[0m00:24:37.058175 [debug] [ThreadPool]: Databricks adapter: conn: 2282733120080: get_thread_connection: sess: 5e452c91-f072-440d-96ee-54e72588f8b3, name: list_hive_metastore_saleslt, idle: 0.13124513626098633s, acqrelcnt: 1, lang: None, thrd: (26240, 21616), cmpt: ``, lut: 1707845076.9269304
[0m00:24:37.058175 [debug] [ThreadPool]: Databricks adapter: conn: 2282733120080: idle check connection: sess: 5e452c91-f072-440d-96ee-54e72588f8b3, name: list_hive_metastore_saleslt, idle: 0.13124513626098633s, acqrelcnt: 1, lang: None, thrd: (26240, 21616), cmpt: ``, lut: 1707845076.9269304
[0m00:24:37.059074 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:24:37.059074 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */

      select current_catalog()
  
[0m00:24:37.185617 [debug] [ThreadPool]: SQL status: OK in 0.12999999523162842 seconds
[0m00:24:37.189523 [debug] [ThreadPool]: Databricks adapter: conn: 2282733120080: get_thread_connection: sess: 5e452c91-f072-440d-96ee-54e72588f8b3, name: list_hive_metastore_saleslt, idle: 0.2625925540924072s, acqrelcnt: 1, lang: None, thrd: (26240, 21616), cmpt: ``, lut: 1707845076.9269304
[0m00:24:37.190536 [debug] [ThreadPool]: Databricks adapter: conn: 2282733120080: idle check connection: sess: 5e452c91-f072-440d-96ee-54e72588f8b3, name: list_hive_metastore_saleslt, idle: 0.26360607147216797s, acqrelcnt: 1, lang: None, thrd: (26240, 21616), cmpt: ``, lut: 1707845076.9269304
[0m00:24:37.190536 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_saleslt"
[0m00:24:37.190536 [debug] [ThreadPool]: On list_hive_metastore_saleslt: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "list_hive_metastore_saleslt"} */
show views in `hive_metastore`.`saleslt`
  
[0m00:24:37.357666 [debug] [ThreadPool]: SQL status: OK in 0.17000000178813934 seconds
[0m00:24:37.361680 [debug] [ThreadPool]: Databricks adapter: conn: 2282733120080: _release sess: 5e452c91-f072-440d-96ee-54e72588f8b3, name: list_hive_metastore_saleslt, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (26240, 21616), cmpt: ``, lut: 1707845077.361681
[0m00:24:37.365768 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e43015e3-2a8a-4ce1-bf7f-d0e3d05c90da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002137D5B6F10>]}
[0m00:24:37.366785 [debug] [MainThread]: Databricks adapter: conn: 2282733281168: _release sess: None, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (26240, 10484), cmpt: ``, lut: 1707845077.3667858
[0m00:24:37.366785 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m00:24:37.367703 [info ] [MainThread]: 
[0m00:24:37.374910 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.my_first_dbt_model
[0m00:24:37.375908 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: Creating DatabricksDBTConnection sess: None, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0s, acqrelcnt: 0, lang: None, thrd: (26240, 8308), cmpt: ``, lut: None
[0m00:24:37.376908 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.medallion_dbt_spark.my_first_dbt_model'
[0m00:24:37.376908 [debug] [Thread-1 (]: Databricks adapter: On thread (26240, 8308): `hive_metastore`.`saleslt`.`my_first_dbt_model` using default compute resource.
[0m00:24:37.376908 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _acquire sess: None, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.0s, acqrelcnt: 1, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.3769088
[0m00:24:37.377908 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.my_first_dbt_model
[0m00:24:37.387167 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.my_first_dbt_model"
[0m00:24:37.388165 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.my_first_dbt_model (compile): 00:24:37.377908 => 00:24:37.388165
[0m00:24:37.389169 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.my_first_dbt_model
[0m00:24:37.390168 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.my_first_dbt_model (execute): 00:24:37.389169 => 00:24:37.389169
[0m00:24:37.390168 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.3901684
[0m00:24:37.391176 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.3911765
[0m00:24:37.392179 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.my_first_dbt_model
[0m00:24:37.392179 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.address_snapshot
[0m00:24:37.393176 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: idle check connection: sess: None, name: model.medallion_dbt_spark.my_first_dbt_model, idle: 0.0019996166229248047s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.3911765
[0m00:24:37.394248 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.my_first_dbt_model, now snapshot.medallion_dbt_spark.address_snapshot)
[0m00:24:37.395168 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: reusing connection model.medallion_dbt_spark.my_first_dbt_model sess: None, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 0.003072023391723633s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.3911765
[0m00:24:37.395168 [debug] [Thread-1 (]: Databricks adapter: On thread (26240, 8308): `hive_metastore`.`snapshots`.`address_snapshot` using default compute resource.
[0m00:24:37.395168 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _acquire sess: None, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 0.003991603851318359s, acqrelcnt: 1, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.3911765
[0m00:24:37.396166 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.address_snapshot
[0m00:24:37.401538 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.address_snapshot (compile): 00:24:37.396166 => 00:24:37.400558
[0m00:24:37.401538 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.address_snapshot
[0m00:24:37.402629 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.address_snapshot (execute): 00:24:37.402629 => 00:24:37.402629
[0m00:24:37.403539 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.4026296
[0m00:24:37.404538 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.4045386
[0m00:24:37.404538 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.address_snapshot
[0m00:24:37.405562 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.customer_snapshot
[0m00:24:37.406536 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: idle check connection: sess: None, name: snapshot.medallion_dbt_spark.address_snapshot, idle: 0.0019981861114501953s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.4045386
[0m00:24:37.406536 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.address_snapshot, now snapshot.medallion_dbt_spark.customer_snapshot)
[0m00:24:37.407535 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: reusing connection snapshot.medallion_dbt_spark.address_snapshot sess: None, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 0.0029964447021484375s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.4045386
[0m00:24:37.407535 [debug] [Thread-1 (]: Databricks adapter: On thread (26240, 8308): `hive_metastore`.`snapshots`.`customer_snapshot` using default compute resource.
[0m00:24:37.408534 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _acquire sess: None, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 0.0029964447021484375s, acqrelcnt: 1, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.4045386
[0m00:24:37.408534 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.customer_snapshot
[0m00:24:37.412555 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.customer_snapshot (compile): 00:24:37.408534 => 00:24:37.411553
[0m00:24:37.412555 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.customer_snapshot
[0m00:24:37.413630 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.customer_snapshot (execute): 00:24:37.412555 => 00:24:37.412555
[0m00:24:37.413630 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.4136302
[0m00:24:37.415535 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.414569
[0m00:24:37.415535 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.customer_snapshot
[0m00:24:37.416538 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m00:24:37.417536 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: idle check connection: sess: None, name: snapshot.medallion_dbt_spark.customer_snapshot, idle: 0.0029671192169189453s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.414569
[0m00:24:37.418536 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.customer_snapshot, now snapshot.medallion_dbt_spark.customeraddress_snapshot)
[0m00:24:37.418536 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: reusing connection snapshot.medallion_dbt_spark.customer_snapshot sess: None, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 0.0039675235748291016s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.414569
[0m00:24:37.419536 [debug] [Thread-1 (]: Databricks adapter: On thread (26240, 8308): `hive_metastore`.`snapshots`.`customeraddress_snapshot` using default compute resource.
[0m00:24:37.420537 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _acquire sess: None, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 0.005968809127807617s, acqrelcnt: 1, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.414569
[0m00:24:37.420537 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m00:24:37.424968 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.customeraddress_snapshot (compile): 00:24:37.421576 => 00:24:37.424968
[0m00:24:37.425973 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m00:24:37.426900 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.customeraddress_snapshot (execute): 00:24:37.425973 => 00:24:37.426900
[0m00:24:37.426900 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.426901
[0m00:24:37.427939 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.42794
[0m00:24:37.428902 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.customeraddress_snapshot
[0m00:24:37.428902 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.product_snapshot
[0m00:24:37.430181 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: idle check connection: sess: None, name: snapshot.medallion_dbt_spark.customeraddress_snapshot, idle: 0.0022420883178710938s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.42794
[0m00:24:37.430181 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.customeraddress_snapshot, now snapshot.medallion_dbt_spark.product_snapshot)
[0m00:24:37.431185 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: reusing connection snapshot.medallion_dbt_spark.customeraddress_snapshot sess: None, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 0.003245830535888672s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.42794
[0m00:24:37.432233 [debug] [Thread-1 (]: Databricks adapter: On thread (26240, 8308): `hive_metastore`.`snapshots`.`product_snapshot` using default compute resource.
[0m00:24:37.433183 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _acquire sess: None, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 0.004292964935302734s, acqrelcnt: 1, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.42794
[0m00:24:37.433183 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.product_snapshot
[0m00:24:37.437281 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.product_snapshot (compile): 00:24:37.434184 => 00:24:37.437281
[0m00:24:37.438184 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.product_snapshot
[0m00:24:37.438184 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.product_snapshot (execute): 00:24:37.438184 => 00:24:37.438184
[0m00:24:37.439183 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.4381847
[0m00:24:37.439183 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.4391837
[0m00:24:37.440184 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.product_snapshot
[0m00:24:37.440184 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.productmodel_snapshot
[0m00:24:37.441230 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: idle check connection: sess: None, name: snapshot.medallion_dbt_spark.product_snapshot, idle: 0.0020465850830078125s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.4391837
[0m00:24:37.442240 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.product_snapshot, now snapshot.medallion_dbt_spark.productmodel_snapshot)
[0m00:24:37.442240 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: reusing connection snapshot.medallion_dbt_spark.product_snapshot sess: None, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 0.0030570030212402344s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.4391837
[0m00:24:37.442240 [debug] [Thread-1 (]: Databricks adapter: On thread (26240, 8308): `hive_metastore`.`snapshots`.`productmodel_snapshot` using default compute resource.
[0m00:24:37.443184 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _acquire sess: None, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 0.00400090217590332s, acqrelcnt: 1, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.4391837
[0m00:24:37.443184 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.productmodel_snapshot
[0m00:24:37.447266 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.productmodel_snapshot (compile): 00:24:37.444225 => 00:24:37.446214
[0m00:24:37.447657 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.productmodel_snapshot
[0m00:24:37.447657 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.productmodel_snapshot (execute): 00:24:37.447657 => 00:24:37.447657
[0m00:24:37.448662 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.4486628
[0m00:24:37.449659 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.4496598
[0m00:24:37.450660 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.productmodel_snapshot
[0m00:24:37.451657 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m00:24:37.452663 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: idle check connection: sess: None, name: snapshot.medallion_dbt_spark.productmodel_snapshot, idle: 0.0030031204223632812s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.4496598
[0m00:24:37.452663 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.productmodel_snapshot, now snapshot.medallion_dbt_spark.salesorderdetail_snapshot)
[0m00:24:37.453659 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: reusing connection snapshot.medallion_dbt_spark.productmodel_snapshot sess: None, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 0.003999471664428711s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.4496598
[0m00:24:37.453659 [debug] [Thread-1 (]: Databricks adapter: On thread (26240, 8308): `hive_metastore`.`snapshots`.`salesorderdetail_snapshot` using default compute resource.
[0m00:24:37.454661 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _acquire sess: None, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 0.003999471664428711s, acqrelcnt: 1, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.4496598
[0m00:24:37.454825 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m00:24:37.457830 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.salesorderdetail_snapshot (compile): 00:24:37.454825 => 00:24:37.457830
[0m00:24:37.458830 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m00:24:37.458830 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.salesorderdetail_snapshot (execute): 00:24:37.458830 => 00:24:37.458830
[0m00:24:37.459840 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.459841
[0m00:24:37.459840 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.459841
[0m00:24:37.460831 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.salesorderdetail_snapshot
[0m00:24:37.461832 [debug] [Thread-1 (]: Began running node snapshot.medallion_dbt_spark.salesorderheader_snapshot
[0m00:24:37.462831 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: idle check connection: sess: None, name: snapshot.medallion_dbt_spark.salesorderdetail_snapshot, idle: 0.0019910335540771484s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.459841
[0m00:24:37.462831 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.salesorderdetail_snapshot, now snapshot.medallion_dbt_spark.salesorderheader_snapshot)
[0m00:24:37.463831 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: reusing connection snapshot.medallion_dbt_spark.salesorderdetail_snapshot sess: None, name: snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle: 0.0029900074005126953s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.459841
[0m00:24:37.463831 [debug] [Thread-1 (]: Databricks adapter: On thread (26240, 8308): `hive_metastore`.`snapshots`.`salesorderheader_snapshot` using default compute resource.
[0m00:24:37.464832 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _acquire sess: None, name: snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle: 0.004991054534912109s, acqrelcnt: 1, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.459841
[0m00:24:37.464832 [debug] [Thread-1 (]: Began compiling node snapshot.medallion_dbt_spark.salesorderheader_snapshot
[0m00:24:37.470829 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.salesorderheader_snapshot (compile): 00:24:37.465831 => 00:24:37.470829
[0m00:24:37.471831 [debug] [Thread-1 (]: Began executing node snapshot.medallion_dbt_spark.salesorderheader_snapshot
[0m00:24:37.471831 [debug] [Thread-1 (]: Timing info for snapshot.medallion_dbt_spark.salesorderheader_snapshot (execute): 00:24:37.471831 => 00:24:37.471831
[0m00:24:37.472831 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.4718318
[0m00:24:37.472831 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.4728312
[0m00:24:37.473830 [debug] [Thread-1 (]: Finished running node snapshot.medallion_dbt_spark.salesorderheader_snapshot
[0m00:24:37.474831 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.my_second_dbt_model
[0m00:24:37.474831 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: idle check connection: sess: None, name: snapshot.medallion_dbt_spark.salesorderheader_snapshot, idle: 0.002000570297241211s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.4728312
[0m00:24:37.475831 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly snapshot.medallion_dbt_spark.salesorderheader_snapshot, now model.medallion_dbt_spark.my_second_dbt_model)
[0m00:24:37.476827 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: reusing connection snapshot.medallion_dbt_spark.salesorderheader_snapshot sess: None, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.0030002593994140625s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.4728312
[0m00:24:37.476827 [debug] [Thread-1 (]: Databricks adapter: On thread (26240, 8308): `hive_metastore`.`saleslt`.`my_second_dbt_model` using default compute resource.
[0m00:24:37.476827 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _acquire sess: None, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.0039958953857421875s, acqrelcnt: 1, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.4728312
[0m00:24:37.477826 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.my_second_dbt_model
[0m00:24:37.481825 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.my_second_dbt_model"
[0m00:24:37.482826 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.my_second_dbt_model (compile): 00:24:37.477826 => 00:24:37.481825
[0m00:24:37.482826 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.my_second_dbt_model
[0m00:24:37.483826 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.my_second_dbt_model (execute): 00:24:37.483826 => 00:24:37.483826
[0m00:24:37.483826 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.483826
[0m00:24:37.484825 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.4848256
[0m00:24:37.485826 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.my_second_dbt_model
[0m00:24:37.485826 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:24:37.487222 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: idle check connection: sess: None, name: model.medallion_dbt_spark.my_second_dbt_model, idle: 0.0023965835571289062s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.4848256
[0m00:24:37.487222 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.my_second_dbt_model, now test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710)
[0m00:24:37.488231 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: reusing connection model.medallion_dbt_spark.my_second_dbt_model sess: None, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0034055709838867188s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.4848256
[0m00:24:37.488231 [debug] [Thread-1 (]: Databricks adapter: On thread (26240, 8308): None using default compute resource.
[0m00:24:37.489229 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _acquire sess: None, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0034055709838867188s, acqrelcnt: 1, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.4848256
[0m00:24:37.489229 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:24:37.499228 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710"
[0m00:24:37.500229 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 00:24:37.489229 => 00:24:37.500229
[0m00:24:37.500229 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:24:37.501245 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 00:24:37.501245 => 00:24:37.501245
[0m00:24:37.501245 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5012457
[0m00:24:37.502229 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.502229
[0m00:24:37.503229 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710
[0m00:24:37.503229 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:24:37.504384 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, idle: 0.002155780792236328s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.502229
[0m00:24:37.505461 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710, now test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321)
[0m00:24:37.505461 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: reusing connection test.medallion_dbt_spark.not_null_my_first_dbt_model_id.5fb22c2710 sess: None, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0032324790954589844s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.502229
[0m00:24:37.506470 [debug] [Thread-1 (]: Databricks adapter: On thread (26240, 8308): None using default compute resource.
[0m00:24:37.506470 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _acquire sess: None, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0042417049407958984s, acqrelcnt: 1, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.502229
[0m00:24:37.506470 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:24:37.512468 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321"
[0m00:24:37.513481 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (compile): 00:24:37.507387 => 00:24:37.513481
[0m00:24:37.514388 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:24:37.514388 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 (execute): 00:24:37.514388 => 00:24:37.514388
[0m00:24:37.514388 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5143886
[0m00:24:37.515385 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5153854
[0m00:24:37.516427 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321
[0m00:24:37.516427 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.dim_customer
[0m00:24:37.517391 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: idle check connection: sess: None, name: test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, idle: 0.0020058155059814453s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5153854
[0m00:24:37.517391 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321, now model.medallion_dbt_spark.dim_customer)
[0m00:24:37.518387 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: reusing connection test.medallion_dbt_spark.unique_my_first_dbt_model_id.16e066b321 sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0.003002166748046875s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5153854
[0m00:24:37.518387 [debug] [Thread-1 (]: Databricks adapter: On thread (26240, 8308): `hive_metastore`.`saleslt`.`dim_customer` using default compute resource.
[0m00:24:37.519422 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _acquire sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0.004037380218505859s, acqrelcnt: 1, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5153854
[0m00:24:37.519422 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.dim_customer
[0m00:24:37.522758 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.dim_customer"
[0m00:24:37.523792 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_customer (compile): 00:24:37.519422 => 00:24:37.523792
[0m00:24:37.523792 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.dim_customer
[0m00:24:37.524770 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_customer (execute): 00:24:37.524770 => 00:24:37.524770
[0m00:24:37.524770 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5247703
[0m00:24:37.525761 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5257616
[0m00:24:37.526763 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.dim_customer
[0m00:24:37.526763 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.dim_product
[0m00:24:37.527764 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: idle check connection: sess: None, name: model.medallion_dbt_spark.dim_customer, idle: 0.0020029544830322266s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5257616
[0m00:24:37.527764 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.dim_customer, now model.medallion_dbt_spark.dim_product)
[0m00:24:37.528783 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: reusing connection model.medallion_dbt_spark.dim_customer sess: None, name: model.medallion_dbt_spark.dim_product, idle: 0.0030221939086914062s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5257616
[0m00:24:37.528783 [debug] [Thread-1 (]: Databricks adapter: On thread (26240, 8308): `hive_metastore`.`saleslt`.`dim_product` using default compute resource.
[0m00:24:37.530002 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _acquire sess: None, name: model.medallion_dbt_spark.dim_product, idle: 0.0040972232818603516s, acqrelcnt: 1, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5257616
[0m00:24:37.530002 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.dim_product
[0m00:24:37.533080 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.dim_product"
[0m00:24:37.534081 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_product (compile): 00:24:37.530002 => 00:24:37.534081
[0m00:24:37.535007 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.dim_product
[0m00:24:37.535007 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_product (execute): 00:24:37.535007 => 00:24:37.535007
[0m00:24:37.535007 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: model.medallion_dbt_spark.dim_product, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5350075
[0m00:24:37.536008 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: model.medallion_dbt_spark.dim_product, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5360086
[0m00:24:37.537079 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.dim_product
[0m00:24:37.537079 [debug] [Thread-1 (]: Began running node model.medallion_dbt_spark.dim_sales
[0m00:24:37.538016 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: idle check connection: sess: None, name: model.medallion_dbt_spark.dim_product, idle: 0.0020079612731933594s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5360086
[0m00:24:37.539077 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.dim_product, now model.medallion_dbt_spark.dim_sales)
[0m00:24:37.539077 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: reusing connection model.medallion_dbt_spark.dim_product sess: None, name: model.medallion_dbt_spark.dim_sales, idle: 0.0030694007873535156s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5360086
[0m00:24:37.540037 [debug] [Thread-1 (]: Databricks adapter: On thread (26240, 8308): `hive_metastore`.`saleslt`.`dim_sales` using default compute resource.
[0m00:24:37.540037 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _acquire sess: None, name: model.medallion_dbt_spark.dim_sales, idle: 0.0040285587310791016s, acqrelcnt: 1, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5360086
[0m00:24:37.540037 [debug] [Thread-1 (]: Began compiling node model.medallion_dbt_spark.dim_sales
[0m00:24:37.544074 [debug] [Thread-1 (]: Writing injected SQL for node "model.medallion_dbt_spark.dim_sales"
[0m00:24:37.545048 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_sales (compile): 00:24:37.541001 => 00:24:37.545048
[0m00:24:37.546000 [debug] [Thread-1 (]: Began executing node model.medallion_dbt_spark.dim_sales
[0m00:24:37.546000 [debug] [Thread-1 (]: Timing info for model.medallion_dbt_spark.dim_sales (execute): 00:24:37.546000 => 00:24:37.546000
[0m00:24:37.547000 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: model.medallion_dbt_spark.dim_sales, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5460007
[0m00:24:37.547000 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: model.medallion_dbt_spark.dim_sales, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5470006
[0m00:24:37.548323 [debug] [Thread-1 (]: Finished running node model.medallion_dbt_spark.dim_sales
[0m00:24:37.548323 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:24:37.549328 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: idle check connection: sess: None, name: model.medallion_dbt_spark.dim_sales, idle: 0.0023272037506103516s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5470006
[0m00:24:37.549328 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.medallion_dbt_spark.dim_sales, now test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778)
[0m00:24:37.550388 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: reusing connection model.medallion_dbt_spark.dim_sales sess: None, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0023272037506103516s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5470006
[0m00:24:37.550388 [debug] [Thread-1 (]: Databricks adapter: On thread (26240, 8308): None using default compute resource.
[0m00:24:37.551329 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _acquire sess: None, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.003387928009033203s, acqrelcnt: 1, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5470006
[0m00:24:37.551329 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:24:37.554603 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778"
[0m00:24:37.555696 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (compile): 00:24:37.551329 => 00:24:37.555696
[0m00:24:37.556606 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:24:37.556606 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 (execute): 00:24:37.556606 => 00:24:37.556606
[0m00:24:37.557618 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5576184
[0m00:24:37.558645 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5576184
[0m00:24:37.558645 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778
[0m00:24:37.559684 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:24:37.560608 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, idle: 0.002065896987915039s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5576184
[0m00:24:37.560608 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778, now test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493)
[0m00:24:37.560608 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: reusing connection test.medallion_dbt_spark.not_null_my_second_dbt_model_id.151b76d778 sess: None, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.002990245819091797s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5576184
[0m00:24:37.561603 [debug] [Thread-1 (]: Databricks adapter: On thread (26240, 8308): None using default compute resource.
[0m00:24:37.561603 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _acquire sess: None, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.003985166549682617s, acqrelcnt: 1, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5576184
[0m00:24:37.562659 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:24:37.565685 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493"
[0m00:24:37.566694 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (compile): 00:24:37.562659 => 00:24:37.566694
[0m00:24:37.567602 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:24:37.567602 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 (execute): 00:24:37.567602 => 00:24:37.567602
[0m00:24:37.568640 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.56864
[0m00:24:37.569638 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.56864
[0m00:24:37.569638 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493
[0m00:24:37.570602 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:24:37.570602 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: idle check connection: sess: None, name: test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, idle: 0.001962423324584961s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.56864
[0m00:24:37.571959 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493, now test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5)
[0m00:24:37.571959 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: reusing connection test.medallion_dbt_spark.unique_my_second_dbt_model_id.57a0f8c493 sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.003319263458251953s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.56864
[0m00:24:37.573001 [debug] [Thread-1 (]: Databricks adapter: On thread (26240, 8308): None using default compute resource.
[0m00:24:37.573001 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _acquire sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.004361152648925781s, acqrelcnt: 1, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.56864
[0m00:24:37.573001 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:24:37.576960 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5"
[0m00:24:37.578048 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (compile): 00:24:37.573966 => 00:24:37.578048
[0m00:24:37.578986 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:24:37.578986 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 (execute): 00:24:37.578986 => 00:24:37.578986
[0m00:24:37.579964 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5789866
[0m00:24:37.580134 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5801342
[0m00:24:37.581224 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5
[0m00:24:37.581224 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:24:37.582158 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, idle: 0.002023935317993164s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5801342
[0m00:24:37.583214 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5, now test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417)
[0m00:24:37.583214 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_name.1b672622d5 sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0030798912048339844s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5801342
[0m00:24:37.584147 [debug] [Thread-1 (]: Databricks adapter: On thread (26240, 8308): None using default compute resource.
[0m00:24:37.584147 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _acquire sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.004013538360595703s, acqrelcnt: 1, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5801342
[0m00:24:37.585136 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:24:37.591228 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417"
[0m00:24:37.592138 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (compile): 00:24:37.585136 => 00:24:37.592138
[0m00:24:37.592138 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:24:37.593144 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 (execute): 00:24:37.593144 => 00:24:37.593144
[0m00:24:37.593144 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5931444
[0m00:24:37.594136 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5941362
[0m00:24:37.595211 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417
[0m00:24:37.595211 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:24:37.596166 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, idle: 0.0020303726196289062s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5941362
[0m00:24:37.597229 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417, now test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546)
[0m00:24:37.597485 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: reusing connection test.medallion_dbt_spark.not_null_dim_product_product_sk.271feb5417 sess: None, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0033495426177978516s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5941362
[0m00:24:37.597485 [debug] [Thread-1 (]: Databricks adapter: On thread (26240, 8308): None using default compute resource.
[0m00:24:37.598522 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _acquire sess: None, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0033495426177978516s, acqrelcnt: 1, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.5941362
[0m00:24:37.598522 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:24:37.602569 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546"
[0m00:24:37.603494 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (compile): 00:24:37.598522 => 00:24:37.603494
[0m00:24:37.603494 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:24:37.604635 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 (execute): 00:24:37.603494 => 00:24:37.603494
[0m00:24:37.604769 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.6047695
[0m00:24:37.605800 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.6047695
[0m00:24:37.605800 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546
[0m00:24:37.606806 [debug] [Thread-1 (]: Began running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:24:37.606806 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: idle check connection: sess: None, name: test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, idle: 0.00203704833984375s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.6047695
[0m00:24:37.607842 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546, now test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c)
[0m00:24:37.607842 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: reusing connection test.medallion_dbt_spark.not_null_dim_product_sellstartdate.40b1801546 sess: None, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.003073453903198242s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.6047695
[0m00:24:37.608799 [debug] [Thread-1 (]: Databricks adapter: On thread (26240, 8308): None using default compute resource.
[0m00:24:37.608799 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _acquire sess: None, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.004030466079711914s, acqrelcnt: 1, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.6047695
[0m00:24:37.609797 [debug] [Thread-1 (]: Began compiling node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:24:37.612848 [debug] [Thread-1 (]: Writing injected SQL for node "test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c"
[0m00:24:37.613852 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (compile): 00:24:37.609797 => 00:24:37.613852
[0m00:24:37.614772 [debug] [Thread-1 (]: Began executing node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:24:37.614772 [debug] [Thread-1 (]: Timing info for test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c (execute): 00:24:37.614772 => 00:24:37.614772
[0m00:24:37.615829 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.6158297
[0m00:24:37.615829 [debug] [Thread-1 (]: Databricks adapter: conn: 2282732536144: _release sess: None, name: test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (26240, 8308), cmpt: ``, lut: 1707845077.6158297
[0m00:24:37.616855 [debug] [Thread-1 (]: Finished running node test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c
[0m00:24:37.617769 [debug] [MainThread]: Connection 'master' was properly closed.
[0m00:24:37.618841 [debug] [MainThread]: Connection 'list_hive_metastore_saleslt' was properly closed.
[0m00:24:37.618841 [debug] [MainThread]: On list_hive_metastore_saleslt: ROLLBACK
[0m00:24:37.618841 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:24:37.619771 [debug] [MainThread]: On list_hive_metastore_saleslt: Close
[0m00:24:37.696335 [debug] [MainThread]: Connection 'test.medallion_dbt_spark.unique_dim_product_product_sk.2248c4335c' was properly closed.
[0m00:24:37.701595 [debug] [MainThread]: Command end result
[0m00:24:37.965935 [debug] [MainThread]: Databricks adapter: conn: 2282738513872: Creating DatabricksDBTConnection sess: None, name: generate_catalog, idle: 0s, acqrelcnt: 0, lang: None, thrd: (26240, 10484), cmpt: ``, lut: None
[0m00:24:37.966936 [debug] [MainThread]: Acquiring new databricks connection 'generate_catalog'
[0m00:24:37.966936 [debug] [MainThread]: Databricks adapter: Thread (26240, 10484) using default compute resource.
[0m00:24:37.967935 [debug] [MainThread]: Databricks adapter: conn: 2282738513872: _acquire sess: None, name: generate_catalog, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (26240, 10484), cmpt: ``, lut: 1707845077.9679353
[0m00:24:37.967935 [info ] [MainThread]: Building catalog
[0m00:24:37.972862 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: Creating DatabricksDBTConnection sess: None, name: hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (26240, 9524), cmpt: ``, lut: None
[0m00:24:37.973863 [debug] [ThreadPool]: Acquiring new databricks connection 'hive_metastore'
[0m00:24:37.973863 [debug] [ThreadPool]: Databricks adapter: Thread (26240, 9524) using default compute resource.
[0m00:24:37.974862 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: _acquire sess: None, name: hive_metastore, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845077.9738638
[0m00:24:37.976863 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: get_thread_connection: sess: None, name: hive_metastore, idle: 0.002999544143676758s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845077.9738638
[0m00:24:37.977863 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: idle check connection: sess: None, name: hive_metastore, idle: 0.002999544143676758s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845077.9738638
[0m00:24:37.977863 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: get_thread_connection: sess: None, name: hive_metastore, idle: 0.003999471664428711s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845077.9738638
[0m00:24:37.978862 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: idle check connection: sess: None, name: hive_metastore, idle: 0.004998207092285156s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845077.9738638
[0m00:24:37.978862 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m00:24:37.979863 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m00:24:37.979863 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "hive_metastore"} */

      select current_catalog()
  
[0m00:24:37.979863 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m00:24:38.205823 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: session opened sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845078.2058234
[0m00:24:38.338572 [debug] [ThreadPool]: SQL status: OK in 0.36000001430511475 seconds
[0m00:24:38.345570 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: get_thread_connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 0.13974690437316895s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845078.2058234
[0m00:24:38.347570 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: idle check connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 0.1407473087310791s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845078.2058234
[0m00:24:38.347570 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m00:24:38.347570 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "hive_metastore"} */
show table extended in `hive_metastore`.`snapshots` like 'customeraddress_snapshot|customer_snapshot|productmodel_snapshot|salesorderheader_snapshot|address_snapshot|salesorderdetail_snapshot|product_snapshot'
  
[0m00:24:39.086205 [debug] [ThreadPool]: SQL status: OK in 0.7400000095367432 seconds
[0m00:24:39.091744 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`snapshots`.`address_snapshot`
[0m00:24:39.099135 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: get_thread_connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 0.892324686050415s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845078.2058234
[0m00:24:39.099135 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: idle check connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 0.8933122158050537s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845078.2058234
[0m00:24:39.100050 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m00:24:39.100050 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`snapshots`.`address_snapshot`
  
[0m00:24:39.341164 [debug] [ThreadPool]: SQL status: OK in 0.23999999463558197 seconds
[0m00:24:39.343109 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`snapshots`.`customer_snapshot`
[0m00:24:39.346158 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: get_thread_connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 1.1393287181854248s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845078.2058234
[0m00:24:39.346158 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: idle check connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 1.1403353214263916s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845078.2058234
[0m00:24:39.346158 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m00:24:39.347089 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`snapshots`.`customer_snapshot`
  
[0m00:24:39.527187 [debug] [ThreadPool]: SQL status: OK in 0.18000000715255737 seconds
[0m00:24:39.532692 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`snapshots`.`customeraddress_snapshot`
[0m00:24:39.536693 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: get_thread_connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 1.3298752307891846s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845078.2058234
[0m00:24:39.536693 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: idle check connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 1.3308701515197754s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845078.2058234
[0m00:24:39.536693 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m00:24:39.537608 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`snapshots`.`customeraddress_snapshot`
  
[0m00:24:39.711463 [debug] [ThreadPool]: SQL status: OK in 0.17000000178813934 seconds
[0m00:24:39.717373 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`snapshots`.`product_snapshot`
[0m00:24:39.722370 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: get_thread_connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 1.5155456066131592s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845078.2058234
[0m00:24:39.722781 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: idle check connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 1.5169575214385986s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845078.2058234
[0m00:24:39.722781 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m00:24:39.723782 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`snapshots`.`product_snapshot`
  
[0m00:24:39.902187 [debug] [ThreadPool]: SQL status: OK in 0.18000000715255737 seconds
[0m00:24:39.905412 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`snapshots`.`productmodel_snapshot`
[0m00:24:39.908410 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: get_thread_connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 1.702587366104126s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845078.2058234
[0m00:24:39.908410 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: idle check connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 1.702587366104126s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845078.2058234
[0m00:24:39.909410 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m00:24:39.910410 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`snapshots`.`productmodel_snapshot`
  
[0m00:24:42.116007 [debug] [ThreadPool]: SQL status: OK in 2.2100000381469727 seconds
[0m00:24:42.118997 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
[0m00:24:42.121997 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: get_thread_connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 3.9161739349365234s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845078.2058234
[0m00:24:42.123168 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: idle check connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 3.917344570159912s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845078.2058234
[0m00:24:42.123168 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m00:24:42.124171 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`snapshots`.`salesorderdetail_snapshot`
  
[0m00:24:42.326036 [debug] [ThreadPool]: SQL status: OK in 0.20000000298023224 seconds
[0m00:24:42.329037 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
[0m00:24:42.332309 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: get_thread_connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 4.12648606300354s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845078.2058234
[0m00:24:42.333311 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: idle check connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 4.127488136291504s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845078.2058234
[0m00:24:42.333311 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m00:24:42.334310 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`snapshots`.`salesorderheader_snapshot`
  
[0m00:24:42.509506 [debug] [ThreadPool]: SQL status: OK in 0.18000000715255737 seconds
[0m00:24:42.518511 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: _release sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:42.519462 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: idle check connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 0.0009515285491943359s, acqrelcnt: 0, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:42.520514 [debug] [ThreadPool]: Databricks adapter: Thread (26240, 9524) using default compute resource.
[0m00:24:42.520514 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: _acquire sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 0.0020029544830322266s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:42.523793 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: get_thread_connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 0.004197835922241211s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:42.523793 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: idle check connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 0.005282402038574219s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:42.523793 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m00:24:42.524715 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "hive_metastore"} */

      select current_catalog()
  
[0m00:24:42.692224 [debug] [ThreadPool]: SQL status: OK in 0.17000000178813934 seconds
[0m00:24:42.700093 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: get_thread_connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 0.1805734634399414s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:42.701012 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: idle check connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 0.18158221244812012s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:42.701012 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m00:24:42.702045 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "hive_metastore"} */
show table extended in `hive_metastore`.`saleslt` like 'dim_customer|customeraddress|productdescription|my_second_dbt_model|salesorderdetail|dim_sales|dim_product|productmodel|product|salesorderheader|customer|address|my_first_dbt_model|productcategory'
  
[0m00:24:43.225991 [debug] [ThreadPool]: SQL status: OK in 0.5199999809265137 seconds
[0m00:24:43.227982 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`saleslt`.`address`
[0m00:24:43.231312 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: get_thread_connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 0.7128009796142578s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:43.231312 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: idle check connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 0.7128009796142578s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:43.232236 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m00:24:43.232236 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`saleslt`.`address`
  
[0m00:24:43.386472 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m00:24:43.390391 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`saleslt`.`customer`
[0m00:24:43.396391 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: get_thread_connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 0.8768789768218994s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:43.397390 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: idle check connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 0.877880334854126s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:43.397390 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m00:24:43.398586 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`saleslt`.`customer`
  
[0m00:24:43.543019 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m00:24:43.546010 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`saleslt`.`customeraddress`
[0m00:24:43.548028 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: get_thread_connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 1.029517412185669s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:43.549016 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: idle check connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 1.0305049419403076s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:43.549016 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m00:24:43.549969 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`saleslt`.`customeraddress`
  
[0m00:24:43.700834 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m00:24:43.705014 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`saleslt`.`dim_customer`
[0m00:24:43.708107 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: get_thread_connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 1.1895959377288818s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:43.709119 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: idle check connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 1.1895959377288818s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:43.709119 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m00:24:43.709119 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`saleslt`.`dim_customer`
  
[0m00:24:43.891566 [debug] [ThreadPool]: SQL status: OK in 0.18000000715255737 seconds
[0m00:24:43.893483 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`saleslt`.`dim_product`
[0m00:24:43.896563 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: get_thread_connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 1.378051996231079s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:43.897484 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: idle check connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 1.378051996231079s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:43.897484 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m00:24:43.897484 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`saleslt`.`dim_product`
  
[0m00:24:44.077196 [debug] [ThreadPool]: SQL status: OK in 0.18000000715255737 seconds
[0m00:24:44.081099 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`saleslt`.`dim_sales`
[0m00:24:44.084106 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: get_thread_connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 1.5655956268310547s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:44.084106 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: idle check connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 1.5655956268310547s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:44.085105 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m00:24:44.085105 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`saleslt`.`dim_sales`
  
[0m00:24:44.280123 [debug] [ThreadPool]: SQL status: OK in 0.1899999976158142 seconds
[0m00:24:44.283126 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`saleslt`.`my_first_dbt_model`
[0m00:24:44.285125 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: get_thread_connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 1.7666146755218506s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:44.286210 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: idle check connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 1.7676992416381836s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:44.286210 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m00:24:44.287176 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`saleslt`.`my_first_dbt_model`
  
[0m00:24:44.474024 [debug] [ThreadPool]: SQL status: OK in 0.1899999976158142 seconds
[0m00:24:44.478026 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`saleslt`.`my_second_dbt_model`
[0m00:24:44.480512 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: get_thread_connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 1.9620015621185303s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:44.481633 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: idle check connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 1.9620015621185303s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:44.481633 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m00:24:44.482534 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`saleslt`.`my_second_dbt_model`
  
[0m00:24:44.632269 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m00:24:44.635360 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`saleslt`.`product`
[0m00:24:44.637336 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: get_thread_connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 2.1188251972198486s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:44.638262 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: idle check connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 2.119750738143921s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:44.638262 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m00:24:44.639262 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`saleslt`.`product`
  
[0m00:24:44.806175 [debug] [ThreadPool]: SQL status: OK in 0.17000000178813934 seconds
[0m00:24:44.809267 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`saleslt`.`productcategory`
[0m00:24:44.812171 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: get_thread_connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 2.2936596870422363s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:44.813172 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: idle check connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 2.2936596870422363s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:44.813172 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m00:24:44.813172 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`saleslt`.`productcategory`
  
[0m00:24:44.961217 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m00:24:44.965133 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`saleslt`.`productdescription`
[0m00:24:44.967206 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: get_thread_connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 2.448695659637451s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:44.968230 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: idle check connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 2.4497194290161133s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:44.968230 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m00:24:44.969135 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`saleslt`.`productdescription`
  
[0m00:24:45.113693 [debug] [ThreadPool]: SQL status: OK in 0.14000000059604645 seconds
[0m00:24:45.117699 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`saleslt`.`productmodel`
[0m00:24:45.120688 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: get_thread_connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 2.6021769046783447s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:45.121605 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: idle check connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 2.6021769046783447s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:45.121605 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m00:24:45.121605 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`saleslt`.`productmodel`
  
[0m00:24:45.272630 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m00:24:45.276025 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`saleslt`.`salesorderdetail`
[0m00:24:45.279023 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: get_thread_connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 2.760512351989746s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:45.280025 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: idle check connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 2.760512351989746s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:45.280025 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m00:24:45.280025 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`saleslt`.`salesorderdetail`
  
[0m00:24:45.426242 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m00:24:45.429229 [debug] [ThreadPool]: Databricks adapter: Getting table schema for relation `hive_metastore`.`saleslt`.`salesorderheader`
[0m00:24:45.433160 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: get_thread_connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 2.91464900970459s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:45.434157 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: idle check connection: sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 2.91464900970459s, acqrelcnt: 1, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845082.5185113
[0m00:24:45.434157 [debug] [ThreadPool]: Using databricks connection "hive_metastore"
[0m00:24:45.434157 [debug] [ThreadPool]: On hive_metastore: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "medallion_dbt_spark", "target_name": "dev", "connection_name": "hive_metastore"} */
describe table `hive_metastore`.`saleslt`.`salesorderheader`
  
[0m00:24:45.580402 [debug] [ThreadPool]: SQL status: OK in 0.15000000596046448 seconds
[0m00:24:45.586484 [debug] [ThreadPool]: Databricks adapter: conn: 2282738461200: _release sess: 40a90410-9173-46f0-9ad4-104b402ea41c, name: hive_metastore, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (26240, 9524), cmpt: ``, lut: 1707845085.5864844
[0m00:24:45.601057 [debug] [MainThread]: Databricks adapter: conn: 2282738513872: _release sess: None, name: generate_catalog, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (26240, 10484), cmpt: ``, lut: 1707845085.6010573
[0m00:24:45.616795 [info ] [MainThread]: Catalog written to D:\Data Engineering\Medallion-Spark-Azure-DBT\medallion_dbt_spark\target\catalog.json
[0m00:24:45.618712 [debug] [MainThread]: Command `dbt docs generate` succeeded at 00:24:45.618712 after 12.06 seconds
[0m00:24:45.619785 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m00:24:45.619785 [debug] [MainThread]: Connection 'hive_metastore' was properly closed.
[0m00:24:45.620753 [debug] [MainThread]: On hive_metastore: ROLLBACK
[0m00:24:45.620753 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m00:24:45.620753 [debug] [MainThread]: On hive_metastore: Close
[0m00:24:45.705276 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002136AEE1F10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002136968A610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002136AE42C10>]}
[0m00:24:45.706275 [debug] [MainThread]: Flushing usage events
[0m00:24:58.441952 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219D3D87790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219D3D8E550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219D4044FD0>]}


============================== 00:24:58.445956 | 8629b6cd-4258-4a39-90a5-dafe4ce8775a ==============================
[0m00:24:58.445956 [info ] [MainThread]: Running with dbt=1.7.7
[0m00:24:58.447258 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'D:\\Data Engineering\\Medallion-Spark-Azure-DBT\\medallion_dbt_spark\\logs', 'profiles_dir': 'C:\\Users\\hanhn\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt docs serve', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m00:24:59.787570 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8629b6cd-4258-4a39-90a5-dafe4ce8775a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219E645BC10>]}
[0m00:24:59.866311 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8629b6cd-4258-4a39-90a5-dafe4ce8775a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219D4454990>]}
